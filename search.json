[{"title":"chap9 现代循环神经网络(6) seq2seq","url":"/2021/12/22/d2l-9-6/","content":"\n机器翻译中的输入序列和输出序列都是长度可变的。 为了解决这类问题，我们在 （5）节中设计了一个通用的”编码器－解码器“结构。\n\n 在本节中，我们将使用两个循环神经网络来设计这个结构中的编码器和解码器，并将其应用于机器翻译的 *序列到序列*（sequence to sequence，seq2seq）学习\n\n<!--more-->\n\n- 编码器是一个 RNN，读取输入句子\n  - 可以是双向（双向可以 encoder，但不能 decoder，即不能用于预测）\n- 解码器使用另外一个 RNN 来输出\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap9 现代循环神经网络(5) 编码器-解码器架构","url":"/2021/12/22/d2l-9-5/","content":"\n *编码器*（encoder）：它接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态\n\n *解码器*（decoder）：它将固定形状的编码状态映射到长度可变的序列。\n\n<!--more-->\n\n架构图示：\n\n{% asset_img encoder-decoder.svg encoder-decoder %}\n\n 一个模型被分为两块：\n\n- 编码器处理输入\n- 解码器生成输出\n\n\n\n## 编码器\n\n```python\nfrom torch import nn\n\n\n#@save\nclass Encoder(nn.Module):\n    \"\"\"编码器-解码器结构的基本编码器接口。\"\"\"\n    def __init__(self, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n\n    def forward(self, X, *args):\n        raise NotImplementedError\n```\n\n\n\n## 解码器\n\n```python\n#@save\nclass Decoder(nn.Module):\n    \"\"\"编码器-解码器结构的基本解码器接口。\"\"\"\n    def __init__(self, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n\n    def init_state(self, enc_outputs, *args):\n        raise NotImplementedError\n\n    def forward(self, X, state):\n        raise NotImplementedError\n```\n\n\n\n## 合并编码器和解码器\n\n```python\n#@save\nclass EncoderDecoder(nn.Module):\n    \"\"\"编码器-解码器结构的基类。\"\"\"\n    def __init__(self, encoder, decoder, **kwargs):\n        super(EncoderDecoder, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, enc_X, dec_X, *args):\n        enc_outputs = self.encoder(enc_X, *args)\n        dec_state = self.decoder.init_state(enc_outputs, *args)\n        return self.decoder(dec_X, dec_state)\n```\n\n\n\n## 小结\n\n* “编码器－解码器”结构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。\n* 编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。\n* 解码器将具有固定形状的编码状态映射为长度可变的序列。\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap9 现代循环神经网络(4) 双向循环神经网络","url":"/2021/12/21/d2l-9-4/","content":"\n在序列学习中，我们以往假设的目标是：到目前为止，在给定观测的情况下对下一个输出进行建模。例如，在时间序列的上下文中或在语言模型的上下文中。虽然这是一个典型的情况，但这并不是我们可能遇到的唯一情况。\n\n为了说明这个问题，考虑以下三个在文本序列中填空的任务：\n\n<!--more-->\n\n- 我 `___`。\n- 我 `___` 饿了。\n- 我 `___` 饿了，我可以吃半头猪。\n\n根据可获得的信息量，我们可以用不同的词填空，如“很高兴”（\"happy\"）、“不”（\"not\"）和“非常”（\"very\"）。很明显，短语的结尾（如果有的话）传达了重要信息，而这些信息关乎到选择哪个词来填空，所以不能利用这一点的序列模型（目前为止 RNN 只看过去）将在相关任务上表现不佳。\n\n例如，如果要做好命名实体识别（例如，识别“Green”指的是“格林先生”还是绿色），不同长度的上下文范围重要性是相同的。\n\n## 双向循环神经网络\n\n- 一个前向的 RNN 隐层\n- 一个后向的 RNN 隐层（一个从最后一个词元开始从后向前运行的循环神经网络）\n- 合并两个隐状态得到输出\n\n{% asset_img birnn.svg birnn %}\n\n双向循环神经网络是由 `Schuster.Paliwal.1997` 提出的。让我们看看这样一个网络的细节。\n\n对于任意时间步 $t$，给定一个小批量的输入数据 $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$（样本数：$n$，每个示例中的输入数：$d$），并且令隐藏层激活函数为 $\\phi$。在双向结构中，我们设该时间步的前向和反向隐藏状态分别为 $\\overrightarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$ 和 $\\overleftarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$，其中 $h$ 是隐藏单元的数目。前向和反向隐藏状态的更新如下：\n\n$$\n\\begin{aligned}\n\\overrightarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)}  + \\mathbf{b}_h^{(f)}),\\\\\n\\overleftarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)}  + \\mathbf{b}_h^{(b)}),\n\\end{aligned}\n$$\n\n其中，权重 $\\mathbf{W}_{xh}^{(f)} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh}^{(f)} \\in \\mathbb{R}^{h \\times h}, \\mathbf{W}_{xh}^{(b)} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh}^{(b)} \\in \\mathbb{R}^{h \\times h}$  和偏置 $\\mathbf{b}_h^{(f)} \\in \\mathbb{R}^{1 \\times h}, \\mathbf{b}_h^{(b)} \\in \\mathbb{R}^{1 \\times h}$ 都是模型参数。\n\n接下来，**将前向隐藏状态 $\\overrightarrow{\\mathbf{H}}_t$ 和反向隐藏状态 $\\overleftarrow{\\mathbf{H}}_t$ 连续起来**，获得需要送入输出层的隐藏状态 $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$。\n\n在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层。\n\n最后，输出层计算得到的输出为 $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$（$q$ 是输出单元的数目）：\n$$\n\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.\n$$\n\n\n这里，权重矩阵 $\\mathbf{W}_{hq} \\in \\mathbb{R}^{2h \\times q}$ 和偏置 $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$ 是输出层的模型参数。事实上，这两个方向可以拥有不同数量的隐藏单元。\n\n\n\n## 总结\n\n- 双向循环神经网络通过反向更新的隐藏层来利用方向时间信息\n- 通常用来对序列抽取特征、填空，而不是预测未来\n  - 对句子做特征提取：机器翻译、文本分类\n  - 不能应用在推理上（即，不能用他来训练语言模型），因为得不到未来信息的序列是无法预测的（不靠谱）。\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap9 现代循环神经网络(3) 深度循环神经网络","url":"/2021/12/21/d2l-9-3/","content":"\n到目前为止，我们只讨论了具有一个单向隐藏层的循环神经网络。如何获得更多的非线性呢？\n\n<!--more-->\n\n- 浅 RNN：\n  - 输入层\n  - 隐藏层\n  - 输出层\n\n- 深 RNN：\n  - 输入层\n  - 隐藏层\n  - 隐藏层\n  - …\n  - 输出层\n\n{% asset_img deep_rnn.svg deep_rnn %}\n\n假设我们在时间步 $t$ 有一个小批量的输入数据 $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$（样本数：$n$，每个样本中的输入数：$d$）。\n\n同时，将 $l^\\mathrm{th}$ 隐藏层（$l=1,\\ldots,L$）的隐藏状态设为 $\\mathbf{H}_t^{(l)}  \\in \\mathbb{R}^{n \\times h}$（隐藏单元数：$h$），\n\n输出层变量设为 $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$（输出数：$q$）。\n\n设置 $\\mathbf{H}_t^{(0)} = \\mathbf{X}_t$，第 $l$ 个隐藏层的隐藏状态使用激活函数 $\\phi_l$ 的表示如下：\n$$\n\\mathbf{H}_t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)}  + \\mathbf{b}_h^{(l)}),\n$$\n\n:eqlabel:`eq_deep_rnn_H`\n\n其中，权重 $\\mathbf{W}_{xh}^{(l)} \\in \\mathbb{R}^{h \\times h}$ 和 $\\mathbf{W}_{hh}^{(l)} \\in \\mathbb{R}^{h \\times h}$ 和偏置 $\\mathbf{b}_h^{(l)} \\in \\mathbb{R}^{1 \\times h}$ **都是第 $l$ 个隐藏层的模型参数。**\n\n最后，输出层的计算仅基于第 $l$ 个隐藏层最终的隐藏状态：\n\n$$\n\\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{hq} + \\mathbf{b}_q,\n$$\n\n其中，权重 $\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$ 和偏置 $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$ 都是**输出层的模型参数**。\n\n\n\n> 与多层感知机一样，**隐藏层的数目 $L$ 和隐藏单元的数目 $h$ 都是超参数。**也就是说，它们可以由我们来调整或指定。\n>\n> 另外，用门控循环单元或长短期记忆网络的隐藏状态来代替 :eqref:`eq_deep_rnn_H` 中的隐藏状态进行计算，可以很容易地得到**深度门控循环神经网络。**\n\n\n\n## 总结\n\n- 深度循环神经网络使用多个隐藏层来获得更多的非线性。\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap9 现代循环神经网络(2) LSTM","url":"/2021/12/21/d2l-9-2/","content":"\n长期以来，隐变量模型存在着长期信息保存和短期输入跳跃的问题。解决这一问题的最早方法之一是长短期存储器（long short-term memory, LSTM） 。它有许多与门控循环单元一样的属性。\n\n有趣的是，长短期记忆网络（LSTM）的设计比门控循环单元稍微复杂一些，却比门控循环单元（GRU）早诞生了近20年。\n\n<!--more-->\n\n## 门控记忆单元\n\n长短期记忆网络引入了 *存储单元*（memory cell），或简称为 *单元*（cell）。有些文献认为存储单元是隐藏状态的一种特殊类型，它们与隐藏状态具有相同的形状，其设计目的是用于记录附加的信息。\n\n为了控制存储单元，我们需要许多门。\n\n- 输入门（$I_t$）：决定是否忽略掉输入数据\n- 遗忘门（$F_t$）：将值朝 0 减少\n- 输出门（$O_t$）：决定是否使用隐变量的值\n\n{% asset_img lstm-0.svg lstm-0 %}\n\n数学描述，假设有 $h$ 个隐藏单元，批量大小为 $n$，输入数为 $d$。因此，输入为 $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$，前一时间步的隐藏状态为 $\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$。相应地，时间步 $t$ 的门被定义如下：输入门是 $\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$，遗忘门是 $\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$，输出门是 $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$。它们的计算方法如下：\n$$\n\\begin{aligned}\n\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\\n\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\\n\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o),\n\\end{aligned}\n$$\n\n其中 $\\mathbf{W}_{xi}, \\mathbf{W}_{xf}, \\mathbf{W}_{xo} \\in \\mathbb{R}^{d \\times h}$ 和 $\\mathbf{W}_{hi}, \\mathbf{W}_{hf}, \\mathbf{W}_{ho} \\in \\mathbb{R}^{h \\times h}$ 是**权重参数**，$\\mathbf{b}_i, \\mathbf{b}_f, \\mathbf{b}_o \\in \\mathbb{R}^{1 \\times h}$ 是**偏置参数**。\n\n  \n\n## 候选记忆单元\n\n接下来，设计记忆单元。由于还没有指定各种门的操作，所以先介绍 *候选记忆单元*（candidate memory cell）$\\tilde{\\mathbf{C}}_t \\in \\mathbb{R}^{n \\times h}$。\n\n它的计算与上面描述的三个门的计算类似，但是使用 $\\tanh$ 函数作为激活函数，函数的值范围为 $(-1, 1)$。下面导出在时间步 $t$ 处的方程：\n$$\n\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c),\n$$\n\n其中 $\\mathbf{W}_{xc} \\in \\mathbb{R}^{d \\times h}$ 和 $\\mathbf{W}_{hc} \\in \\mathbb{R}^{h \\times h}$ 是权重参数，$\\mathbf{b}_c \\in \\mathbb{R}^{1 \\times h}$ 是偏置参数。\n\n候选记忆单元的图示如 ：\n\n{% asset_img lstm-1.svg lstm-1 %}\n\n\n\n## 记忆单元（辅助）\n\n在门控循环单元中，有一种机制来控制输入和遗忘（或跳过）。\n\n类似地，在长短期记忆网络中，也有两个门用于这样的目的：\n\n输入门 $\\mathbf{I}_t$ 控制采用多少来自 $\\tilde{\\mathbf{C}}_t$ 的新数据，\n\n遗忘门 $\\mathbf{F}_t$ 控制保留了多少旧记忆单元 $\\mathbf{C}_{t-1} \\in \\mathbb{R}^{n \\times h}$ 的内容。使用与前面相同的按元素做乘法的技巧，得出以下更新公式：\n$$\n\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.\n$$\n\n\n如果遗忘门始终为 $1$ 且输入门始终为 $0$，则过去的记忆单元 $\\mathbf{C}_{t-1}$ 将随时间被保存并传递到当前时间步。引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。\n\n这样就得到了流程图，如:\n\n{% asset_img lstm-2.svg lstm-2 %}\n\n\n\n## 隐藏状态\n\n最后，我们需要定义如何计算隐藏状态 $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$。这就是输出门发挥作用的地方。\n\n在长短期记忆网络中，它仅仅是记忆单元的 $\\tanh$ 的门控版本。这就确保了 $\\mathbf{H}_t$ 的值始终在区间 $(-1, 1)$ 内。\n$$\n\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).\n$$\n\n\n只要输出门接近 $1$，我们就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近 $0$，我们只保留存储单元内的所有信息，并且没有进一步的过程需要执行。\n\n{% asset_img lstm-3.svg lstm-3 %}\n\n\n\n## 总结\n\n$$\n\\begin{aligned}\n\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\\n\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\\n\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o),\\\\\n\\tilde{\\mathbf{C}}_t &= \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c),\\\\\n\\mathbf{C}_t &= \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t,\\\\\n\\mathbf{H}_t &= \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).\n\\end{aligned}\n$$\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap9 现代循环神经网络(1) GRU","url":"/2021/12/20/d2l-9-1/","content":"\n\n\n关注一个序列\n\n- 不是每个观察值都同等重要\n- 想只记住相关的观察需要\n  - 能遗忘的机制：重置门\n  - 能关注的机制：更新门\n\n<!--more-->\n\n## 重置门和更新门\n\n数学描述，对于给定的时间步 $t$，假设输入是一个小批量 $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ （样本个数：$n$，输入个数：$d$），上一个时间步的隐藏状态是 $\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$（隐藏单元个数：$h$）。然后，重置门 $\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}$ 和更新门 $\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}$ 的计算如下：\n\n$$\n\\begin{aligned}\n\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r),\\\\\n\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z),\n\\end{aligned}\n$$\n\n其中 $\\mathbf{W}_{xr}, \\mathbf{W}_{xz} \\in \\mathbb{R}^{d \\times h}$ 和 $\\mathbf{W}_{hr}, \\mathbf{W}_{hz} \\in \\mathbb{R}^{h \\times h}$ 是**权重参数**，$\\mathbf{b}_r, \\mathbf{b}_z \\in \\mathbb{R}^{1 \\times h}$ 是**偏置参数**。【都是可学习参数】\n\n请注意，在求和过程中会触发广播机制。我们使用 sigmoid 函数将输入值转换到区间 $(0, 1)$。\n\n## 候选隐藏状态（重置门的应用）\n\n接下来，让我们将重置门 $\\mathbf{R}_t$ 与常规隐状态更新机制集成，得到在时间步 $t$ 的候选隐藏状态 $\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$。\n\n$$\n\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h),\n$$\n:eqlabel:`gru_tilde_H`\n\n其中 $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$ 和 $\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$ 是权重参数，$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$ 是偏置项，符号 $\\odot$ 是哈达码乘积（按元素乘积）运算符。在这里，我们使用 tanh 非线性激活函数来确保候选隐藏状态中的值保持在区间 $(-1, 1)$ 中。\n\n计算的结果是 *候选者*（candidate），因为我们仍然需要结合更新门的操作。与 rnn 相比 :eqref:`gru_tilde_H` 中的 $\\mathbf{R}_t$ 和 $\\mathbf{H}_{t-1}$ 的元素相乘可以减少以往状态的影响。\n\n每当重置门 $\\mathbf{R}_t$ 中的项接近 $1$ 时，我们恢复一个如 rnn 中的普通的循环神经网络。\n\n对于重置门 $\\mathbf{R}_t$ 中所有接近 $0$ 的项，候选隐藏状态是以 $\\mathbf{X}_t$ 作为输入的多层感知机的结果【即，丢弃过往信息】。因此，任何预先存在的隐藏状态都会被 **重置** 为默认值。\n\n\n\n## 隐藏状态（更新门的应用）\n\n最后，我们需要结合更新门 $\\mathbf{Z}_t$ 的效果。\n\n这确定新的隐藏状态 $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$ 在多大程度上就是旧的状态 $\\mathbf{H}_{t-1}$ ，以及对新的候选状态 $\\tilde{\\mathbf{H}}_t$ 的使用量。\n\n更新门 $\\mathbf{Z}_t$ 仅需要在 $\\mathbf{H}_{t-1}$ 和 $\\tilde{\\mathbf{H}}_t$ 之间进行按元素的凸组合就可以实现这个目标。这就得出了门控循环单元的最终更新公式：\n$$\n\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.\n$$\n\n\n每当更新门 $\\mathbf{Z}_t$ 接近 $1$ 时，我们就只保留旧状态。此时，来自 $\\mathbf{X}_t$ 的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步 $t$。\n\n相反，当 $\\mathbf{Z}_t$ 接近 $0$ 时，新的隐藏状态 $\\mathbf{H}_t$ 就会接近候选的隐藏状态 $\\tilde{\\mathbf{H}}_t$。\n\n这些设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系。\n\n例如，如果整个子序列的所有时间步的更新门都接近于 $1$，则无论序列的长度如何，在序列起始时间步的旧隐藏状态都将很容易保留并传递到序列结束。\n\n\n\n\n\n## 总结\n\n\n\n{% asset_img gru-3.svg gru-3%}\n\n\n\n门控循环单元具有以下两个显著特征：\n\n* 重置门有助于捕获序列中的短期依赖关系。\n* 更新门有助于捕获序列中的长期依赖关系。\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap8 循环神经网络（4） RNN","url":"/2021/12/19/d2l-8-2/","content":"\n\n\n在上一小节中，我们介绍了 $n$ 元语法模型，其中单词 $x_t$ 在时间步 $t$ 的条件概率仅取决于前面 $n-1$ 个单词。如果我们想将时间步 $t-(n-1)$ 之前的单词的可能产生的影响合并到 $x_t$ 上就需要增加 $n$，然而模型参数的数量也会随之呈指数增长，因为词表 $\\mathcal{V}$ 需要存储 $|\\mathcal{V}|^n$ 个数字，因此与其将 $P(x_t \\mid x_{t-1}, \\ldots, x_{t-n+1})$ 模型化，不如使用隐变量模型：\n\n<!--more-->\n$$\nP(x_t \\mid x_{t-1}, \\ldots, x_1) \\approx P(x_t \\mid h_{t-1}),\n$$\n\n\n其中 $h_{t-1}$ 是 *隐藏状态*（也称为隐藏变量），其存储了到时间步 $t-1$ 的序列信息。通常，可以基于当前输入 $x_{t}$ 和先前隐藏状态 $h_{t-1}$ 来计算时间步 $t$ 处的任何时间的隐藏状态：\n\n$$\nh_t = f(x_{t}, h_{t-1})\n$$\n:eqlabel:`eq_ht_xt`\n\n对于一个足够强大的函数 $f$（ :eqref:`eq_ht_xt` ），隐变量模型不是近似值。**毕竟 $h_t$ 是可以仅仅存储到目前为止观察到的所有数据，然而这样的操作可能会使计算和存储的代价都变得昂贵。**\n\n回想一下，我们在 第四章 中讨论过的具有隐藏单元的隐藏层。值得注意的是，隐藏层和隐藏状态指的是两个截然不同的概念。如上所述，隐藏层是在输入到输出的路径上以观测角度来理解的**隐藏的层**，而隐藏状态则是在给定步骤所做的任何事情**以技术角度来定义的 *输入***，并且这些状态只能通过先前时间步的数据来计算。\n\n***循环神经网络*（Recurrent neural networks， RNNs）是具有隐藏状态的神经网络。**\n\n\n\n## 循环神经网络\n\n循环神经网络指网络的隐含层输出又作为自身的输入。\n\n假设我们在时间步$t$有小批量输入$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$。换言之，对于$n$个序列样本的小批量，$\\mathbf{X}_t$的每一行对应于来自该序列的时间步$t$处的一个样本。接下来，用$\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$表示时间步$t$的隐藏变量。与多层感知机不同的是，我们在这里保存了前一个时间步的隐藏变量$\\mathbf{H}_{t-1}$，并引入了一个**新的权重参数$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$**来描述如何在当前时间步中使用前一个时间步的隐藏变量。具体地说，**当前时间步隐藏变量的计算由当前时间步的输入与前一个时间步的隐藏变量一起确定：**\n\n$$\n\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h).\n$$\n:eqlabel:`rnn_h_with_state`\n\n与 无隐藏状态 相比， :eqref:`rnn_h_with_state` 多添加了一项$\\mathbf{H}_{t-1} \\mathbf{W}_{hh}$，从而实例化了 :eqref:`eq_ht_xt`。**从相邻时间步的隐藏变量$\\mathbf{H}_t$和$\\mathbf{H}_{t-1}$之间的关系可知，这些变量捕获并保留了序列直到其当前时间步的历史信息，就如当前时间步下神经网络的状态或记忆，因此这样的隐藏变量被称为 *隐藏状态*（hidden state）。**\n\n**由于在当前时间步中隐藏状态使用的定义与前一个时间步中使用的定义相同，因此 :eqref:`rnn_h_with_state` 的计算是 *循环的*（recurrent）。**于是基于循环计算的隐状态神经网络被命名为 *循环神经网络*（recurrent neural networks）。在循环神经网络中执行 :eqref:`rnn_h_with_state` 计算的层称为 *循环层*（recurrent layers）。\n\n有许多不同的方法可以构建循环神经网络，由 :eqref:`rnn_h_with_state` 定义的隐藏状态的循环神经网络是非常常见的一种。对于时间步$t$，输出层的输出类似于多层感知机中的计算：\n\n$$\n\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.\n$$\n\n\n循环神经网络的参数包括隐藏层的权重$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$和偏置$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$，以及输出层的权重$\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$和偏置$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$。值得一提的是，即使在不同的时间步，循环神经网络也总是使用这些模型参数。因此，循环神经网络的参数开销不会随着时间步的增加而增加。\n\n\n{% asset_img rnn.svg RNN %}\n\n\n上图展示了循环神经网络在三个相邻时间步的计算逻辑。\n\n在任意时间步$t$，隐藏状态的计算可以被视为：\n\n1、拼接当前时间步$t$的输入$\\mathbf{X}_t$和前一时间步$t-1$的隐藏状态$\\mathbf{H}_{t-1}$；\n\n2、将拼接的结果送入带有激活函数$\\phi$的全连接层。全连接层的输出是当前时间步$t$的隐藏状态$\\mathbf{H}_t$。在本例中，模型参数是$\\mathbf{W}_{xh}$和$\\mathbf{W}_{hh}$的拼接，以及$\\mathbf{b}_h$的偏置，所有这些参数都来自 :eqref:`rnn_h_with_state`。当前时间步$t$的隐藏状态$\\mathbf{H}_t$将参与计算下一时间步$t+1$的隐藏状态$\\mathbf{H}_{t+1}$。而且$\\mathbf{H}_t$还将送入全连接输出层用于计算当前时间步$t$的输出$\\mathbf{O}_t$。\n\n\n\n## 困惑度\n\n**困惑度是度量语言模型质量的指标。**\n\n如果想要压缩文本，我们可以询问根据当前词元集预测的下一个词元。一个更好的语言模型应该能让我们更准确地预测下一个词元。因此，它应该允许我们在压缩序列时花费更少的比特。所以我们可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量：\n\n$$\n\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\n$$\n\n:eqlabel:`eq_avg_ce_for_lm`\n\n其中$P$由语言模型给出，**$x_t$ 是在时间步$t$从该序列中观察到的实际词元【label】**。这使得不同长度的文档的性能具有了可比性。由于历史原因，自然语言处理的科学家更喜欢使用一个叫做 *困惑度*（perplexity）的量。简而言之，它是 :eqref:`eq_avg_ce_for_lm` 的指数：\n\n$$\n\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right)\n$$\n\n\n当我们决定下一个词元是哪个时，**困惑度的最好的理解可以是下一个词元的实际选择数的调和平均数**。让我们看看一些案例：\n\n* 在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。\n\n* 在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。\n\n* 在基线上，该模型的预测是词汇表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词汇表中唯一词元的数量。事实上，如果我们在没有任何压缩的情况下存储序列，这将是我们能做的最好的编码方式。因此，这种方式提供了一个重要的上限，而任何实际模型都必须超越这个上限。\n\n  \n\n\n## 梯度裁剪\n\n- 迭代计算这 T 个时间布上的梯度，在反向传播过程中产生 $O(T)$ 的矩阵乘法链，导致数值不稳定\n\n- 梯度裁剪能有效预防梯度爆炸\n\n  如果梯度长度超过 $\\theta$ ，则拖影回长度 $\\theta$ \n\n$$\ng \\gets min(1,\\frac{\\theta}{||g||} )g\n$$\n\n## 总结\n\n- 循环神经网络的输出取决于当下输入和前一时间的隐变量\n- 应用到语言模型中时，循环神经网络根据当前词预测下一时刻词\n- 常使用困惑度来衡量语言模型的好坏\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap8 循环神经网络(1) 语言模型","url":"/2021/12/19/d2l-8-1/","content":"\n卷积神经网络可以有效地处理空间信息，*循环神经网络*（recurrent neural network, RNN）这种设计可以更好地处理序列信息。 循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。\n\n许多使用循环网络的例子都是基于文本数据的，因此我们将在本章中重点介绍语言模型。 在对序列数据进行更详细的回顾之后，我们将介绍文本预处理的实用技术。 然后，我们将讨论语言模型的基本概念，并将此讨论作为循环神经网络设计的灵感。 最后，我们描述了循环神经网络的梯度计算方法，以探讨训练此类网络时可能遇到的问题。\n\n<!--more-->\n\n> 序列模型的核心在于预测文本序列出现的概率。\n\n\n\n给定文本序列 x1,x2,…xT , 语言模型的目标是估计联合概率 p(x1,…,xT) 。\n\n他的应用包括：\n\n- 做预训练模型 BERT， GPT-3\n- 生成文本，给定前面几个词，生成后续文本\n- 判断多个序列中哪个更常见\n\n\n\n## N元语法\n\n一元语法：每个 x 的信息独立，马尔可夫假设中 $\\tau = 0$\n\n二元语法：$\\tau = 1$，每个 x 的信息只和他前一个项有关\n\n三元语法：$\\tau = 2$ ，每个 x 的信息只和他前 2 个项有关\n$$\n\\begin{aligned}\nP(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4),\\\\\nP(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_2) P(x_4  \\mid  x_3),\\\\\nP(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_1, x_2) P(x_4  \\mid  x_2, x_3).\n\\end{aligned}\n$$\n优点：可以处理较长的文本序列\n\n\n\n## 总结\n\n- 语言模型估计文本序列的联合概率\n- 使用统计方法时常采用 n 元语法\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap6(5) 池化层","url":"/2021/12/19/d2l-6-5/","content":"\n本节将介绍 *池化*（pooling）层，它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。\n<!--more-->\n\n通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率，聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。\n\n而我们的机器学习任务通常会跟全局图像的问题有关（例如，“图像是否包含一只猫呢？”）， 所以我们最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。\n\n此外，当检测较底层的特征时，我们通常希望这些特征保持某种程度上的平移不变性。例如，如果我们拍摄黑白之间轮廓清晰的图像 `X`，并将整个图像向右移动一个像素，即 `Z[i, j] = X[i, j + 1]`，则新图像 `Z` 的输出可能大不相同。而在现实中，随着拍摄角度的移动，任何物体几乎不可能发生在同一像素上。即使用三脚架拍摄一个静止的物体，由于快门的移动而引起的相机振动，可能会使所有物体左右移动一个像素（除了高端相机配备了特殊功能来解决这个问题）。\n\n## 池化层\n\n最大池化层：\n\n- 返回滑动窗口中的最大值\n\n平均池化层：\n\n- 返回滑动窗口中的平均值\n\n\n\n## 填充、步幅、多个通道\n\n- 与卷积层类似\n- 没有可学习的参数（没有 kernel）\n- 在每个输入通道应用池化层获得相应的输出通道，不会做融合\n- 输出通道数=输入通道数\n\n\n\n## 总结\n\n- 池化层返回窗口中最大或平均值\n- 缓解卷积层对位置的敏感性\n- 同样有窗口大小、填充、步幅作为超参数\n\n\n\n## torch 知识点\n\n### **1 torch.cat()**\n\n> `torch.cat`(*tensors*,*dim=0*,*out=None*)→ Tensor\n\ntorch.cat()对tensors沿指定维度拼接，但返回的Tensor的维数不会变【理解为缝合拼接，拉长向量长度】\n\n```python3\n>>> import torch\n>>> a = torch.rand((2, 3))\n>>> b = torch.rand((2, 3))\n>>> c = torch.cat((a, b))\n>>> a.size(), b.size(), c.size()\n(torch.Size([2, 3]), torch.Size([2, 3]), torch.Size([4, 3]))\n```\n\n可以看到 c 和 a、b 一样都是二维的。\n\n\n\n### **2 torch.stack()**\n\n> `torch.stack`(*tensors*,*dim=0*,*out=None*)→ Tensor\n\ntorch.stack()同样是对tensors沿指定维度拼接，但返回的Tensor会多一维【可以理解为叠加，一层层摞上去】\n\n```python3\n>>> import torch\n>>> a = torch.rand((2, 3))\n>>> b = torch.rand((2, 3))\n>>> c = torch.stack((a, b))\n>>> a.size(), b.size(), c.size()\n(torch.Size([2, 3]), torch.Size([2, 3]), torch.Size([2, 2, 3]))\n```\n\n可以看到 c 是三维的，比 a、b 多了一维。\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap6 卷积神经网络(4) 多输入多输出通道","url":"/2021/12/19/d2l-6-4/","content":"\n虽然我们在 第（1）小节中描述了构成每个图像的多个通道和多层卷积层。例如彩色图像具有标准的 RGB 通道来指示红、绿和蓝。\n\n但是到目前为止，我们仅展示了单个输入和单个输出通道的简化例子。这使得我们可以将输入、卷积核和输出看作二维张量。\n\n当我们添加通道时，我们的输入和隐藏的表示都变成了三维张量。\n\n例如，每个RGB输入图像具有 $3\\times h\\times w$ 的形状。我们**将这个大小为 $3$ 的轴称为 *通道*（channel） 维度**。在本节中，我们将更深入地研究具有多输入和多输出通道的卷积核。\n\n<!--more-->\n\n## 多输入通道\n\n- **每个通道都有一个卷积核，结果是所有通道卷积结果的和**\n\n{% asset_img conv-multi-in.svg 两个输入通道的互相关计算。 %}\n\n当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数目的卷积核【n 维卷积核】，以便与输入数据进行互相关运算。\n\n假设输入的通道数为 $c_i$，那么卷积核的输入通道数也需要为 $c_i$ 。如果卷积核的窗口形状是 $k_h\\times k_w$，那么当 $c_i=1$ 时，我们可以把卷积核看作形状为 $k_h\\times k_w$ 的二维张量。\n\n然而，当 $c_i>1$ 时，我们卷积核的每个输入通道将包含形状为 $k_h\\times k_w$ 的张量。将这些张量 $c_i$ 连结在一起可以得到形状为 $c_i\\times k_h\\times k_w$ 的卷积核。由于输入和卷积核都有 $c_i$ 个通道，我们可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和（将 $c_i$ 的结果相加）得到二维张量。这是多通道输入和多输入通道卷积核之间进行二维互相关运算的结果。\n\n\n\n## 多输出通道\n\n每一层有多个输出通道是至关重要的。在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。\n\n直观地说，我们可以将每个通道看作是对不同特征的响应。而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。因此，多输出通道并不仅是学习多个单通道的检测器。\n\n\n\n- 无论有多少输入通道，到目前位置我们只用到单输出通道\n\n- 我们可以有【多个三维卷积核】，每个核生成一个输出通道\n\n- 用 $c_i$ 和 $c_o$ 分别表示输入和输出通道的数目，并让 $k_h$ 和 $k_w$ 为卷积核的高度和宽度。为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为 $c_i\\times k_h\\times k_w$ 的卷积核张量，这样卷积核的形状是 $c_o\\times c_i\\times k_h\\times k_w$。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。\n\n- 每个输出通道可以识别特定模式\n\n- 输入通道核识别并组合输入中的模式\n\n  \n\n## 1x1 卷积层\n\n- $k_h=k_w=1$ 是一个受欢迎的选择。它不识别空间模式【只关注一个元素，不关心元素周边】，只是融合通道。\n- 相当于输入形状为 $n_hn_w\\times c_i$，权重为 $c_o \\times c_i$ 的全连接层。\n\n\n\n{% asset_img conv-1x1.svg fig_conv_1x1 %}\n\n\n\n上图展示了使用 $1\\times 1$ 卷积核与 $3$ 个输入通道和 $2$ 个输出通道的互相关计算。\n\n这里输入和输出具有相同的高度和宽度，输出中的每个元素都是从输入图像中同一位置的元素的线性组合。\n\n我们可以将 $1\\times 1$ 卷积层看作是在每个像素位置应用的全连接层，以 $c_i$ 个输入值转换为 $c_o$ 个输出值。\n\n因为这仍然是一个卷积层，所以跨像素的权重是一致的。同时，$1\\times 1$ 卷积层需要的权重维度为 $c_o\\times c_i$ ，再额外加上一个偏置。\n\n\n\n## 总结\n\n- 输出通道数是卷积层的超参数\n- 每个输入通道有独立的二维卷积核，所有通道结果相加得到一个输出通道结果\n- 每个输出通道有独立的三维卷积核\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap6 卷积神经网络(3) 填充和步幅","url":"/2021/12/13/d2l-6-3/","content":"\n假设输入形状为 $n_h\\times n_w$，卷积核形状为 $k_h\\times k_w$，那么输出形状将是 $(n_h-k_h+1) \\times (n_w-k_w+1)$。\n因此，卷积的输出形状取决于输入形状和卷积核的形状。\n\n还有什么因素会影响输出的大小呢？本节我们将介绍 *填充*（padding）和 *步幅* (stride)。\n\n假设以下情景：\n有时，在应用了连续的卷积之后，我们最终**得到的输出远小于输入大小**。这是由于卷积核的宽度和高度通常大于 $1$ 所导致的。比如，一个 $240 \\times 240$ 像素的图像，经过 $10$ 层 $5 \\times 5$ 的卷积后，将减少到 $200 \\times 200$ 像素。如此一来，原始图像的边界丢失了许多有用信息。 而***填充*** 是解决此问题最有效的方法。\n有时，我们可能希望**大幅降低图像的宽度和高度**。例如，如果我们发现**原始的输入分辨率十分冗余**。 ***步幅 ***则可以在这类情况下提供帮助。\n\n[本章代码复现](https://github.com/karin0018/d2l_MuLi/tree/master/convolutional-neural-network)\n\n<!--more-->\n\n## 填充\n\n通常，如果我们添加 $p_h$ 行填充（大约一半在顶部，一半在底部）和 $p_w$ 列填充（左侧大约一半，右侧一半），则输出形状将为\n\n$$\n(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1)。\n$$\n\n\n这意味着输出的高度和宽度将分别增加 $p_h$ 和 $p_w$。\n\n在许多情况下，我们需要设置 $p_h=k_h-1$ 和 $p_w=k_w-1$，使输入和输出具有相同的高度和宽度。\n\n这样可以在构建网络时更容易地预测每个图层的输出形状。\n\n假设 $k_h$ 是奇数，我们将在高度的两侧填充 $p_h/2$ 行。\n\n如果 $k_h$ 是偶数，则一种可能性是在输入顶部填充 $\\lceil p_h/2\\rceil$ 行，在底部填充 $\\lfloor p_h/2\\rfloor$ 行。同理，我们填充宽度的两侧。\n\n> 卷积神经网络中卷积核的高度和宽度通常为奇数，例如 1、3、5 或 7。\n> 选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。\n\n此外，使用奇数核和填充也提供了书写上的便利。对于任何二维张量 `X`，当满足：\n\n1. 内核的大小是奇数；\n\n2. 所有边的填充行数和列数相同；\n\n3. 输出与输入具有相同高度和宽度\n\n  则可以得出：输出 `Y[i, j]` 是通过以输入 `X[i, j]` 为中心，与卷积核进行互相关计算得到的。\n\n\n\n## 步幅\n\n- 填充减小的输出大小与层数线性相关\n  - 需要大量计算才能得到较小的输出\n\n- 步幅是指行/列的滑动步长\n\n通常，当垂直步幅为 $s_h$ 、水平步幅为 $s_w$ 时，输出形状为\n\n$$\n\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.\n$$\n\n\n如果我们设置了 $p_h=k_h-1$ 和 $p_w=k_w-1$，则输出形状将简化为 \n$$\n\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor\n$$\n\n更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为 $(n_h/s_h) \\times (n_w/s_w)$。\n\n\n\n## 小结\n\n- 填充和步幅是卷积神经网络的超参数。\n\n* 填充可以增加输出的高度和宽度。这常用来使输出与输入具有相同的高和宽。\n* 步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的 $1/n$（ $n$ 是一个大于 $1$ 的整数）。\n* 填充和步幅可用于有效地调整数据的维度。\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap6 卷积神经网络(2) 图像卷积","url":"/2021/12/13/d2l-6-2/","content":"上节我们解析了卷积层的原理，现在我们看看它的实际应用。由于卷积神经网络的设计是用于探索图像数据，本节我们将以图像为例。\n\n[本章代码复现](https://github.com/karin0018/d2l_MuLi/tree/master/convolutional-neural-network)\n\n<!-- more -->\n\n## 互相关运算\n\n严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是 *互相关运算* (cross-correlation)，而不是卷积运算。根据 :numref:`sec_why-conv` 中的描述，在卷积层中，输入张量和核张量通过(**互相关运算**)产生输出张量。\n\n在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。\n\n当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到一个单一的标量值，由此我们得出了这一位置的输出张量值。\n\n{% asset_img correlation.svg 二维互相关运算。阴影部分是第一个输出元素，以及用于计算这个输出的输入和核张量元素：$0\\times0+1\\times1+3\\times2+4\\times3=19$. %}\n\n\n\n## 卷积层\n\n**功能**：卷积层对输入和卷积核权重进行**互相关运算**，并在**添加标量偏置**之后产生输出。\n\n**被训练的参数**：**卷积核权重**和**标量偏置**。 \n\n**初始化**：就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。\n\n\n\n### 二维卷积层\n\n- 输入： $X:n_h\\times n_w$\n\n- 核： $W: k_h \\times k_w$\n\n- 偏差： $b \\in R$\n\n- 输出：$Y : (n_h-k_h+1)\\times(n_w-k_w+1)$\n  $$\n  \\mathbf{Y=X*W}+b\n  $$\n  $\\mathbf{W},b$ 都是可学习的参数，$*$ 是前面定义的互相关运算子\n\n  > 所以卷积核是学习出来的 :)\n\n\n\n## 总结\n\n- 卷积层是将输入和核矩阵进行交叉相关计算，加上偏移之后得到输出\n- 核矩阵和偏移是可以学习的参数\n- 核矩阵的大小是超参数\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap6 卷积神经网络(1)","url":"/2021/12/13/d2l-6-1/","content":"[本章教材地址](https://zh.d2l.ai/chapter_convolutional-neural-networks/index.html), [本章课程视频](https://www.bilibili.com/video/BV1L64y1m7Nh?spm_id_from=333.999.0.0)\n\n> 在前面的章节中，我们遇到过图像数据。 这种数据的每个样本都由一个二维像素网格组成， 每个像素可能是一个或者多个数值，取决于是黑白还是彩色图像。 到目前为止，我们处理这类结构丰富的数据的方式还不够有效。 我们仅仅通过将图像数据展平成一维向量而忽略了每个图像的空间结构信息，再将数据送入一个全连接的多层感知机中。 因为这些网络特征元素的顺序是不变的，因此最优的结果是利用先验知识，即利用相近像素之间的相互关联性，从图像数据中学习得到有效的模型。\n\n本章介绍的卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。 基于卷积神经网络结构的模型在计算机视觉领域中已经占主导地位，当今几乎所有的图像识别、对象检测或语义分割相关的学术竞赛和商业应用都以这种方法为基础。\n\n<!-- more -->\n\n现代卷积神经网络的设计得益于生物学、群论和一系列的补充实验。 卷积神经网络需要的参数少于全连接结构的网络，而且卷积也很容易用 GPU 并行计算。 因此卷积神经网络除了能够高效地采样从而获得精确的模型，还能够高效地计算。 久而久之，从业人员更能多地应用卷积神经网络，即使在通常使用循环神经网络的一维序列结构任务上（例如音频、文本和时间序列分析），卷积神经网络也越来越受欢迎。 通过对卷积神经网络一些巧妙的调整，也使它们在图结构数据和推荐系统中发挥作用。\n\n在本章的开始，我们将介绍构成所有卷积网络主干的基本元素。 这包括卷积层本身、填充（padding）和步幅（stride）的基本细节、用于在相邻区域的汇聚层（pooling）、在每一层中多通道（channel）的使用，以及有关现代卷积网络架构的仔细讨论。 在本章的最后，我们将介绍一个完整的、可运行的LeNet模型：这是第一个成功应用的卷积神经网络，比现代深度学习兴起时间还要早。 在下一章中，我们将深入研究一些流行的、相对较新的卷积神经网络架构的完整实现，这些网络架构涵盖了现代从业者通常使用的大多数经典技术。\n\n## 从全连接层到卷积\n\n适合于计算机视觉的神经网络结构有以下两个原则：\n\n1. *平移不变性*（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。\n\n   > 不管目标在图片的什么位置，都不影响目标的识别。\n\n2. *局部性*（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，在后续神经网络，整个图像级别上可以集成这些局部特征用于预测。\n\n   > 识别的目标是图的一个部分。\n\n## 限制多层感知机\n\n首先，多层感知机的输入是二维图像$\\mathbf{X}$，其隐藏表示 $\\mathbf{H}$ 在数学上是一个矩阵，在代码中表示为二维张量。\n其中 $\\mathbf{X}$ 和 $\\mathbf{H}$ 具有相同的形状。\n为了方便理解，我们可以认为，无论是输入还是隐藏表示都拥有空间结构。\n\n使用  $[\\mathbf{X}]_{i, j}$ 和 $[\\mathbf{H}]_{i, j}$ 分别表示输入图像和隐藏表示中位置($i$, $j$)处的像素。\n\n为了使每个隐藏神经元都能接收到每个输入像素的信息，我们将参数从权重矩阵（如同我们先前在多层感知机中所做的那样）替换为四阶权重张量 $\\mathsf{W}$。假设 $\\mathbf{U}$ 包含偏置参数，我们可以将全连接层形式化地表示为\n$$\n\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l}\\\\ &=  [\\mathbf{U}]_{i, j} +\n\\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b}  [\\mathbf{X}]_{i+a, j+b}.\\end{aligned}\n$$\n\n\n其中，从 $\\mathsf{W}$ 到 $\\mathsf{V}$ 的转换只是形式上的转换，因为在这两个四阶张量的元素之间存在一一对应的关系。我们只需重新索引下标 $(k, l)$，使 $k = i+a$、$l = j+b$， 由此可得 $[\\mathsf{V}]_{i, j, a, b} = [\\mathsf{W}]_{i, j, i+a, j+b}$。\n\n索引 $a$ 和 $b$ 通过在正偏移和负偏移之间移动覆盖了整个图像。\n\n对于隐藏表示中任意给定位置（$i$, $j$）处的像素值$[\\mathbf{H}]_{i, j}$，可以通过在 $x$ 中以 $(i, j)$ 为中心对像素进行加权求和得到，加权使用的权重为 $[\\mathsf{V}]_{i, j, a, b}$ 。\n\n\n\n### 平移不变性\n\n现在引用上述的第一个原则：平移不变性。\n这意味着检测对象在输入 $\\mathbf{X}$ 中的平移，应该仅仅导致隐藏表示 $\\mathbf{H}$ 中的平移。也就是说， **$\\mathsf{V}$ 和 $\\mathbf{U}$ 实际上不依赖于 $(i, j)$ 的值**，即 $[\\mathsf{V}]_{i, j, a, b} = [\\mathbf{V}]_{a, b}$。并且  $\\mathbf{U}$ 是一个常数，比如 $u$。因此，我们可以简化 $\\mathbf{H}$ 定义为：\n$$\n[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b} [\\mathbf{X}]_{i+a, j+b}.\n$$\n\n\n这就是 ***卷积*** （convolution）。我们是在使用系数 $[\\mathbf{V}]_{a, b}$ 对位置 $(i, j)$ 附近的像素 $(i+a, j+b)$ 进行加权得到$[\\mathbf{H}]_{i, j}$。\n\n> 显著的进步：$[\\mathbf{V}]_{a, b}$ 的系数比 $[\\mathsf{V}]_{i, j, a, b}$ 少很多，因为前者不再依赖于图像中的位置，进而模型复杂度大大降低。\n\n\n\n### 局部性\n\n现在引用上述的第二个原则：局部性。\n\n- 评估 $[\\mathbf{H}]_{i, j}$ 时，我们不应该用远离 $X_{(i, j)}$ 的参数\n- 解决方案：在 $|a|> \\Delta$ 或 $|b| > \\Delta$ 的范围之外，我们可以设置 $[\\mathbf{V}]_{a, b} = 0$。\n\n因此，我们可以将 $[\\mathbf{H}]_{i, j}$ 重写为\n$$\n[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.\n$$\n:eqlabel:`eq_conv-layer`\n\n简而言之， :eqref:`eq_conv-layer` 是一个 *卷积层* （convolutional layer），而卷积神经网络是包含卷积层的一类特殊的神经网络。\n\n- 在深度学习研究社区中， **$\\mathbf{V}$ 被称为 *卷积核* （convolution kernel） 或者  *滤波器* （filter）**，它仅仅是可学习的一个层的**权重。**\n  当图像处理的局部区域很小时，卷积神经网络与多层感知机的训练差异可能是巨大的：**以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。**\n\n- 参数大幅减少的代价：我们的特征现在是平移不变的，并且当确定每个隐藏激活的值时，每一层只能包含局部的信息。\n\n> 以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到未知数据中。\n> 但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据。\n\n### 总结\n\n- 对全连接层使用 **平移不变性** 和 **局部性** 得到卷积层。\n\n  \n\n## 卷积\n\n在进一步讨论之前，我们先简要回顾一下为什么上面的操作被称为卷积。在数学中，两个函数（比如 $f, g: \\mathbb{R}^d \\to \\mathbb{R}$）之间的“卷积”被定义为\n\n$$\n(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}.\n$$\n\n\n也就是说，卷积是测量 $f$ 和 $g$ 之间（把其中一个函数“翻转”并移位 $\\mathbf{x}$ 时）的重叠。\n当我们有离散对象时，积分就变成求和。例如：对于由索引为$\\mathbb{Z}$的、平方可和的、无限维向量集合中抽取的向量，我们得到以下定义：\n$$\n(f * g)(i) = \\sum_a f(a) g(i-a).\n$$\n\n\n对于二维张量，则为 $f$ 的索引 $(a, b)$ 和 $g$ 的索引 $(i-a, j-b)$ 上的对应和：\n\n$$\n(f * g)(i, j) = \\sum_a\\sum_b f(a, b) g(i-a, j-b).\n$$\n:eqlabel:`eq_2d-conv-discrete`\n\n这看起来类似于 :eqref:`eq_conv-layer`，但有一个主要区别：这里不是使用 $(i+a, j+b)$ ，而是使用差值。然而，**这种区别主要是装饰性的**，因为我们总是可以匹配  :eqref:`eq_conv-layer` 和 :eqref:`eq_2d-conv-discrete` 之间的符号。\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap5 深度学习计算","url":"/2021/12/13/chap5-computation/","content":"[本章教材地址](https://zh.d2l.ai/chapter_deep-learning-computation/index.html), [本章课程视频](https://www.bilibili.com/video/BV1AK4y1P7vs?spm_id_from=333.999.0.0)\n\n> 到目前为止，我们已经介绍了一些基本的机器学习概念，并慢慢介绍了功能齐全的深度学习模型。在上一章中，我们从零开始实现了多层感知机的每个组件，然后展示了如何利用高级API轻松地实现相同的模型。为了易于学习，我们调用了深度学习库，但是跳过了它们工作的细节。在本章中，我们开始深入探索深度学习计算的关键组件，即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘，以及利用GPU实现显著的加速。这些知识将使你从基础用户变为高级用户。虽然本章不介绍任何新的模型或数据集，但后面的高级模型章节在很大程度上依赖于本章的知识。\n\n本章没有介绍新的概念，教材中的代码复现在 [这里](https://github.com/karin0018/d2l_MuLi)，欢迎指正！\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap4 多层感知机(3)","url":"/2021/12/04/d2l-4-3/","content":"# 数值稳定性和模型初始化\n\n> 到目前为止，我们实现的每个模型都是根据**某个预先指定的分布**来初始化模型的参数。直到现在，我们认为初始化方案是理所当然的，忽略了如何做出这些选择的细节。你甚至可能会觉得，初始化方案的选择并不是特别重要。相反，**初始化方案的选择在神经网络学习中起着非常重要的作用**，**它对保持数值稳定性至关重要**。\n>\n> 此外，这些选择可以与非线性激活函数的选择以有趣的方式结合在一起。**我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快**。糟糕选择可能会导致我们在训练时遇到**梯度爆炸**或梯度消失。在本节中，我们将更详细地探讨这些主题，并讨论一些有用的启发式方法。你会发现这些启发式方法在你的整个深度学习生涯中都很有用。\n\n<!--more-->\n\n## 数值稳定性\n\n神经网络的梯度：\n\n- 考虑如下的 d 层神经网络\n  $$\n  \\mathbf{h}^{(l)} = f_l (\\mathbf{h}^{(l-1)}) \\text{ 因此 } \\mathbf{o} = f_L \\circ \\ldots \\circ f_1(\\mathbf{x})\n  $$\n\n- 如果所有隐藏变量和输入都是向量，我们可以将$\\mathbf{o}$关于任何一组参数$\\mathbf{W}^{(l)}$的梯度写为下式：\n\n$$\n\\partial_{\\mathbf{W}^{(l)}} \\mathbf{o} = \\underbrace{\\partial_{\\mathbf{h}^{(L-1)}} \\mathbf{h}^{(L)}}_{ \\mathbf{M}^{(L)} \\stackrel{\\mathrm{def}}{=}} \\cdot \\ldots \\cdot \\underbrace{\\partial_{\\mathbf{h}^{(l)}} \\mathbf{h}^{(l+1)}}_{ \\mathbf{M}^{(l+1)} \\stackrel{\\mathrm{def}}{=}} \\underbrace{\\partial_{\\mathbf{W}^{(l)}} \\mathbf{h}^{(l)}}_{ \\mathbf{v}^{(l)} \\stackrel{\\mathrm{def}}{=}}.\n$$\n\n\n可以看见，我们做了很多的矩阵乘法，大量的矩阵乘法带来数值稳定性的常见两个问题：梯度爆炸和梯度消失。\n\n### 梯度爆炸\n\n参数更新过大，破坏了模型的稳定收敛。\n\ne.g. 使用 ReLU 函数作为激活函数。\n\n问题：\n\n- 值超出值域，对于16 位浮点数尤为严重\n- 对学习率敏感\n  - 学习率过大->大参数值->更大的梯度\n  - 学习率太小->训练没有进展\n  - 可能要在训练过程中不断调整学习率\n\n### 梯度消失\n\n参数更新过小，每次更新时几乎不会移动，导致无法学习。\n\ne.g. 使用 sigmoid 函数作为激活函数\n\n问题：\n\n- 梯度值变为 0，对于16 位浮点数尤为严重\n- 不管如何选择学习率，训练都没有进展\n- 对底部层尤为严重，仅仅顶部层训练的较好，无法让神经网络更深\n\n### 总结\n\n- 当数值过大或者过小时会导致数值问题\n- 常发生在深度模型中，因为其会对 n 个数累乘\n\n\n\n## 如何让训练更加稳定\n\n目标：让梯度值在合理的范围内。\n\n- 让乘法变加法\n  - ResNet，LSTM\n- 归一化\n  - 梯度归一化（均值为 0，方差为 1），梯度裁剪\n- 合理的权重初始和激活函数\n\n### 权重初始化\n\n- 在合理值区间里随机初始化参数\n- 训练开始的时候更容易有数值不稳定\n  - 远离最优解的地方损失函数表面可能很复杂\n  - 最优解附近表面可能会比较平\n\n- 使用 $N(0,0.01)$ 来初始化可能对小网络没问题，但不能保证深度神经网络\n\n#### 默认初始化\n\n在前面的部分中，我们使用正态分布来初始化权重值。如果我们不指定初始化方法，框架将使用默认的随机初始化方法，对于中等规模的问题，这种方法通常很有效。\n\n#### Xavier初始化\n\n**关键点：**\n\n- 将每层的输出和梯度都看作随机变量\n- 让他们的均值和方差都保持一致\n\n让我们看看某些*没有非线性*的全连接层输出(例如，隐藏变量)$o_{i}$的尺度分布。\n对于该层$n_\\mathrm{in}$输入$x_j$及其相关权重$w_{ij}$，输出由下式给出\n\n$$\no_{i} = \\sum_{j=1}^{n_\\mathrm{in}} w_{ij} x_j.\n$$\n\n权重$w_{ij}$都是从同一分布中独立抽取的。此外，让我们假设该分布具有零均值和方差$\\sigma^2$。请注意，这并不意味着分布必须是高斯的，只是均值和方差需要存在。现在，让我们假设层$x_j$的输入也具有零均值和方差$\\gamma^2$，并且它们独立于$w_{ij}$并且彼此独立。在这种情况下，我们可以按如下方式计算$o_i$的平均值和方差：\n\n$$\n\\begin{aligned}\n    E[o_i] & = \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij} x_j] \\\\&= \\sum_{j=1}^{n_\\mathrm{in}} E[w_{ij}] E[x_j] \\\\&= 0, \\\\\n    \\mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\\\\n        & = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\\\\n        & = \\sum_{j=1}^{n_\\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\\\\n        & = n_\\mathrm{in} \\sigma^2 \\gamma^2.\n\\end{aligned}\n$$\n\n**保持方差不变的一种方法是设置$n_\\mathrm{in} \\sigma^2 = 1$**。现在考虑反向传播过程，我们面临着类似的问题，尽管梯度是从更靠近输出的层传播的。使用与正向传播相同的推理，我们可以看到，除非$n_\\mathrm{out} \\sigma^2 = 1$，否则梯度的方差可能会增大，其中$n_\\mathrm{out}$是该层的输出的数量。这使我们进退两难：我们不可能同时满足这两个条件。相反，我们只需满足：\n\n$$\n\\begin{aligned}\n\\frac{1}{2} (n_\\mathrm{in} + n_\\mathrm{out}) \\sigma^2 = 1 \\text{ 或等价于 }\n\\sigma = \\sqrt{\\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}}.\n\\end{aligned}\n$$\n\n\n通常，Xavier初始化从均值为零，方差$\\sigma^2 = \\frac{2}{n_\\mathrm{in} + n_\\mathrm{out}}$的高斯分布中采样权重。我们也可以利用Xavier的直觉来选择从均匀分布中抽取权重时的方差。注意均匀分布$U(-a, a)$的方差为$\\frac{a^2}{3}$。将$\\frac{a^2}{3}$代入到$\\sigma^2$的条件中，将得到初始化的建议：\n\n$$\nU\\left(-\\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}, \\sqrt{\\frac{6}{n_\\mathrm{in} + n_\\mathrm{out}}}\\right).\n$$\n\n尽管上述数学推理中，不存在非线性的假设在神经网络中很容易被违反，但Xavier初始化方法在实践中被证明是有效的。\n\n#### 检查常用激活函数\n\n线性的激活函数若想满足 Xavier初始化，则激活函数就必须为 $\\sigma(x)=x$\n\n可以对常用的激活函数做泰勒展开，看其是否与 $f(x)=x$ 低阶近似。\n\n- $sigmoid(x) = 1/2 + x/4 - x^3/48+O(x^5)$\n- $tanh(x)=0+x-x^3/3+O(x^5)$\n- $relu(x)=0+x~ for ~ x \\ge 0$\n\n可以看出 tanh 和 relu 函数可以低阶近似于 $f(x)=x$ ，但是 sigmoid 函数需要调整：\n\n$4 \\times sigmoid(x)-2$\n\n\n\n### 总结\n\n- 合理的权重初始值和激活函数的选取可以提升数值的稳定性\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap4 多层感知机(2)","url":"/2021/11/29/d2l-4-2/","content":"\n# 模型选择、欠拟合和过拟合\n\n> 作为机器学习科学家，我们的目标是发现*模式*（pattern）。但是，我们如何才能确定模型是真正发现了一种泛化的模式，而不是简单地记住了数据呢？例如，我们想要在患者的基因数据与痴呆状态之间寻找模式，其中标签是从集合 { 痴呆,轻度认知障碍,健康 } 中提取的。因为基因可以唯一确定每个个体（不考虑双胞胎），所以在这个任务中是有可能记住整个数据集的。\n>\n> 将模型在训练数据上拟合得比在潜在分布中更接近的现象称为**过拟合**（overfitting），用于对抗过拟合的技术称为**正则化**（regularization）。\n\n<!--more-->\n\n## 训练/泛化误差\n\n- 训练误差：模型在训练数据上的误差\n\n- 泛化误差：模型在新数据上的误差\n\n  \n\n## 数据集\n\n- 训练数据集：训练模型参数\n\n- 验证数据集：一个用来评估模型好坏的数据集。\n  - 例如拿出 50% 的训练数据\n  - 不要和训练数据混在一起\n  - 用来不断**调整超参数**。验证集上的精度不能准确代表模型在新数据上的泛化能力。\n- 测试数据集：只用一次的数据集。\n\n\n\n### K-则交叉验证\n\n- 背景：没有足够多的数据使用\n\n- 算法：\n\n  - 将训练数据分割成 k 块\n  - for i=1,…,k\n    - 使用第 i 块作为验证数据集，其余作为训练数据集\n\n  - 报告 k 个验证集误差的平均\n\n- 常用 k=5 或 10\n\n  \n\n## 过/欠拟合\n\n| 模型容量\\数据 | 简单   | 复杂   |\n| ------------- | ------ | ------ |\n| 低            | 正常   | 欠拟合 |\n| 高            | 过拟合 | 正常   |\n\n### 模型容量\n\n- 拟合各种函数的能力\n- 低容量的模型难以拟合训练数据\n- 高容量的模型可以记住所有的训练数据\n\n![模型复杂度对欠拟合和过拟合的影响](d2l-4-2/capacity-vs-error.svg)\n\n{% asset_img capacity-vs-error.svg This is 模型复杂度对欠拟合和过拟合的影响 image %}\n\n#### 估计模型的容量\n\n- 难以在不同的种类算法之间比较\n  - 例如树模型和神经网络\n- 给定一个模型种类，将有两个主要因素\n  - 参数的个数：例如，单层感知机和多层感知机\n  - 参数值的选择范围\n\n- VC 维\n  - 统计学习理论的一个核心思想\n  - 对于一个分类模型，VC 等于一个最大的数据集的大小，不管如何给定标号，都存在一个模型来对它进行完美分类。换句话说，模型的容量等于它能完美学习（记住）的最大的数据集\n  - 例如，二维输入的感知机，VC 维 = 3（能支持分类任意三个点，但不支持 4 个（XOR））；支持 N 维输入的感知机的 VC 维 = N+1；一些多层感知机的 VC 维 = $O(Nlog_2N)$\n  - 用处：\n    - 提供一个评价模型好坏的理论依据：可以衡量训练误差和泛化误差之间的间隔。\n    - 在深度学习中很少使用：衡量不准确，计算困难\n\n### 数据复杂度\n\n- 样本个数\n- 每个样本的元素个数\n- 时间/空间结构\n- 多样性\n- … 多个重要因素\n\n\n\n### 权重衰退\n\n是一种常见的**处理过拟合**的方法。\n\n**使用均方范数作为硬性限制**\n\n- 通过限制参数值的选择范围来控制模型容量 \n  $$\n  min~l(\\mathbf{w},b) \\space subject \\space to ~ ||\\mathbf{w}||^2\\le\\theta\n  $$\n  \n- 通常不限制偏移 b（限制与否都差不多）\n\n- 小的 $\\theta$ 意味着更强的正则项\n\n**使用均方范数作为柔性限制**\n\n- 对每个 $\\theta $，都可以找到 $\\lambda$ 使得之前的目标函数等价于下面：\n  $$\n  min~l(\\mathbf{w},b)+\\frac{\\lambda}{2}||\\mathbf{w}||^2\n  $$\n\n- 超参数 $\\lambda $ 控制了正则项的重要程度\n\n  = 0：无作用\n\n  趋于无穷大，**w\\*** -> 0\n\n**参数更新法则**\n$$\n\\mathbf{w_{t+1}}=(1-\\eta\\lambda)\\mathbf{w_t}-\\eta\\frac{\\partial l(\\mathbf{w_t},b_t)}{\\partial \\mathbf{w_t}}\n$$\n\n\n- 通常 $\\eta\\lambda <1$，在深度学习中通常叫做权重衰退。\n\n### 总结\n\n- 权重衰退通过 L2 正则项使得模型参数不会过大，从而控制模型复杂度\n- 正则项权重是控制模型复杂度的超参数\n\n\n\n### 丢弃法\n\n是一种常见的**处理过拟合**的方法。\n\n**动机：**\n\n- 一个好的模型需要对输入数据的扰动鲁棒\n- 使用有噪音的数据等价于 Tikhonov 正则\n- 丢弃法：在层之间加入噪音\n\n那么关键的挑战就是如何注入这种噪声。一种想法是以一种*无偏*的方式注入噪声。这样在固定住其他层时，每一层的期望值等于没有噪音时的值。\n\n即，预期是 $E[\\mathbf{x}'] = \\mathbf{x}$。\n\n#### 定义\n\n在标准dropout正则化中，通过按保留（未丢弃）的节点的分数进行归一化来消除每一层的偏差。换言之，每个中间激活值$h$以*丢弃概率*$p$由随机变量$h'$替换，如下所示：\n$$\n\\begin{aligned}\nh' =\n\\begin{cases}\n    0 & \\text{ 概率为 } p \\\\\n    \\frac{h}{1-p} & \\text{ 其他情况}\n\\end{cases}\n\\end{aligned}\n$$\n\n根据设计，期望值保持不变，即$E[h'] = h$。\n\n#### 使用丢弃法\n\n通常作用在隐藏全连接层的输出上\n$$\n\\bf{h}=\\sigma(W_1x+b_1)\\\\\n\\bf{h'}=dropout(h)\\\\\n\\bf{o}=W_2h'+b_2\\\\\n\\bf{y}=softmax(o)\n$$\n正则项只在训练过程中使用：他们影响模型参数的更新\n\n在测试（推理）过程中，丢弃法直接返回输入，保证确定性的输出。\n\n#### 总结\n\n- 丢弃法将一些输出项随机置零来控制模型复杂度\n- 常作用在多层感知机的隐藏层输出上\n- 丢弃概率是控制模型复杂度的超参数\n\n#### 简洁实现\n\n对于高级API，我们所需要做的就是在每个全连接层之后添加一个`Dropout`层，将丢弃概率作为唯一的参数传递给它的构造函数。\n\n在训练过程中，`Dropout`层将根据指定的丢弃概率随机丢弃上一层的输出（相当于下一层的输入）。当不处于训练模式时，`Dropout`层仅在测试时传递数据。\n\n\n\n```python\nnet = nn.Sequential(nn.Flatten(),\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        # 在第一个全连接层之后添加一个dropout层\n        nn.Dropout(dropout1),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n        # 在第二个全连接层之后添加一个dropout层\n        nn.Dropout(dropout2),\n        nn.Linear(256, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\n```\n\n模型训练和测试\n\n```python\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n\n\n\n\n## Q&A\n\n1. 神经网络的优点？\n\n   > 神经网络的本质是一种简洁高效的编程语言，可以解决大规模数据的训练，可以通过卷积提取大量特征。\n\n   \n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap4 多层感知机(1)","url":"/2021/11/29/d2l-4-1/","content":"\n[本章视频地址](https://www.bilibili.com/video/BV1hh411U7gn?spm_id_from=333.999.0.0) [本章讲义地址](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/index.html)\n\n> 在本章中，我们将介绍你的第一个真正的*深度*网络。最简单的深度网络称为**多层感知机**，它们由多层神经元组成，每一层都与下面一层（从中接收输入）和上面一层（反过来影响当前层的神经元）完全相连。当我们训练大容量模型时，我们面临着过拟合的风险。因此，我们需要为你提供第一次严格的概念介绍，包括过拟合、欠拟合和模型选择。为了帮助你解决这些问题，我们将介绍**权重衰减**和**dropout等正则化技术**。我们还将讨论**数值稳定性**和**参数初始化**相关的问题，这些问题是成功训练深度网络的关键。在整个过程中，我们的目标不仅是让你掌握概念，还希望让你掌握深度网络的实践方法。在本章的最后，我们将把到目前为止所介绍的内容应用到一个真实的案例：房价预测。我们将有关于模型计算性能、可伸缩性和效率相关的问题放在后面的章节中讨论。\n\n<!--more-->\n\n# 感知机\n\n## 模型定义\n\n给定输入 **x**，权重 **w**，偏移 b，感知机输出：\n\n$$\n\\begin{aligned}\n    \\mathbf{o} & = \\sigma(<\\mathbf{w} ,\\mathbf{x}> + b), ~\n    \\sigma(x)=\\left\\{\n    \t\t\t\\begin{aligned}\n                1,& ~if~x>0 \\\\\n                -1,& ~otherwise \n                \\end{aligned}\n                \\right.\n            \\end{aligned}\n$$\n输出 1 或 -1，是一个二分类模型。\n\n## 模型训练\n\n```\n# 初始化模型参数\ninitialize w=0 and b=0\n# 如果分类错误，更新参数\nrepeat\n    if yi[<wi,xi>+b]<= 0 then\n        w <- w+yixi and b <- b+yi\n    end if\n# 直到所有样本分类正确为止\nuntil all classified correctly\n```\n\n等价于使用批量大小为 1 的梯度下降，并使用如下损失函数： $l(y,\\mathbf{x,w})=max(0,-y<\\mathbf{w,x}>)$\n\n**收敛定理：**\n\n- 数据在半径 r 内\n- 余量（margin） $\\rho$ ，对于 $||w||^2+b^2\\le1$ 这个分界面，所有数据样本都能分类正确：  $y(\\mathbf{x^T w}+b)\\ge  \\rho$ \n- 感知机保证在 $(r^2+1)/\\rho^2 $ 步后收敛\n\n## 模型缺陷\n\n感知机不能拟合 XOR 函数，它只能产生线性分割面。这导致了人工智能领域的第一个寒冬。\n\n\n\n# 多层感知机\n\n上文我们提到过，感知机（单层）不能拟合 XOR 函数。这个缺陷可以由多层感知机来解决，它的基本想法是，先通过一些不同的简单函数来从多个角度拟合数据，再用函数来综合这些简单函数的拟合结果，即，在感知机中加入隐藏层。\n\n## 隐藏层\n\n我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。要做到这一点，最简单的方法是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。\n\n我们可以把前$L-1$层看作表示，把最后一层看作线性预测器。这种架构通常称为*多层感知机*（multilayer perceptron），通常缩写为*MLP*。下面，我们以图的方式描述了多层感知机。\n\n![mlp](E:\\DeepLearning\\pytorch\\img\\mlp.svg)\n\n​    {% asset_img mlp.svg This is an multilayer perceptron image %}\n\n> 这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。\n>\n> 输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算；因此，这个多层感知机中的层数为2。\n>\n> 注意，这两个层都是全连接的。每个输入都会影响隐藏层中个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。\n\n然而，正如 之前所说，具有全连接层的多层感知机的参数开销可能会高得令人望而却步，即使在不改变输入或输出大小的情况下，也可能促使在参数节约和模型有效性之间进行权衡。\n\n\n\n## 模型定义： 从线性到非线性\n\n对于具有$h$个隐藏单元的单隐藏层多层感知机，用 $\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$ 表示隐藏层的输出，称为*隐藏表示*（hidden representations）。在数学或代码中，$\\mathbf{H}$ 也被称为*隐藏层变量*（hidden-layer variable）或*隐藏变量*（hidden variable）。\n\n- 输入：$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ 来表示 $n$ 个样本的小批量，其中每个样本具有 $d$ 个输入(特征)。\n- 隐藏层：隐藏层权重$\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d \\times h}$，隐藏层偏置$\\mathbf{b}^{(1)} \\in \\mathbb{R}^{1 \\times h}$\n- 输出层：输出层权重$\\mathbf{W}^{(2)} \\in \\mathbb{R}^{h \\times q}$，输出层偏置$\\mathbf{b}^{(2)} \\in \\mathbb{R}^{1 \\times q}$\n- 形式上，我们按如下方式计算单隐藏层多层感知机的输出$\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$：\n\n$$\n\\begin{aligned}\n    \\mathbf{H} & = \\sigma(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}), \\\\\n    \\mathbf{O} & = \\mathbf{H}\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}.\n\\end{aligned}\n$$\n\n> 若是多类分类问题，则对输出再加一层 softmax 函数。\n\n可以发现，我们在仿射变换之后对每个隐藏单元使用了**非线性**的*激活函数*（activation function）$\\sigma$。激活函数的输出（例如，$\\sigma(\\cdot)$）被称为*激活值*（activations）。若没有激活函数（或者激活函数也是线性的），则我们的多层感知机本质上和线性模型没有区别，因为对于任意权重值，我们只需合并隐藏层，便可产生具有参数$\\mathbf{W} = \\mathbf{W}^{(1)}\\mathbf{W}^{(2)}$和$\\mathbf{b} = \\mathbf{b}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}$的等价单层模型。\n\n一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型。\n\n为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，例如，$\\mathbf{H}^{(1)} = \\sigma_1(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)})$和$\\mathbf{H}^{(2)} = \\sigma_2(\\mathbf{H}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)})$，一层叠一层，从而产生更有表达能力的模型。\n\n- 超参数：1. 隐藏层数；2. 每层隐藏层的大小；\n\n\n\n## 激活函数\n\n### sigmoid 函数\n\n[**对于一个定义域在$\\mathbb{R}$中的输入，*sigmoid函数*将输入变换为区间(0, 1)上的输出**]。因此，sigmoid通常称为*挤压函数*（squashing function）：它将范围(-inf, inf)中的任意输入压缩到区间(0, 1)中的某个值：\n$$\n\\operatorname{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)},\\sigma(x)=\\left\\{\n    \t\t\t\\begin{aligned}\n                1,& ~if~x>0 \\\\\n                0,& ~otherwise \n                \\end{aligned}\n                \\right.\n$$\n\n```python\ny = torch.sigmoid(x)\n```\n\nsigmoid函数的导数为下面的公式：\n$$\n\\frac{d}{dx} \\operatorname{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\operatorname{sigmoid}(x)\\left(1-\\operatorname{sigmoid}(x)\\right).\n$$\n\n注意，当输入为0时，sigmoid函数的导数达到最大值0.25。而输入在任一方向上越远离0点，导数越接近0。\n\n```python\nx.grad.data.zero_()\ny.backward(torch.ones_like(x))\nx.grad\n```\n\n\n\n### Tanh 函数\n\n和 sigmoid 函数很像，不过是把输入投影到 (-1,1)：\n$$\n\\operatorname{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}\n$$\n\n```python\ny = torch.tanh(x)\n```\n\ntanh函数的导数是：\n\n$$\n\\frac{d}{dx} \\operatorname{tanh}(x) = 1 - \\operatorname{tanh}^2(x).\n$$\n\n当输入接近0时，tanh函数的导数接近最大值1。与我们在sigmoid函数图像中看到的类似，输入在任一方向上越远离0点，导数越接近0。\n```python\nx.grad.data.zero_()\ny.backward(torch.ones_like(x))\nx.grad\n```\n\n\n\n### ReLu 函数\n\n最受欢迎的选择是*线性整流单元*（Rectified linear unit，*ReLU*），因为它实现简单（**不用做指数运算**），同时在各种预测任务中表现良好。\n[**ReLU提供了一种非常简单的非线性变换**]。\n给定元素$x$，ReLU函数被定义为该元素与$0$的最大值：\n\n（**$$\\operatorname{ReLU}(x) = \\max(x, 0).$$**）\n\n通俗地说，ReLU函数通过将相应的激活值设为0来仅保留正元素并丢弃所有负元素。很容易理解到，激活函数是分段线性的。\n```python\ny = torch.relu(x)\n```\n\n当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。\n\n> 注意，当输入值精确等于0时，ReLU函数不可导。\n>\n> 在此时，我们默认使用左侧的导数，即当输入为0时导数为0。我们可以忽略这种情况，因为输入可能永远都不会是0。\n\n\n\n## 总结\n\n- 多层感知机使用隐藏层和激活函数得到非线性模型\n- 常用的激活函数是 Sigmoid，Tanh，ReLU\n- 使用 softmax 来处理多类分类问题\n- 超参数是隐藏层数，和各个隐藏层的大小。\n\n\n\n## Q&A\n\n1. 神经网络为什么是增加深度而不是增加神经元的个数？\n\n   > 每层的神经元过多会有过拟合的风险，而且增加深度的模型训练更简单，效果更好。\n\n2. 激活函数的本质是什么？\n\n   > 激活函数的本质是引入非线性性。所以选择什么样的激活函数本质上没有区别，一切从简就好（ReLU 大法好）。\n\n3. 如何选择多层感知机的层数和每层神经元的个数？\n\n   > 基本方法：由简到繁，挨个试（开始炼丹）\n\n   \n\n\n\n## 简洁实现\n\n```python\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l\n\n# 模型\nnet = nn.Sequential(\n    nn.Flatten(),nn.Linear(784, 256),nn.ReLU(),nn.Linear(256, 10))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\nnet.apply(init_weights);\n\n# 训练\nbatch_size, lr, num_epochs = 256, 0.1, 10\nloss = nn.CrossEntropyLoss()\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\n\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n```\n\n\n\n### train_ch3\n\n\n\n```python\nclass Accumulator:  #@save\n    \"\"\"在`n`个变量上累加。\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ndef evaluate_accuracy(net, data_iter):  #@save\n    \"\"\"计算在指定数据集上模型的精度。\"\"\"\n    if isinstance(net, torch.nn.Module):\n        net.eval()  # 将模型设置为评估模式\n    metric = Accumulator(2)  # 正确预测数、预测总数\n    for X, y in data_iter:\n        metric.add(accuracy(net(X), y), y.numel())\n    return metric[0] / metric[1]\n\n    \ndef train_epoch_ch3(net, train_iter, loss, updater):  #@save\n    \"\"\"训练模型一个迭代周期（定义见第3章）。\"\"\"\n    # 将模型设置为训练模式\n    if isinstance(net, torch.nn.Module):\n        net.train()\n    # 训练损失总和、训练准确度总和、样本数\n    metric = Accumulator(3)\n    for X, y in train_iter:\n        # 计算梯度并更新参数\n        y_hat = net(X)\n        l = loss(y_hat, y)\n        if isinstance(updater, torch.optim.Optimizer):\n            # 使用PyTorch内置的优化器和损失函数\n            updater.zero_grad()\n            l.backward()\n            updater.step()\n            metric.add(float(l) * len(y), accuracy(y_hat, y),\n                       y.size().numel())\n        else:\n            # 使用定制的优化器和损失函数\n            l.sum().backward()\n            updater(X.shape[0])\n            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n    # 返回训练损失和训练准确率\n    return metric[0] / metric[2], metric[1] / metric[2]\n\n\nclass Animator:  #@save\n    \"\"\"在动画中绘制数据。\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        # 增量地绘制多条线\n        if legend is None:\n            legend = []\n        d2l.use_svg_display()\n        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes, ]\n        # 使用lambda函数捕获参数\n        self.config_axes = lambda: d2l.set_axes(\n            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        # 向图表中添加多个数据点\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        self.axes[0].cla()\n        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x, y, fmt)\n        self.config_axes()\n        display.display(self.fig)\n        display.clear_output(wait=True)\n        \ndef train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save\n    \"\"\"训练模型（定义见第3章）。\"\"\"\n    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n                        legend=['train loss', 'train acc', 'test acc'])\n    for epoch in range(num_epochs):\n        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n        test_acc = evaluate_accuracy(net, test_iter)\n        animator.add(epoch + 1, train_metrics + (test_acc,))\n    train_loss, train_acc = train_metrics\n    assert train_loss < 0.5, train_loss\n    assert train_acc <= 1 and train_acc > 0.7, train_acc\n    assert test_acc <= 1 and test_acc > 0.7, test_acc\n```\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap3 线性神经网络(2)","url":"/2021/11/26/d2l-3-2/","content":"# softmax 回归\n[本章视频地址](https://www.bilibili.com/video/BV1K64y1Q7wu?p=1)，[本章讲义地址](https://zh-v2.d2l.ai/chapter_linear-networks/index.html)\n\n>遇到分类问题时，我们可以使用 softmax 方法将模型的输出作为概率，我们将优化参数以最大化观测数据的概率。为了得到预测结果，我们将设置一个阈值，如选择具有最大概率的标签。\n\n<!-- more -->\n\n**回归 vs 分类**\n\n- 回归估计一个连续值\n- 分类预测一个离散类别\n\n回归：\n\n- 单连续值输出\n- 自然区间 R\n- 跟真实值的区别作为损失\n\n分类：\n\n- 通常多个输出\n- 输出 i 时预测为第 i 类的置信度\n\n**分类 -> 回归**\n\n统计学家使用 **独热编码** 的形式来表示类别数据。\n\n- *独热编码*（one-hot encoding）。独热编码是一个向量，它的分量和类别一样多。类别对应的分量设置为1，其他所有分量设置为0。\n\n  例如，将用一个三维向量标签 $y$ 表示【猫，鸡，狗】中的一种动物，那么可以有如下对应关系：$(1, 0, 0)$对应于“猫”、$(0, 1, 0)$对应于“鸡”、$(0, 0, 1)$对应于“狗”：\n\n  $$y \\in \\{(1, 0, 0), (0, 1, 0), (0, 0, 1)\\}.$$\n\n## softmax 算子\n\nsoftmax由三个步骤组成：\n（1）对每个项求幂（使用`exp`）；\n（2）对每一行求和（小批量中每个样本是一行），得到每个样本的归一化常数；\n（3）将每一行除以其归一化常数，确保结果的和为1。\n$$\n\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{其中}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}\n$$\n\n```python\ndef softmax(X):\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(1, keepdim=True)\n    return X_exp / partition  # 这里应用了广播机制\n```\n\n\n\n## 模型\n\n\n假设我们读取了一个批量的样本$\\mathbf{X}$，其中特征维度（输入数量）为 $d$，批量大小为 $n$。此外，假设我们在输出中有 $q$ 个类别。那么小批量特征为 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，权重为$\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$，偏置为$\\mathbf{b} \\in \\mathbb{R}^{1\\times q}$。softmax 回归的矢量计算表达式为：\n\n$$\n\\begin{aligned} \\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\ \\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O}). \\end{aligned}\n$$\n相对于一次处理一个样本，小批量样本的矢量化加快了$\\mathbf{X}和\\mathbf{W}$的矩阵-向量乘法。\n\n注：\n\n> 由于$\\mathbf{X}$中的每一行代表一个数据样本，所以 softmax 运算可以*按行*（rowwise）执行：对于$\\mathbf{O}$的每一行，我们先对所有项进行幂运算，然后通过求和对它们进行标准化。\n> 小批量的未归一化预测$\\mathbf{O}$和输出概率$\\hat{\\mathbf{Y}}$都是形状为 $n \\times q$ 的矩阵。\n\n```python\ndef net(X):\n    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)\n```\n\n\n\n\n\n## 损失函数\n\n用来衡量真实值和预测值之间的区别，他的导数指明参数的更新方向。\n\n### L2 loss 均方误差\n\n$l(y,y')=\\frac{1}{2}(y-y')^2$\n\n- 离原点较远的数据更新梯度较大，随着靠近原点，梯度慢慢变小。\n\n### L1 loss 绝对值\n\n$l(y,y')=|y-y'|$\n\n- 所有数据更新的梯度值都是常数，大部分情况下比 L2 更稳定。\n- 缺点：绝对值函数在 0 点不可导，预测值和真实值之间相距过近的时候，导数值会在 -1 和 1 之间震荡（不稳定）\n\n### Huber‘s Robust Loss\n\nL1 和 L2 取长补短：\n\n$if~|y-y'|>1:l(y,y')=|y-y'|-\\frac{1}{2}$;  \n\n$otherwise:\\frac{1}{2}(y-y')^2$\n\n\n\n### 交叉熵损失\n\n####  对数似然\n\nsoftmax函数给出了一个向量$\\hat{\\mathbf{y}}$，我们可以将其视为给定任意输入$\\mathbf{x}$的每个类的估计条件概率。例如，$\\hat{y}_1$=$P(y=\\text{猫} \\mid \\mathbf{x})$。假设整个数据集$\\{\\mathbf{X}, \\mathbf{Y}\\}$具有$n$个样本，其中索引$i$的样本由特征向量$\\mathbf{x}^{(i)}$和独热标签向量$\\mathbf{y}^{(i)}$组成。我们可以将估计值与实际值进行比较：\n\n$$\nP(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).\n$$\n\n根据最大似然估计，我们最大化$P(\\mathbf{Y} \\mid \\mathbf{X})$，相当于最小化负对数似然（最大化似然估计等同于最小化损失函数）：\n\n$$\n-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),\n$$\n\n\n**疑问 ：第二个等号为什么成立？？？？？**\n\n其中，对于任何标签$\\mathbf{y}$和模型预测$\\hat{\\mathbf{y}}$，损失函数为：\n$$\nl(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j.\n$$\n:eqlabel:`eq_l_cross_entropy`\n\n在本节稍后的内容会讲到， :eqref:`eq_l_cross_entropy`中的损失函数通常被称为*交叉熵损失*（cross-entropy loss）。由于$\\mathbf{y}$是一个长度为$q$的独热编码向量，所以除了一个项以外的所有项$j$都消失了。由于所有$\\hat{y}_j$都是预测的概率，所以它们的对数永远不会大于$0$。\n因此，如果正确地预测实际标签，即，如果实际标签$P(\\mathbf{y} \\mid \\mathbf{x})=1$，则损失函数不能进一步最小化。\n注意，这往往是不可能的。例如，数据集中可能存在标签噪声（某些样本可能被误标），或输入特征没有足够的信息来完美地对每一个样本分类。\n\n#### 交叉熵\n- 交叉熵用来衡量两个概率的区别： $H[p,q] = \\sum_j - p(j) \\log q(j).$\n\n- 将他作为损失： $l[y,\\hat y] = \\sum_j - y(j) \\log \\hat y(j)=-log~\\hat y_y$\n\n  只关心正确类的置信值是多大\n\n- 其梯度是真实概率和预测概率的区别： \n  $$\n  \\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j.\n  $$\n\n  \n\n```python\ny = torch.tensor([0, 2])\ny_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n\ndef cross_entropy(y_hat, y):\n    return - torch.log(y_hat[range(len(y_hat)), y])\n\ncross_entropy(y_hat, y)\n```\n\n\n\n## softmax 回归的简洁实现\n\n1. 导入包和数据集\n\n ```python\n import torch\n from torch import nn\n from d2l import torch as d2l\n \n batch_size = 256\n train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n ```\n\n2. 初始化模型参数\n\n   softmax 回归的输出层是一个**全连接层**\\*。\n\n   > 全连接层的每一个节点都与上一层的所有结点相连，用来把前面提取到的特征综合起来。基于这个特性，全连接层的参数也是最多的。\n   > 具体来说，对于任何具有$d$个输入和$q$个输出的全连接层，参数开销为$\\mathcal{O}(dq)$，在实践中可能高得令人望而却步。\n   幸运的是，将$d$个输入转换为$q$个输出的成本可以减少到$\\mathcal{O}(\\frac{dq}{n})$，其中超参数$n$可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性 \n\n   ```python\n   # PyTorch不会隐式地调整输入的形状。因此，\n   # 我们在线性层前定义了展平层（flatten），来调整网络输入的形状\n   # flatten: 将任意维度的 tensor 都转为 2-维 tensor，第0维度保留，其他维度展开成向量\n   # 输入：784，输出：10\n   net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n   \n   def init_weights(m):\n       if type(m) == nn.Linear:\n           nn.init.normal_(m.weight, std=0.01)\n   \n   net.apply(init_weights);\n   ```\n\n   \n\n3. 在交叉熵损失函数中传递未归一化的预测，并同时计算 softmax 及其对数\n\n   ```python\n   loss = nn.CrossEntropyLoss()\n   ```\n\n     \n\n4. 使用学习率为0.1的**小批量随机梯度下降**作为**优化算法**\n\n   ```python\n   trainer = torch.optim.SGD(net.parameters(), lr=0.1)\n   ```\n\n   \n\n5. 训练\n\n   ```python\n   num_epochs = 10\n   d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)\n   ```\n\n   \n\n   \n\n## 问题记录\n\n1. softmax 和 logistic 回归分析是一样的吗，如果不一样，区别在哪里？\n\n   > logistic 是二分类的预测，是 softmax 的一个特例。\n\n2. 为什么使用交叉熵作为损失函数，而不用相对熵/互信息等其他基于信息量的度量？\n\n   > 我们的目标：需要量化两个概率之间的区别，这些函数理论上都能做到。\n   >\n   > 不用其他的函数是因为交叉熵是最好计算的，在此之外没有什么区别。\n   >\n   > 互信息 $I(p,q)=I(q,p)$，交叉熵 $H(p,q)!=H(q,p)$\n\n3.  极大似然估计和交叉熵损失函数之间的关系？？？\n\n   \n\n# python 语法点\n\n记录一些不太熟悉的 python 语法知识点，方便回顾。\n\n## zip() 函数\n\n### 描述\n\n**zip()** 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。\n\n如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。\n\n> zip 方法在 Python 2 和 Python 3 中的不同：在 Python 3.x 中为了减少内存，zip() 返回的是一个对象。如需展示列表，需手动 list() 转换。\n>\n> 如果需要了解 Python3 的应用，可以参考 [Python3 zip()](https://www.runoob.com/python3/python3-func-zip.html)。\n\n### 语法\n\nzip 语法：\n\n```\nzip([iterable, ...])\n```\n\n参数说明：\n\n- iterable -- 一个或多个迭代器;\n\n### 返回值\n\n返回元组列表。\n\n### 实例\n\n以下实例展示了 zip 的使用方法：\n\n```python\n>>>a = [1,2,3] \n>>> b = [4,5,6] \n>>> c = [4,5,6,7,8] \n>>> zipped = zip(a,b)     # 打包为元组的列表\n[(1, 4), (2, 5), (3, 6)] \n>>> zip(a,c)              # 元素个数与最短的列表一致\n[(1, 4), (2, 5), (3, 6)] \n>>> zip(*zipped)          # 与 zip 相反，*zipped 可理解为解压，返回二维矩阵式 \n[(1, 2, 3), (4, 5, 6)]\n```\n\n\n\n\n\n## isinstance() 函数\n\nisinstance() 函数来判断一个对象是否是一个已知的类型，类似 type()。 \n\n> isinstance() 与 type() 区别：\n>\n> - type() 不会认为子类是一种父类类型，不考虑继承关系。\n> - isinstance() 会认为子类是一种父类类型，考虑继承关系。\n>\n> 如果要判断两个类型是否相同推荐使用 isinstance()。\n\n### 语法\n\n以下是 isinstance() 方法的语法:\n\n```\nisinstance(object, classinfo)\n```\n\n### 参数\n\n- object -- 实例对象。\n- classinfo -- 可以是直接或间接类名、基本类型或者由它们组成的元组。\n\n### 返回值\n\n如果对象的类型与参数二的类型（classinfo）相同则返回 True，否则返回 False。\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"chap3 线性神经网络(1)","url":"/2021/11/22/deeplearning-0/","content":"\n\n\n[本章视频地址](https://www.bilibili.com/video/BV1PX4y1g7KC?spm_id_from=333.999.0.0)，[本章讲义地址](https://zh-v2.d2l.ai/chapter_linear-networks/index.html)\n\n> 在介绍深度神经网络之前，我们需要了解神经网络训练的基础知识。在本章中，我们将介绍神经网络的整个训练过程，包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型。经典统计学习技术中的线性回归和 softmax 回归可以视为*线性*神经网络。为了更容易学习，我们将从这些经典算法开始，向你介绍神经网络的基础知识。这些知识将为本书其他部分中更复杂的技术奠定基础。\n\n<!--more-->\n\n# 线性回归 (linear regression)\n\n线性回归 (linear regression) 基于几个简单的假设：\n\n**首先**，假设自变量 $x$ 和因变量 $y$ 之间的关系是线性的，即 $y$ 可以表示为 $x$ 中元素的加权和，这里通常允许包含观测值的一些噪声；**其次**，我们假设任何噪声都比较正常，如噪声遵循正态分布。\n\n\n\n## 线性模型\n\n线性假设指目标可以表示为特征的加权和：$y=w_1x_1+w_2x_2+w_3x_3 +...+ b$\n\n$w_i$ 称为权重（weight），$b $称为*偏置*（bias），或称为*偏移量*（offset）、*截距*（intercept）。权重决定了每个特征对我们预测值的影响。偏置是指当所有特征都取值为 0 时，预测值应该为多少。\n\n对于特征集合$X$，预测值 $\\hat y \\in R_n$ 可以通过**矩阵-向量**乘法表示为：$\\hat y=Xw+b$\n\n训练数据：$X=[x_1,x_2,...,x_n]^T,y=[y_1,y_2,...y_n]^T$\n\n需要学习的参数：$W=[w_1,w_2,...,w_n]^T,b$\n\n**线性模型可以看作是单层神经网络**\n\n```mermaid\ngraph BT\n\to1\n\tx1-->o1\n\tx2-->o1\n\tx3[...]-->o1\n\txd-->o1\n```\n\n```python\ndef linreg(X, w, b): \n    \"\"\"线性回归模型。\"\"\"\n    return torch.matmul(X, w) + b\n```\n\n\n\n## 损失函数\n\n**定义损失** -> 平方损失：$l(y,y')=\\frac{1}{2}(y-\\hat y)^2$\n\n**损失函数**-> 在训练集的 n 个样本上的损失均值：$L(w,b)=\\frac{1}{n}∑_{i=1}^{n}l^{(i)}(w,b)=\\frac{1}{2n}∑_{i=1}^{n}(w^⊤x^{(i)}+b−y{(i)})^2$\n\n**训练过程** -> 最小化损失来学习参数：$w^*,b^*=arg~min_{w,b} ~ l(\\textbf{X,y,w},b)$\n\n\n\n```python\ndef squared_loss(y_hat, y):  #@save\n    \"\"\"均方损失。\"\"\"\n    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n```\n\n\n\n\n\n## 解析解（显示解）\n\n解析解指可以通过数学公式简单地表达出来的解。\n\n1. 将偏差加入权重，合并方法是在包含所有参数的矩阵中附加一列\n\n2. 损失是凸函数，所以梯度值为 0 的地方就是最优解。\n   我们的预测问题是最小化$\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2$。这在损失平面上只有一个临界点，这个临界点对应于整个区域的损失最小值。将损失关于 $\\mathbf{w}$ 的导数设为0，得到解析解（闭合形式）：\n   $$\n   \\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}\n   $$\n   \n\n\n> 像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。解析解可以进行很好的数学分析，但解析解的限制很严格，导致它无法应用在深度学习里。\n\n\n\n## 基础优化方法\n\n当一个问题没有最优解时（NPC问题），应该使用什么方法来优化模型的预测结果（即学习参数）呢？\n\n下面介绍几种基础的优化方法 :)\n\n### 梯度下降\n\n*梯度下降*（gradient descent）方法几乎可以优化所有深度学习模型。它通过不断地在**损失函数递减的方向**上**更新参数**来降低误差。\n\n#### 优化过程\n\n1. 挑选（如，随机初始化）待学习参数的初始值 $w_0$\n\n   ```python\n   w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)#需要计算梯度\n   b = torch.zeros(1, requires_grad=True)\n   ```\n\n2. 重复迭代参数 t=1,2,3…\n\n   $w_t=w_{t-1}-\\eta \\frac{\\partial l}{\\partial  w_t-1}$\n\n   - 沿着梯度方向将增加损失函数的值，负梯度方向即为损失函数值减小的方向。\n\n   - 学习率 $\\eta$（learning rate）：步长的超参数，人为选定，不能太大也不能太小。\n\n     > 太大：结果震荡，不收敛；太小：计算梯度的成本太高。\n\n   - $l$ ：损失函数\n\n   \n\n**小批量随机梯度下降**：\n\n梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度）。**但实际中的执行可能会非常慢**：因为在每一次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候**随机抽取一小批样本**，这种变体叫做*小批量随机梯度下降*（minibatch stochastic gradient descent）。\n\n```python\n# 随机选取小批量样本\ndef data_iter(batch_size, features, labels):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    # 这些样本是随机读取的，没有特定的顺序\n    random.shuffle(indices)\n    for i in range(0, num_examples, batch_size):\n        batch_indices = torch.tensor(\n            indices[i: min(i + batch_size, num_examples)])\n        yield features[batch_indices], labels[batch_indices]\n\nbatch_size = 10\nfor X, y in data_iter(batch_size, features, labels):\n    print(X, '\\n', y)\n    break\n```\n\n\n\n```python\ndef sgd(params, lr, batch_size):  \n    \"\"\"小批量随机梯度下降。\"\"\"\n    with torch.no_grad():\n        # 参数更新时不需要计算梯度\n        for param in params:\n            param -= lr * param.grad / batch_size\n            # 梯度手动设置为0，s.t.下一维参数梯度的计算与上一维度不相关\n            param.grad.zero_()\n```\n\n\n\n**总结：**\n\n- 梯度下降通过不断沿着反梯度方向更新参数求解\n- 小批量随机梯度下降是深度学习默认的求解算法\n- 两个重要的超参数是批量大小和学习率\n\n\n\n## 线性回归的简洁实现\n\n\n\n```python\nimport numpy as np\nimport torch\nfrom torch.utils import data\nfrom d2l import torch as d2l\n\n\"\"\"生成数据集\"\"\"\ntrue_w = torch.tensor([2, -3.4])\ntrue_b = 4.2\nfeatures, labels = d2l.synthetic_data(true_w, true_b, 1000)\n\n\"\"\"读取数据集\"\"\"\ndef load_array(data_arrays, batch_size, is_train=True):  #@save\n    \"\"\"构造一个PyTorch数据迭代器。\"\"\"\n    dataset = data.TensorDataset(*data_arrays)\n    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n\nbatch_size = 10\ndata_iter = load_array((features, labels), batch_size)\n# 为了验证是否正常工作，让我们读取并打印第一个小批量样本\nnext(iter(data_iter))\n\n\"\"\"1. 定义模型\"\"\"\n\n# `nn` 是神经网络的缩写\nfrom torch import nn\n\n# 指定输入和输出维度\nnet = nn.Sequential(nn.Linear(2, 1))\n\n\"\"\"2. 初始化模型参数\"\"\"\nnet[0].weight.data.normal(0,0.01)\nnet[0].bias.data.fill_(0)\n\n\"\"\"3. 定义损失函数：均方误差 MSE\"\"\"\nloss = nn.MSELoss()\n\n\"\"\"4. 定义优化算法：随机梯度下降 SGD\"\"\"\ntrainer = torch.optim.SGD(net.parameters(), lr=0.03)\n\n\"\"\"5. 训练\"\"\"\nnum_epochs = 3 # 迭代三个周期\nfor epoch in range(num_epochs):\n    for X, y in data_iter:\n        l = loss(net(X) ,y)\n        trainer.zero_grad() # 梯度清零\n        l.backward()\n        trainer.step() # 模型更新\n    l = loss(net(features), labels)\n    print(f'epoch {epoch + 1}, loss {l:f}')\n\n\"\"\"打印误差\"\"\"\nw = net[0].weight.data\nprint('w的估计误差：', true_w - w.reshape(true_w.shape))\nb = net[0].bias.data\nprint('b的估计误差：', true_b - b)\n```\n\n\n\n### 小结\n\n* 我们可以使用PyTorch的高级API更简洁地实现模型。\n* 在PyTorch中，`data`模块提供了数据处理工具，`nn`模块定义了大量的神经网络层和常见损失函数。\n* 我们可以通过`_`结尾的方法将参数替换，从而初始化参数。\n\n\n\n## 问题记录\n\n1. 损失函数为什么用平方损失而不是绝对插值？\n\n   > 二者没有绝对区别，平方损失更便于求导。\n\n2. 损失为什么要求平均？\n\n   > 不是绝对要求平均，若不除以 n，梯度值会过大；\n   >\n   > 求平均的好处在于，计算出的样本梯度大小不会受到样本数量的影响，这样调整学习率时会更方便。\n\n3. 随机梯度下降中的**随机**指什么？\n\n   > 指随机采取一定量的样本。\n\n\n","tags":["DeepLearning"],"categories":["跟李沐学 AI"]},{"title":"Attention Is All You Need","url":"/2021/11/19/transformer/","content":"\n本文为 [transformer模型原论文](https://arxiv.org/abs/1706.03762)的学习笔记。\n\n## 预备知识\n\n**CNN，RNN，LSTM都是什么？**\n\n[参考文章](https://cloud.tencent.com/developer/article/1523622)\n\n<!-- more -->\n#### **卷积神经网络（Convolutional Neural Network, CNN）**\n\nCNN 是一种前馈神经网络，通常由一个或多个卷积层（Convolutional Layer）和全连接层（Fully Connected Layer，对应经典的 NN）组成，此外也会包括池化层（Pooling Layer）。\n\nCNN 的结构使得它易于利用输入数据的二维结构。\n\n> 注意：前馈神经网络（Feedforward NN）指每个神经元只与前一层的神经元相连，数据从前向后单向传播的 NN。其内部结构不会形成有向环（对比后面要讲到的 RNN/LSTM）。 它是最早被发明的简单 NN 类型，前面讲到的 NN、DNN 都是前馈神经网络。\n\n每个卷积层由若干卷积单元组成——可以想象成经典 NN 的神经元，只不过激活函数变成了卷积运算。\n\n卷积运算是有其严格的数学定义的。不过在 CNN 的应用中，卷积运算的形式是数学中卷积定义的一个特例，它的目的是提取输入的不同特征。\n\n一般情况下，从直观角度来看，CNN 的卷积运算，就是下图这样：\n\n\n{% asset_img A1.gif This is an CNN image %}\n\n> 上图中左侧的蓝色大矩阵表示输入数据，在蓝色大矩阵上不断运动的绿色小矩阵叫做卷积核，每次卷积核运动到一个位置，它的每个元素就与其覆盖的输入数据对应元素相乘求积，然后再将整个卷积核内求积的结果累加，结果填注到右侧红色小矩阵中。 卷积核横向每次平移一列，纵向每次平移一行。最后将输入数据矩阵完全覆盖后，生成完整的红色小矩阵就是卷积运算的结果。\n\n\n\nCNN 结构相对简单，可以使用反向传播算法进行训练，这使它成为了一种颇具吸引力的深度学习网络模型。\n\n{% asset_img A2.jpg This is an backword image %})\n\n除了图像处理，CNN 也会被应用到语音、文本处理等其他领域。\n\n#### **循环神经网（Recurrent Neural Network，RNN）**\n\nRNN，循环神经网络，也有人将它翻译为**递归神经网络**。从这个名字就可以想到，它的结构中存在着“环”。\n\n确实，RNN 和 NN/DNN 的数据单一方向传递不同。RNN 的神经元接受的输入除了“前辈”的输出，还有自身的状态信息，其状态信息在网络中循环传递。\n\nRNN 的结构用图形勾画出来，是下图这样的：\n\n{% asset_img A3.jpg This is an RNN image %}\n\n图1\n\n> 注意：图中的 AA 并不是一个神经元，而是一个神经网络块，可以简单理解为神经网络的一个隐层。\n\nRNN 的这种结构，使得它很适合应用于序列数据的处理，比如文本、语音、视频等。这类数据的样本间存在顺序关系（往往是时序关系），每个样本和它之前的样本存在关联。\n\nRNN 把所处理的数据序列视作时间序列，在每一个时刻 $t$，每个 RNN 的神经元接受两个输入：当前时刻的输入样本 $x_t$，和上一时刻自身的输出 $h_{t-1}$\n\n$t 时刻输出：h_t=F_{\\theta}(h_{t-1},x_t)$\n\n图1经过进一步简化，将隐层的自连接重叠，就成了下图：\n\n{% asset_img A4.jpg This is an siplifiedRNN image %}\n\n图2\n\n上图展示的是最简单的 RNN 结构，此外 RNN 还存在着很多变种，比如双向 RNN（Bidirectional RNN），深度双向 RNN（Deep Bidirectional RNN）等。\n\nRNN 的作用最早体现在手写识别上，后来在语音和文本处理中也做出了巨大的贡献，近年来也不乏将其应用于图像处理的尝试。\n\n#### **长短时记忆（Long Short Term Memory，LSTM）**\n\nLSTM 可以被简单理解为是一种神经元更加复杂的 RNN，处理时间序列中当间隔和延迟较长时，LSTM 通常比 RNN 效果好。\n\n相较于构造简单的 RNN 神经元，LSTM 的神经元要复杂得多，每个神经元接受的输入除了当前时刻样本输入，上一个时刻的输出，还有一个元胞状态（Cell State），LSTM 神经元结构请参见下图：\n\n{% asset_img LSTM.jpg This is an CNN image %}\n\nLSTM 神经元中有三个门：遗忘门，输入门和输出门。\n\n遗忘门（Forget Gate)：接受 xt 和  ht-1 为输入，输出一个 0 到 1 之间的值，用于决定在多大程度上保留上一个时刻的元胞状态 ct-1。1表示全保留，0表示全放弃。\n\n输入门（Input Gate）: 用于决定将哪些信息存储在这个时刻的元胞状态 ct中。\n\n输出门（Output Gate）：用于决定输出哪些信息。\n\n\n\n![lstm-a]({% asset_img LSTM-A.jpg This is an LSTM-A image %}/LSTM-A.jpg)\n\n【LSTM 结构图】\n\n{% asset_img RNN-A.jpg This is an RNN-A.jpg image %}\n\n【RNN 结构图】\n\n> 注意：如果把 LSTM 的遗忘门强行置0，输入门置1，输出门置1，则 LSTM 就变成了标准 RNN。\n\n可见 LSTM 比 RNN 复杂得多，要训练的参数也多得多。\n\n但是，LSTM 在很大程度上缓解了一个在 **RNN 训练中非常突出的问题：梯度消失/爆炸（Gradient Vanishing/Exploding）**。这个问题不是 RNN 独有的，深度学习模型都有可能遇到，但是对于 RNN 而言，特别严重。\n\n梯度消失和梯度爆炸虽然表现出来的结果正好相反，但出现的原因却是一样的。\n\n因为神经网络的训练中用到反向传播算法，而这个算法是基于梯度下降的——在目标的负梯度方向上对参数进行调整。如此一来就要对激活函数求梯度。\n\n又因为 RNN 存在循环结构，因此激活函数的梯度会乘上多次，这就导致：\n\n- 如果梯度小于1，那么随着层数增多，梯度更新信息将会以指数形式衰减，即发生了梯度消失（Gradient Vanishing）；\n- 如果梯度大于1，那么随着层数增多，梯度更新将以指数形式膨胀，即发生梯度爆炸（Gradient Exploding）。\n\n因为三个门，尤其是遗忘门的存在，LSTM 在训练时能够控制梯度的收敛性，从而梯度消失/爆炸的问题得以缓解，同时也能够保持长期的记忆性。\n\n果然，LSTM 在语音处理、机器翻译、图像说明、手写生成、图像生成等领域都表现出了不俗的战绩。\n\n\n\n## Abstract\n\n段落大意：\n\n主流的序列转录模型主要依赖于循环或者卷积神经网络，它们一般使用 encoder-decoder 架构。在性能最好的这些模型中，通常会在 encoder 和 decoder 之间使用注意力机制。我们提出了一个新的简单的网络架构，Transformer，它仅依赖于注意力机制，而没有用循环或者卷积。通过两个机器翻译实验的验证，证明 Transformer 能比现有的模型效果更好且训练时间更快，最后写明 Transformer 也能很好的泛化到其他的任务上。\n\n总结：\n\n前人工作+本篇文章的创新点+模型的性能很棒\n\n\n\n## Conclusion\n\n段落大意：\n\n我们首次提出了一个仅依赖于注意力机制的模型，Transformer。它用 multi-head self-attention 层替换了前人用的循环层。\n\n在机器翻译的任务上，Transformer 起到了很好的效果。\n\n基于纯注意力机制的模型在机器翻译任务上取得的优越效果令人激动，我们认为他还能适用在图片/语音/视频等材料的研究任务中，making generation less sequential 也是一个新的研究目标。\n\n总结：\n\n主要贡献是提出了仅依赖于注意力机制的模型，重点在 multi-head self-attention 层。\n\n展望未来：这种方法还能泛化到其他任务中。\n\n\n\n## Introduction\n\n段落大意：\n\n> 基本是 Abstract 的扩充\n\n","tags":["论文阅读"],"categories":["跟李沐学 AI"]},{"title":"实用机器学习","url":"/2021/11/19/ML/","content":"\n\n本文为学习李沐老师[《实用机器学习》](https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358496)这门课程的笔记。\n\n<!-- more -->\n## overview\n\n1. **任务分类**\n\n    监督学习，半监督学习，无监督学习，强化学习\n\n2. **任务组成部分**\n\n    模型：给定输入，有输出\n\n    损失函数：模型预测值和真实值的差距如何定量计算\n\n    目标函数：模型训练过程中的优化目标，如，使得模型预测的值与真实值的差距尽量小\n\n    模型优化：调整参数，最小化损失\n\n3. **模型分类**：\n\n    决策树；\n\n    线性模型：根据输入的线性组合来做决定。\n\n    核方法：用不同的核函数来衡量两个样本之间的相似度，来达到非线性的效果。\n\n    神经网络；\n\n## 决策树\n\n### 优缺点\n\n优点：\n\n1. 模型可解释。\n2. 可以处理数值型和类别型的特征，可分类可回归。\n3. 不太需要调参，可以广泛应用于工业界。\n\n缺点：\n\n1. 不稳定，受数据的噪声影响大 - embedding\n\n2. 复杂树导致过拟合 - 剪枝\n\n3. 不好并行计算，性能吃亏\n\n\n\n### 随机森林\n\n**随机生成多个决策树**，决策树训练彼此独立，用投票法或加权平均做出最后决定。\n\n **如何随机训练单棵树？**\n\n1. bagging：数据随机采样，有放回。\n2. randomly：随机选取一些特征。\n\n### 梯度提升（Gradient Boosting Decision Trees ）\n\n**序列化生成若干树**。各个决策树中彼此不独立。\n\n1. 设 $t=1 … , F_t(x)$ 是前 $t-1$ 棵树的函数输出的和。\n\n2. 在 $t$ 时刻训练新的树 $f_t$，训练数据集为残差数据集：$[(x_i,y_i-F_t(x_i))]_{i=1...}$，用来预测当前模型和真实数据之间的误差。\n\n3. 那么，新的模型为： $F_{t+1}=F_t(x)+f_t(x)$\n\n> 残差等同于平均均方误差的负梯度 $- \\partial L/\\partial F$。\n>\n> L 为 F 的预测值和真实值之间的均方误差。后续的决策树都在拟合均方误差的负梯度->梯度下降。\n\n","tags":["MachineLearning"],"categories":["跟李沐学 AI"]},{"title":"如何快速读论文","url":"/2021/11/19/read-paper/","content":"\n1. 明确目的\n2. 做笔记\n\n<!-- more -->\n\n### 论文结构\n\n1. title\n2. abstract\n3. intro\n4. method\n5. exp\n6. conclusion\n\n### pass1\n\n先读摘要，然后直接看结论。\n\n再去看看实验部分的图和表，以及方法中的图表。\n\n> 这样可以快速了解到这篇文章大致做了什么，是否适合自己。\n\n### pass2\n\n顺着从 intro 往下读，**关键抓住图表**，**图中的细节都需要了解和掌握**。\n\n作者提出的方法和其他人提出的方法之间有什么区别。\n\n圈出论文中引用的，但是自己不知道的文献。\n\n> 这一步可以了解到作者具体做了什么样的工作，以及这篇文章对自己的难易程度。\n>\n> 如果发现本篇文章太难，但是又是自己感兴趣的内容，那可以去读圈出来的前人的工作。读完之后再回来读。\n\n### pass3\n\n需要知道每一句话，每个字都在讲什么。\n\n提出了什么问题，用什么方法解决这个问题，如何实现的，如果换我来我可以怎么来解决这个问题？\n\n脑补实验过程，好像自己做了这个工作。\n\n合起来文章之后能复述这个工作。","tags":["论文阅读"],"categories":["跟李沐学 AI"]},{"title":"计算机网络复习 - 无线和移动网络","url":"/2021/03/01/ComputerNetwork-chap6/","content":"\n# IEEE 802.11 无线局域网\n\n**802.11 无线局域网架构：**\n\n802.11 无线 LAN 的基本组成单元是基本服务集（BSS）\n\n<!-- more -->\n一个 BSS 包括:\n- 若干无线终端\n- 一个无线接入点 AP\n每个无线接口（终端及 AP）均有一个全局唯一的 MAC 地址\n\n  {% asset_img 1.PNG This is an 802.11 image %}\n**802.11 信道与关联：**\n\n802.11 将通信频段划分成若干信道，每个 BSS 分配一个信道：\n- 管理员安装 AP 时，为 AP 分配一个**服务集标识符（SSID）**，并选择 AP 使用的信道\n- 相邻 AP 使用的信道**可能相互干扰**\n主机**必须与一个 AP 关联**：\n- 扫描信道，监听各个 AP 发送的信标帧（包含 AP 的 SSID 和 MAC 地址）\n\n    > 主动扫描：\n    >\n    > 1. 主机广播**探测请求帧**\n    > 2. AP 发送探测**响应**帧\n    > 3. 主机从收到的探测响应中**选择一个 AP 发送关联请求**\n    > 4. AP 发送关联响应帧\n    >\n    > 被动扫描：\n    >\n    > 1. 主机监听 AP 发送的信标帧\n    > 2. 主机选择一个 AP 发送关联请求帧\n    > 3. AP 向主机发送关联响应帧\n\n- 选择一个 AP 进行关联（可能需要身份鉴别）\n\n- 使用 DHCP 获得 AP 所在子网中的一个IP地址\n\n# 802.11: 多路访问控制\n\n802.11：基于 CSMA - 发送数据前监听信道。\n\n特点：发前监听，边发边听（最小帧长），冲突避让（BEB）。\n\n- 避免与正在进行传输的其他结点冲突\n\n802.11：不能像 CSMA/CD 那样，边发送，边检测冲突。\n\n- 无线信道很难实现：无线网络中，信号强度急剧衰减。\n\n- 无法侦听到所有可能的冲突：隐藏站、信号衰落\n\n    > 例如：\n    >\n    >   {% asset_img 2.PNG This is an 802.11eg image %}\n    >\n    > A-B、B-C 之间可以正常传输，A/C 之间有障碍物，遮挡了信号的传播，导致 A-C 之间的信号无法传输，若 A,C 同时和 B 传输数据，则会出现冲突。\n\n- 目标：**避免冲突 CSMA/C(illision)A(voidance)** （只能避免数据帧的冲突）\n\n##  MAC 协议：CSMA/CA\n\n### sender/receiver 工作流程\n\n**802.11 sender：**\n\n1. if 监听信道空闲了 **DIFS** 时间 then\n\n    发送整个帧（无同时检测冲突，即 CD（collision detection）\n\n2. if 监听到信道忙 then\n\n    开始随机退避计时\n\n    信道忙，计时冻结\n\n    当信道空闲时，计时器倒计时\n\n    当计时器超时时，发送帧\n\n    **if 没有收到 ACK then**\n\n    ​\t增加随机退避的间隔时间\n\n    重复第 2 步\n\n**802.11 receiver：**\n\n- if 正确接收帧\n\n    延迟 SIFS（< DIFS） 时间之后，向发送端发送 ACK （由于存在隐藏站问题）\n\n> 802.11 利用帧间间隔时间的长短来区分数据的优先级，间隔时间越长，优先级越低。\n>\n> 例如，DIFS 比 SIFS 长，说明 数据帧（data）的优先级低于 ACK 的优先级。\n\n  {% asset_img 3.PNG This is an sender-receiver image %}\n\n### 如何实现冲突避免？\n\n**基本思想：**\n\n允许发送端 **预约** 信道，而不是随机发送数据帧，从而避免长数据帧的冲突。\n\n- 发送端首先利用  CSMA 向 BS（基站） 发送一个**很短**的 **RTS** 帧\n\n    RTS （request to send）帧仍然有可能彼此冲突，但 RTS 帧很短，\n\n- BS 广播一个 **CTS**（clear to send） 帧作为对 RTS 的响应\n\n- CTS 可以被 **所有节点** 接收\n\n    可以消除隐藏站的影响\n\n    发送端可以发送数据帧\n\n    其他结点推迟发送\n\n**利用很小的预约帧，彻底避免数据帧冲突。**\n\n举个栗子：\n\n  {% asset_img 4.PNG This is an e.g. image %}\n\n\n\n### 802.11 MAC 帧\n\n  {% asset_img 5.PNG This is an MACframe image %}\n\n  {% asset_img 6.PNG This is an MACframe2 image %}","tags":["计算机网络"],"categories":["复习笔记"]},{"title":"计算机网络复习 - 链路层","url":"/2021/02/26/ComputerNetwork-chap5/","content":"\n# 综述\n通过这一章的学习，我们需要：\n理解数据链路层服务原理。\n- 差错检测和纠正\n- 共享广播信道: 多址接入\n- 链路层编址\n- 可靠传输、流量控制：done!\n了解链路层的实现。\n- 以太网\n- 点对点协议 PPP\n    <!--more-->\n\n# 链路层简介\n\n网络层：\n\n- 选路：确定从源路由器到目的路由的路径\n- 转发：路由器将数据报从一个端口转移到另一个端口\n\n链路层：\n\n- 将数据报从一个结点传输到相邻的下一个结点。\n\n    > 源主机->源路由器 -> 下一跳路由器 -> … -> 目的路由器 -> 目的主机\n\n**一些术语**：\n\n- 节点（nodes）：主机和路由器都统称为节点\n\n- 链路（links）：连接相邻节点的通信信道\n\n    > 包括：有限链路，无线链路，局域网\n\n- 帧（frame）：链路层分组称为帧。\n\n**链路层服务：**\n\n1. 组帧（framing）：\n\n    > 封装数据报构成数据帧（添加首部和尾部）\n    >\n    > 以及从帧中解封装数据报\n\n2. 链路接入（link access）：\n\n    > 在广播信道上协调各个节点的发送行为。\n    >\n    > 帧首部的 MAC 地址，用于标识帧的源和目的（不同于 IP 地址）\n\n3. 相邻结点间的可靠交付：\n\n    > 通过确认、重传等机制确保接收节点正确收到每一个帧。\n\n4. 流量控制：\n\n    > 协调相邻的发送节点和接收\n\n5. 差错检测:\n\n    > 接收端检测到差错：通知发送端重传或者直接丢弃帧。\n\n6. 差错纠正：\n\n    > 接收端直接纠正比特差错。\n\n7. 全双工和半双工通信控制：\n\n    > 全双工：链路两端节点**同时**双向传输。\n    >\n    > 半双工：两路两端节点**交替**双向传输。\n\n**链路层在哪实现？**\n\n- 路由器：链路层内在线卡中实现。\n\n- 主机：链路层主体部分在网络适配器（网卡）中实现。\n\n    网卡连接物理媒体，所以兼具物理层功能。\n\n    >  链路层由硬件和软件实现：\n    >\n    > - 网卡中的控制器芯片：组帧、链路接入、检错、可靠交付、流量控制等。\n    > - 主机上的链路层软件：与网络层接口，激活控制器硬件、响应控制器中断等。\n\n# 差错检测和纠正\n\n  {% asset_img 1.PNG This is an error detective image %}\n\n## 如何检测与纠正错误？\n\n- 码字（codeword）：由 m 比特的数据加上 r 比特的冗余位（校验位）构成\n\n- 有效编码集：由 2m 个符合编码规则的码字组成\n\n- **检错**：若收到的码字为无效码字，判定出现传输错误\n\n- **海明距离**（Hamming Distance）：两个码字的对应位取值不同的位数\n\n- **纠错**：将收到的无效码字纠正到距其最近的有效码字\n\n    > 检错码与纠错码的能力都是有限的！\n\n- **编码集的海明距离**：编码集中任意两个有效码字的海明距离的**最小值**。\n- **检错能力**：为检测出所有 **d** 比特错误，编码集的海明距离至少应为 **d+1**\n- **纠错能力**：为纠正所有 **d** 比特错误，编码集的海明距离至少应为 **2d+1**\n\n## 奇偶校验\n\n**单比特奇偶校验**：检测单比特错误。\n\n**二维奇偶校验：**\n\n> 检测奇数位差错、部分偶数位差错。\n>\n> 纠正同一行/列的奇数位错。\n\n## Checksum 校验和\n\n发送端：\n\n- 将数据（校验内容）划分为 16 位二进制序列\n- 求和：如果遇到最高位进位 1，则返回最低位继续加\n- 校验和 = 和的反码\n- 放入分组 UDP/TCP/IP 的校验和字段\n\n接收端：\n\n- 与发送端相同算法计算\n\n- 得到的 checksum ：\n\n    16 位全 0 （或 sum 16 位为 1）： 无措\n\n    否则有错\n\n## 循环冗余校验码 CRC\n\nCRC 是一种多项式编码，它将位串看成一个一元多项式的系数。\n\n信息多项式 M(x)：由 m 个信息比特为系数构成的多项式\n\n冗余多项式 R(x)：由 r 个冗余比特为系数构成的多项式\n\n码多项式 T(x)：在 m 个信息比特后加上 r 个冗余比特构成的码字所对应的多项式，表达式为 $T(x) = x^r·M(x) + R(x)$\n\n> 模 2 除法 == 按位异或 XOR == 相同为 0 ，不同为 1\n\n生成多项式 G(x)：双方确定用来计算 R(x) 的一个多项式，最高 r 次幂。\n\n**编码方法**：$R(x) = x^r·M(x) ÷ G(x) $的余式\n\n**检验方法**：若 $T(x) ÷G(x)$ 的余式为 0，判定传输正确\n\nCRC码检错能力极强，可用硬件实现，是应用最广泛的检错码。\n\n  {% asset_img 2.PNG This is an CRC image %}\n\n**每个 CRC 标准都能检测小于 r+1 比特的突发差错。**\n\n**在适当的假设下，长度大于 r+1 比特的突发差错以概率 $1- 0.5^r$ 被检测到。**\n\n**循环码性质：**\n\n⑴由任何多于一项的生成多项式g(x)产生的循环码能够检测所有单个错误；\n\n⑵每个被(1+x)除尽的多项式都具有偶数项。若生成多项式g(x)具有偶数项，则由它产生的编码就能检测所有奇数个错误；\n\n⑶若码长n不大于生成多项式g(x)的指数e(即n≤e)，则由g(x)产生的码能够检测所有单个和两个错码；\u000bg(x)的指数e：e是使g(x)能除尽xe+1的最小正整数；\n\n⑷若码长n不大于g1(x)的指数，则由生成多项式g(x)=(x+1)g1(x)产生的码能检测所有单个、两个及三个错误；\n\n⑸由(n-m)次多项式产生的任一循环码，能检测所有长度不超过(n-m)的突发错误；\n\n⑹ 长度为b>(n-m)的突发错误中：\u000b若b=n-m+1，则不能检测部分占2 ^-(n-m-1) ；\u000b若b>n-m+1，则不能检测部分占2^ -(n-m) 。\n\n> 为什么 G = 1011 能够检测出任何单比特错误？\n>\n> - 某一位出错相当于在二进制的那一位+1，对于整个接收到的二进制数来说相当于+2^i ，2^i 不能被 G= 1001 = 9 整除，因此能检测出单比特错误\n>\n> 上述 G 能检测任何奇数比特错误吗？为什么？\n>\n> -  奇数个 1 是不能被 (11)_2 整除的（异或运算）,例如 10101 不能被 11 整除，但是 G=1001 可以被奇数个 1 整除。\n\n# 多路访问控制（MAC）协议\n\n## 链路的两种类型：\n\n1. 点到点链路：\n\n    > 仅连接了一个发送方和一个接收方的链路。\n\n2. 广播链路：\n\n    > 连接了许多节点的**单一共享链路**，任何一个节点发送的数据可被链路上的其它节点接收到。\n\n## 多址接入（Multiple Access）\n\n- 冲突：\n\n    > 在广播链路上，若两个或多个节点同时发送，发送的信号会发生干扰，导致接收失败。\n\n- 多址接入协议：\n\n    > 规定节点共享信道（谁可以发送）的方法\n    > 多址接入协议也称媒体接入控制（Medium Access Control，MAC）协议\n\n**理想的多址接入协议：**\n\n在速率为 R bps 的广播信道上\n\n1. 当只有一个节点发送时，它应能以速率 R 发送\n\n2. 当有 M 个节点发送时，每个节点应能以 R/M 的平均速率发送\n\n3. 协议是完全分布式的:\n\n    不需要一个特殊的节点来协调发送\n    不需要时钟或时隙同步\n\n4. 简单\n\n## MAC （多址接入）协议的分类\n\n**信道划分 **\n\n- 将信道划分为若干子信道，每个节点固定分配一个子信道，不会发生冲突。\n- 包括 时分多址 TDMA，频分多址 FDMA 和 CDMA 码分多址\n\n**随机接入**\n- 不划分信道，节点可自行决定何时发送；\n- 允许出现冲突，发生冲突后设法恢复。\n\n**轮流使用信道**\n- 不划分信道，有数据要发送的节点在信道上轮流发送，不会出现冲突\n\n### 随机访问 MAC 协议\n\n随机访问 MAC 协议需要定义：\n\n- 如何检测冲突\n- 如何从冲突中恢复（e.g. 通过延迟重传）\n\n#### 时隙 ALOHA 协议\n\n假定：\n\n- 所有帧大小相同\n- 时间被划分为等长的时隙，每个时隙可以传输一个帧\n- 结点只能在时隙开始时刻发送帧\n- 节点间时钟同步\n- 如果2 个或 2 个以上结点在同一时隙发送帧，结点即检测到冲突\n\n运行：\n\n- 当结点有新的帧时，在下一个时隙发送\n\n    如果无冲突，该节点可以在下一个时隙继续发送新的帧\n\n    如果有冲突，该节点在下一个时隙以概率 p 重传该帧（1-p 不发帧），直至成功\n\n优点：\n\n- 单个节点活动时，可以连续以信道全部速率传输数据\n- 高度分散化：只需同步时隙\n- 简单\n\n缺点：\n\n- 冲突 -> 浪费时隙\n- 空闲时隙\n- 节点也许能以远小于分组传输时间检测到冲突，从而可以终止传输\n- 要求节点之间时钟同步\n\n\n\n效率：当网络中存在大量活跃节点时，长期运行过程中成功时隙所占的比例\n\n- 假设 N 个结点有很多帧等待传输，每个节点在每个时隙均以概率 p 发送数据。\n\n- 对给定**一个结点**，在一个时隙将帧发送成功的概率 $p(1-p)^{N-1}$\n\n    > 这个数据帧选择发送，其余 N-1 个结点选择不发送\n\n- 给定时隙中**有节点发送成功**的概率 = $Np(1-p)^{N-1}$\n\n    > 同时是信道空闲的比例（对那个成功发送出去的帧来说，信道是空闲的）\n\n- **最大效率**\n\n    > 1. 找到令  $Np(1-p)^{N-1}$ 最大的概率 $p^* = \\frac{1}{N}$\n    >\n    > 2. 带入$Np(1-p)^{N-1}$，并令N 趋于无穷\n    >\n    >      $lim_{N-> \\infty} (1-\\frac{1}{N})^{(N-1)} = lim_{N-> \\infty} (1+\\frac{1}{-N})^{(-N)(-1)} = \\frac{1}{e} $\n    >\n    > 3. 得到，最大效率 $E = 1/e = 0.37$\n\n- **最好的情况下，信道平均被成功利用（占所有的带宽）的时间占 37%**\n\n    **此时，成功发送一个帧需要的发送次数为 $1/E = e = 2.71828... = 2.72$**\n\n\n\n#### 纯 ALOHA 协议\n\n非时隙：更加简单，无需同步\n\n当有新的帧生成时，立即发送数据\n\n**冲突可能性增大：**\n\n  {% asset_img 3.PNG This is an pure aloha image %}\n\n**所以，纯 aloha 协议的效率肯定比 aloha 协议低。**\n\n  {% asset_img 4.PNG This is an pure aloha efficiency image %}\n\n### 载波侦听多址协议 CSMA\n\n发送前监听信道：\n\n- 信道空闲：发送整个帧\n- 信道忙：推迟发送\n\n冲突仍然有可能发生：\n\n- 存在传输延迟，所以节点可能没有监听到其他结点正在发送\n- 一旦发生冲突，整个帧的传输时间被浪费\n\n### 带冲突检测的 CSMA  –  CSMA/CD\n\n- 通过测量收到的信号强度检测冲突：\n\n    冲突信号的强度较大。\n\n    > 仅适用于有线网络，如 **以太网**\n\n- 检测到冲突之后立即停止传输损坏的帧：\n\n    减少信道浪费。\n\n- 需要满足的条件：\n\n    假设网络带宽 $R~bps$  数据帧最小长度 $L_{min}(bits)$，信号传播速度 $V(m/s)$，源到目的结点的最远距离 $d_{max}$\n\n    需满足公式：$\\frac{L_{min}}{R} = RTT_{max} = \\frac{2d_{max}}{V}$\n\n     {% asset_img 5.PNG This is an CSMA/CD image %}\n\n- **效率：**\n\n    Tprop = LAN 中两个节点间的最大传播延迟\n\n    $t_{trans}$ = 最长帧传输延迟\n\n    效率 = $1/(1+5t_{prop}/t_{trans})$\n\n    prop 趋于 0，或者 trans 趋于无穷时，效率趋于 1\n\n    > 远优于 aloha 协议，且更加简单、分散。\n\n\n\n### 轮转访问 MAC 协议\n\n综合 **信道划分 MAC 协议** 和 **随机访问 MAC 协议** 的优点：\n\n1. 数据传输过程中不会出现冲突\n2. 数据传输时利用链路的全部带宽\n\n**轮询 polling**：\n\n- 主结点轮流邀请从属结点发送数据\n- 问题：\n    1. 轮询开销\n    2. 等待延迟\n    3. 单点故障风险（如果主结点宕机，则网络瘫痪）\n\n**令牌传递 token passing**：\n\n- 控制令牌依次从一个结点传递到下一个结点\n- 令牌：特殊帧，获得令牌的结点可以发送数据\n- 问题：\n    1. 令牌开销\n    2. 等待延迟\n    3. 单点故障（令牌丢失）\n\n### MAC 协议比较\n\n**信道划分MAC协议:**\n\nTDMA / FDMA / CDMA\n\n- 重负载下高效：没有冲突，节点公平使用信道\n- 轻负载下低效：即使只有一个活跃节点也只能使用1/N的带宽\n\n**随机接入MAC协议：**\n\nALOHA / CSMA / CSMA-CD （以太网） / CSMA-CA（802.11 无线局域网）\n\n- 轻负载时高效：单个活跃节点可以使用整个信道\n- 重负载时低效：频繁发生冲突，信道使用效率低\n\n**轮流协议（试图权衡以上两者）**\n\n蓝牙、FDDI、令牌环网\n\n- 按需使用信道（避免轻负载下固定分配信道的低效）\n-  消除竞争（避免重负载下的发送冲突）\n\n# 局域网 LANs\n\n**局域网 LAN（Local Area Network）**\n\n- 将小范围内的计算机及外设连接起来的网络，范围在几公里以内\n\n**城域网 MAN（Metropolitan Area Network ）**\n\n- 通常覆盖一个城市的范围（几十公里），如有线电视网、宽带无线网等\n\n- 城域网要能支持数据、音频和视频在内的综合业务，服务质量好，支持用户数量多\n\n**广域网 WAN（Wide Area Network）**\n\n- 通常覆盖一个国家或一个洲（一百公里以上），规模和容量可任意扩大\n\n## 链路层编址（MAC 地址）和 ARP 协议\n\n每一块网络适配器（网卡）固定分配一个地址，称为 **MAC地址** ，也称物理地址、硬件地址、链路层地址、**以太网地址**、**局域网地址**等。\n\n**MAC 地址长 6 个字节**，一般用由 “ : ” 或 “ - ” 分隔的 6 个十六进制数表示：\n\n- e.g. `1A-2F-BB-76-09-AD`\n\nMAC 地址由 IEEE 负责分配，**每块适配器的地址是全球唯一的**：\n\n- 网卡生产商向 IEEE 购买一块 MAC 地址空间（前 3 字节）\n\n- 生产商确保生产的每一块网卡有不同的 MAC 地址\n\n- MAC地址固化在网卡的 ROM 中\n- **MAC 地址（链路层） == 身份证号（可携带）用于标识帧；**\n- **IP 地址（网络层） == 邮政地址，存在上下级的归属关系（不可携带）用于标识数据报；**\n\n现在用软件改变网卡的 MAC 地址也是可能的\n\n### MAC 地址类型\n\n**目的 MAC 地址**有三种类型：\n\n- 单播地址：适配器的 MAC 地址，地址最高比特为0\n\n- 多播地址：标识一个多播组的逻辑地址，地址最高比特为 1\n\n- 广播地址：`ff:ff:ff:ff:ff:ff `\n\n网络适配器仅将发送给本节点的帧（单播，广播，多播）交给主机\n\n若将适配器设置成混收模式，适配器将收到的所有帧交给主机\n\n### ARP：地址解析协议\n\n**问题：在同一个 LAN 内，如何在已知目的节点的 IP 地址前提下确定其 MAC 地址？**\n\nANS：LAN 中每个 IP 结点（主机、路由器）维护一个表 – ARP 表。\n\n- 用来存储 LAN 结点的 IP/MAC 地址的映射关系：`<IP 地址; MAC 地址;TTL>`\n\n    > TTL： time to live\n    >\n    > 经过这个时间以后该映射关系会被遗弃（典型值为 20min）\n\n\n\n**ARP 的解析（获得与 IP 地址对应的 MAC 地址）过程：**\n\n**同一个局域网里的结点：**\n\n**假设 A 想知道 B 的 MAC 地址：**\n\n- **A 构造一个 ARP 请求**，在发送方字段填入自己的 MAC 地址和 IP 地址，在目标字段填入 B 的 IP 地址。\n\n- A 将 ARP 请求封装在**广播帧**中发送\n\n    > 目的 MAC 地址（广播地址） = FF-FF-FF-FF-FF-FF\n\n- 每个收到 ARP 请求的节点**用目标 IP 地址与自己的 IP 地址比较**，地址相符的节点进行响应（B响应）。\n\n- **B 构造一个 ARP 响应**，交换发送方与目标字段内容，在发送方硬件地址字段填入自己的 MAC 地址，修改操作字段为 2\n\n- B 将 ARP 响应封装在**单播帧（目的地址为A的MAC地址）**中发送。\n\n- A 在其 ARP 表中，缓存 B 的 IP-MAC 地址对，直到超时；超时后，再次刷新。\n\n- **ARP 是 即插即用 的协议：结点自主创建 ARP 表，无需干预。**\n\n\n\n**不同局域网中的结点：**\n\n  {% asset_img 6.PNG This is an LAN-LAN image %}\n\n  {% asset_img 7.PNG This is an LAN-LAN-2 image %}\n\n\n\n## 以太网 ETHERNET\n\n以太网是第一个广泛应用的局域网技术，也是目前占主导地位的有线局域网技术。\n\n比其他局域网技术简单、成本低，且能满足网络速率需求。\n\n### 以太网：物理拓扑\n\n**总线型拓扑（共享式以太网）：**\n\n- 以同电缆作为共享传输媒体（总线）\n\n- 所有节点通过特殊接口连接到这条总线上\n- 所有节点在同一冲突域（collision domain）（可能彼此冲突）\n\n**星型拓扑**：\n\n- **基于集线器（hub）的星型拓扑（共享式以太网）：**是一个**物理层设备**，把从一个端口进入的物理信号（光，电）放大后立即从其它端口输出\n\n- **基于交换机（switch）的星型拓扑（交换式以太网）：** 是一个**链路层设备**。\n\n    > 主机通过双绞线或光纤连接到交换机；\n    >\n    > 交换机在端口之间存储－转发帧；\n    >\n    > 各节点仅与中心节点直接通信，各节点之间不直接通信，**所以每个节点和交换机之间单独冲突域，结点间彼此不冲突。**\n    >\n    > 如果没有源和目的地址之间的冲突，其总带宽为单个以太网段带宽的 N 倍，此时这些主机之间能够同时进行双工通信，而不会产生干扰。\n\n### 无连接、不可靠的传输\n\n**无连接：**\n\n- 发送帧的网卡与接收端的网卡没有握手过程。\n\n**不可靠：**\n\n- 接收网卡不向发送网卡进行确认\n\n- 差错帧直接丢弃，丢弃帧中的数据恢复依靠高层协议（如，TCP），否则，发生数据丢失。\n\n\n\n**以太网的 MAC 协议：采用二进制指数退避算法的 CSMA/CD**\n\n> 在发生冲突时，用该算法来计算下一个帧的传输等待时间。\n>\n> 算法在网卡（NIC）上实现并运行。\n\n1. NIC 从网络层接收数据包，创建数据帧\n\n2. 监听信道：\n\n    如果 NIC 监听到信道空闲，则开始发送帧；\n\n    如果监听到信道忙，则一直等待到信道空闲，然后发送帧；\n\n3. 如果 NIC 检测到其他结点传输数据，则中止发送，并发送堵塞信号 jam signal。\n\n4. 中止发送后，NIC 进入**二进制指数退避**：\n\n    第 m 次连续冲突后：\n\n    - 取 n = Max(m,10)\n\n    - NIC 从 {0，1，2，…，2^n -1} 中随机选择一个数 K\n    - **NIC 等待 K\\*512 比特的传输延迟时间**，再返回第二步\n\n\t连续冲突次数越多，平均等待时间越长。\n\n\t连续 16 次冲突之后，不再尝试，直接向上级报错。\n\n   {% asset_img 8.PNG This is an ethernet image %}\n\n## 以太网交换机\n\n**链路层设备：**\n\n- 存储-转发以太网帧\n- 检验到达帧的目的 MAC 地址，**选择性**向一个或多个输出链路转发帧\n- 利用 CSMA/CD 访问链路，发送帧\n\n**透明：**\n\n- 主机感知不到交换机的存在\n\n**即插即用：**\n\n- 直接接入网络，通过自学习就可以使用。\n\n**自学习：**\n\n- 交换机无需配置\n\n### 端口转发表\n\n   {% asset_img 9.PNG This is an switch image %}\n\n   {% asset_img 10.PNG This is an switch-selflearning image %}\n\n### 帧的过滤和转发\n\n   {% asset_img 11.PNG This is an switch-selflearning-2 image %}\n\n**交换机收到帧的处理过程：**\n\n> **用帧的目的地址查找转发表（转发决策）：**\n>\n> 若目的地址所在端口 = 帧的进入端口，丢弃帧\n>\n> 若目的地址所在端口 ≠ 帧的进入端口，转发帧\n>\n> 若目的地址不在转发表中，扩散帧\n>\n> **用帧的源地址查找转发表（更新转发表）：**\n>\n> 若找到地址，将对应表项的生存期设为最大值\n>\n> 若没有找到该地址，添加源地址和进入端口到转发表，设置表项的生存期为最大值\n\n### 交换机互联\n\n   {% asset_img 12.PNG This is an switch-selflearning-3 image %}\n\n\n\n## 交换机 VS 路由器\n\n1. 两者均为存储-转发设备：\n\n    交换机：链路层设备，检测网络分组首部，转发速度快，成本低（二层设备）\n\n    路由器：网络层设备，检测链路层帧首部，转发速度慢，成本高（三层设备）\n\n2. 二者均使用转发表：\n\n    交换机：利用自学习、泛洪构建转发表，依据  MAC 地址\n\n    路由器：利用路由算法（路由协议）计算，依据 IP 地址\n\n\n\n# 虚拟局域网 VLAN\n\n**基本概念：**\n\n支持 VLAN 划分的交换机，在一个物理局域网上，配置、定义多个 VLAN。\n\n每个 VLAN 在逻辑上是一个独立的 IP 子网：\n\n- 每个 VLAN 是一个单独的广播域，一个 VLAN 中的所有帧流量被限制在该 VLAN 中\n- 不同VLAN之间的通信要依赖于网络层路由\n\n   {% asset_img 13.PNG This is an VLAN image %}\n\n\n\n**如何知道一个帧属于哪个 VLAN？**\n\n根据帧的到达端口、源 MAC 地址或源 IP 地址（由VLAN的划分方法确定），查找 VLAN的配置表获悉\n\n为避免重复查找 VLAN配置表，帧头中携带其所属的VLAN标识\n\n后续交换机通过检查帧头的VLAN标识得知这个帧所属的VLAN\n\n> 802.1Q 规定了新的以太帧格式，帧头中包含一个 VLAN 标签（tag），用于指明帧属于哪个 VLAN。\n\n\n\n**三层交换机和路由器：**\n\n不同子网或 VLAN 之间通过路由器转发，太慢！太贵！\n\n三层交换机：\n\n- 具有部分路由功能、又有二层转发速度的交换机\n\n- 专为加快大型局域网内部的数据交换而设计\n\n- 但在安全、协议支持等方面不如专业路由器\n\n三层交换机的使用：\n\n通常用在机构网络的核心层，连接不同的子网或 VLAN\n\n专业路由器：连接机构网络与外网\n\n# PPP 协议\n\n**点对点数据链路控制**：一个发送端，一个接收端，一条链路：比广播链路容易。\n\n**PPP** 是因特网中广泛使用的点到点数据链路协议，用于 PC 机到因特网的拨号连接，以及路由器到路由器之间的专线连接。\n\n**PPP 由以下三部分组成：**\n\n1. 一种在串行通信线路上的**组帧**方式，用于区分帧的边界，并**支持差错检测**。\n2. 一个用于**建立、配置、测试和拆除**数据链路的链路控制协议 LCP。\n3. 一组网络控制协议（NCP），用以**支持不同的网络层协议**。\n\n**PPP 无需支持的功能：**\n\n1. 无需差错纠正、恢复（一般由高层协议处理）\n2. 无需流量控制\n3. 不存在乱序交付\n4. 无需支持多点链路\n\n**PPP 数据帧：**\n\n   {% asset_img 14.PNG This is an VLAN image %}","tags":["计算机网络"],"categories":["复习笔记"]},{"title":"MySQL - 关系模型","url":"/2021/02/05/mysql-RelationalModel/","content":"\n\n关系模型本质上是若干个存储数据的二维表。\n\n表的每一行称为 **记录（Record）**，每一列称为 **字段（Column）**。\n\n字段定义了数据类型（整型、浮点型、字符串、日期等），以及是否允许为 NULL（表示字段数据不存在）。\n\n关系数据库中，关系是通过 **主键** 和 **外键** 来维护的。\n\n<!--more-->\n\n**RDBMS（关系数据库管理系统）的特点：**\n\n1. 数据以表格形式出现\n2. 每行为一条记录\n3. 每列为字段名称对应的数据域（每列数据类型相同）\n4. 行和列组成表单（数据矩阵）\n5. 若干表单组成 database\n\n## 主键\n\n关系表中的每一条记录都包含若干字段。同一个表的所有记录都有相同的字段定义。\n\n关系表的约束：表中任意两条记录不能重复。\n\n> 不能重复不代表不存在两条完全相同的记录，而是指当两条记录不同时，要能通过**某个字段 唯一**区分出他们。\n\n主键用来唯一定位记录，故而在记录插入表中之后，不能轻易修改。\n\n选取主键的**基本原则**：不使用任何业务相关的字段作为主键（最大程度上避免主键修改的可能性），当然也不能是 NULL。\n\n主键字段一般命名为 id，常见可作为 id 的字段类型：\n\n1. 自增整数类型：数据库会在插入数据时自动为每个记录分配一个自增整数。\n2. 全局 GUID 类型：使用一种全局唯一的字符串作为主键。类似`8f55d96b-8acc-4636-8cb8-76bf8abc2f57`。GUID算法通过网卡MAC地址、时间戳和随机数保证任意计算机在任意时间生成的字符串都是不同的，大部分编程语言都内置了GUID算法，可以自己预算出主键。\n\n> 对于大部分应用来说，通常自增类型的主键就能满足需求。\n>\n> BIGINT 自增类型最多可以记录 922亿亿条记录。\n\n\n\n## 联合主键\n\n顾名思义，联合主键就是用多个字段来唯一标识记录，这些字段共同组成主键，就叫联合主键。\n\n对于联合主键，允许有一列重复，只要不是所有主键列都重复即可。\n\n> 说白了就是互相帮助。你不行了我上，但是不能都不行。\n\n联合主键的存在会提高关系表的复杂度，所以不常用。\n\n\n\n## 外键\n\n### 从 一对多 引入\n\n一个班级对应多个学生，是典型的 “一对多” 关系。即一个`classes`的记录可以对应多个`students`表的记录。\n\n为了表达这种一对多的关系，我们需要在`students`表中加入一列`class_id`，让它的值与`classes`表的某条记录相对应。\n\n这样，我们就可以根据`class_id`这个列直接定位出一个`students`表的记录应该对应到`classes`的哪条记录。\n\n在`students`表中，通过`class_id`的字段，可以把数据与另一张表关联起来，这种列称为 **外键**。\n\n外键在数据库中的两种表现形式：\n\n1. 名副其实的外键：\n\n    定义了外键约束\n\n    ```sql\n    ALTER TABLE students\n    ADD CONSTRAINT fk_class_id\n    FOREIGN KEY (class_id)\n    REFERENCES classes (id);\n    ```\n\n    - 外键约束的名称 `fk_class_id` 可以任意。\n    - `FOREIGN KEY (class_id)` 指定了 `class_id` 作为外键。\n    - `REFERENCES classes (id)` 指定了这个外键将关联到 `classes` 表的 `id` 列（即 `classes` 表的主键）。\n\n    删除外键约束，通过 `ALTER TABLE实现`：\n\n    ```sql\n    ALTER TABLE students\n    DROP FOREIGN KEY fk_class_id;\n    ```\n\n    这里没有删除外键这一列，只是删除约束而已。删除列是通过`DROP COLUMN ...`实现的。\n\n2. 逻辑上的外键：\n\n    外键约束会降低数据库的性能，所以为了追求速度，不设置外键约束，而仅靠应用程序自身来保证逻辑的正确性。\n\n    这时 （1）中的 `class_id` 就只是普通的列，而只是在逻辑意义上起到了外键的作用而已。\n\n### 多对多\n\n多对多关系实例：一个老师对应多个班级，一个班级也可以对应多个老师。因此班级和老师之间存在多对多关系。\n\n多对多关系是通过两个一对多关系实现的：有一个中间表，关联两个一对多关系。\n\n### 一对一\n\n一对一关系：一个表的记录对应到另一个表的唯一一个记录。\n\n这样的表可以根据外键合并也可以拆分。\n\n\n\n> 综上，关系数据库通过外键可以实现一对多，多对多和一对一的关系。外键既可以通过数据库来约束，也可以不设置约束，仅靠应用陈旭的逻辑实现。\n\n\n\n## 索引\n\n在关系数据库中存有多条记录，为了快速的找到某条记录，我们使用索引。\n\n**索引**：关系数据库中对某一列或多个列的值进行预排序的数据结构。可在查找记录时直接定位到符合条件的记录，大大加快查询速度。\n\n例如，对 `students` 表：\n\n- id 学号\n- class_id 班级\n- name 姓名\n- gender 性别\n- score 成绩\n\n如果要经常根据`score`列进行查询，就可以对`score`列创建索引：\n\n```sql\nALTER TABLE students\nADD INDEX idx_score (score);\n```\n\n使用`ADD INDEX idx_score (score)`就创建了一个名称为`idx_score`，使用列`score`的索引。\n\n索引名称是任意的，索引如果有多列，可以在括号里依次写上，例如：\n\n```sql\nALTER TABLE students\nADD INDEX idx_name_score (name, score);\n```\n\n**索引的效率取决于索引列的值是否散列，对于主键，因为主键保证绝对唯一，所以使用主键索引效率最高。**\n\n**关系数据库会自动对其创建主键索引。**\n\n> 虽然索引提高了查询效率，但是在插入、更新和删除记录使，需要同时修改索引。故而索引越多，插入等操作的速度就会更慢。\n\n### 唯一索引\n\n根据业务要求，具有唯一性约束的列（例如身份证号码）可以添加唯一索引，**从而保证这一列的值具有唯一性**。例如学生的身份证号不能重复：\n\n```sql\nALTER TABLE students\nADD UNIQUE INDEX uni_pid (pid);\n```\n\n> 通过 unique 关键字添加唯一索引\n\n也可以只添加唯一约束不创建唯一索引：\n\n```sql\nALTER TABLE students\nADD CONSTRAINT uni_pid UNIQUE (pid);\n```\n\n添加索引只不过是让数据库的查询锦上添花，没有索引数据库当然也能正常运行，因此索引可以在使用数据库的过程中逐步优化。\n\n","tags":["mysql"],"categories":["教程类"]},{"title":"MySQL 安装教程 + 排坑","url":"/2021/01/27/mysql-install/","content":"**本教程基于 windows10(64位) 操作系统**\n<!--more-->\n# 下载\n\n1. 访问 [官网下载地址](https://dev.mysql.com/downloads/mysql/)\n   {% asset_img 1.PNG This is an download image %}\n\n2. 点击 `No thanks,just start my download` 跳过注册过程，直接下载压缩包。\n\n\n# 安装与配置\n\n- 在指定路径解压，将解压后 bin 文件的路径（`D:\\mysql-8.0.23-winx64\\bin`）添加到系统环境变量中：\n  > 我的电脑->属性->高级->环境变量\n  > 选择PATH,在其后面添加: 你的 mysql bin 文件夹的路径\n- 使用管理员身份打开 cmd：\n  > 在开始菜单中输入 cmd，选择用管理员身份打开；\n- 跳转到 mysql bin 目录下：\n  {% asset_img 2.PNG This is an bin path %}\n- 安装 mysql\n  ```\n  mysqld -install\n  ```\n- 初始化 mysql（一定要初始化！否则容易导致启动不成功）\n  ```\n  mysqld --initialize\n  ```\n- 启动服务\n  ```\n  net start mysql\n  ```\n  {% asset_img 3.PNG This is an start successfully image %}\n\n  > 这里直接写 mysql 是因为我主机服务中 mysql 的服务名就是 mysql，如果大家出现了启动不成功的情况可以自行搜索是不是服务名不同。\n\n- 登录 mysql：\n  ```\n  mysql -u root -p\n  ```\n  据说第一次登录不需要密码，直接按回车就好了，但是博主按了回车发现不行。出现报错 `ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)`\n  {% asset_img 4.PNG This is an start failed image %}\n  看来还是需要我们输入密码，下面一起找密码吧：\n  1. 打开 mysql 的根目录下名为 data 的文件夹；\n  2. 找到以 `.err` 为结尾的文件（专门记录报错信息的）并打开；\n  3. 找到自动生成的密码啦！\n   {% asset_img 5.PNG This is an password image %}\n  4. 输入密码即可~\n   {% asset_img 6.PNG This is an load successfully image %}\n\n# 通过 vscode 连接 mysql\n\n旨在利用 vscode 能更加便捷的编写并执行 SQL 代码。\n\n参考文章：[MySQL vscode开发环境搭建](https://zhuanlan.zhihu.com/p/347159257)\n\n## 基础知识\n\n> MySQL 相当于一个 shell，SQL 就是和 shell 交互的脚本语言。\n> 众所周知，与 shell 打交道的方式有两种：\n> - 一种是直接在shell中输命令执行，但是这样很难让我们看到多条shell的作用。\n> - 还有一种方式是编写 shell 脚本。在 bash 中，shell 脚本的文件名后缀可以是 .sh，在 MySQL 中，脚本的后缀名为 .sql\n\n## 安装插件\n\n1. 打开 vscode 插件商店；\n2. 搜索并安装如下两个插件：\n   - MySQL [by Jun Han]\n   - MySQL Syntax [by Jake Bathman]\n3. 以管理员身份打开 cmd；\n4. `net start mysql` 启动服务；\n5. `mysql -u root -p` 进入 MySQL 账户（密码的查找方式见上）\n6. 进入 mysql 的 shell 之后，输入命令：\n   ```\n    alter user 'root'@'localhost' identified with mysql_native_password by '123456';\n   ```\n   重置密码为 `123456`\n\n## vscode 连接本机数据库\n\n1. 打开 vscode ，点击左下角的 MYSQL 旁边的加号：\n   {% asset_img 7.PNG This is an load successfully image %}\n\n2. 在弹出的对话框中设置参数：\n   - host: 127.0.0.1\n   - username: root\n   - password: 123456 (刚设置的)\n   - 其他参数保持默认，一路回车下去\n\n## 一个栗子\n\n1. 在 MYSQL 下方的蓝色饼饼图标处右键，选择 new_query\n2. 在弹出的编辑栏中输入 SQL 指令，并以 `.sql` 为后缀名保存到本地\n3. 编辑区右键，选择 Run MySQL query\n4. 你会在编辑区右侧看到指令的执行结果。\n\n## 震惊！vscode 的 mysql 插件不支持过程化 sql 语句\n\n太悲惨了，博主爆哭 wwwww\n为了不放弃自己辛辛苦苦搞的 vscode ，博主进行了多方尝试，最终还是不行 wwwwwww\n大家转战 mysql workbench 吧（官网下载安装）\n\n\n","tags":["mysql"],"categories":["教程类"]},{"title":"hexo+stun 博客搭建","url":"/2021/01/26/hexo-stun/","content":"\n# 背景\n\n看到某位同学搭了自己的博客，觉得有一个记录自己成长路线的博客好像挺香的…刚好寒假时间比较充裕，是个动手的好时机~\n\nGithub Pages 功能 +  Hexo 的博客框架 + 自己喜欢的主题 = 华而有实的博客\n\n<!--more-->\n\n使用 Github Pages 搭建博客的好处：\n\n1. 免费，免费，免费！\n2. 都是静态文件，访问速度较优；\n3. 能和 git 搭配使用，管理起来都是熟悉的配方；\n\n使用 hexo 的好处：\n\n1. 简洁高效，安装简单；\n2. 有多种稳定、美观的主题可以挑选；\n3. 使用 Markdown 解析文章，符合自己平时写东西的习惯；\n\n> 当然其他框架也有各自的优点，这个选择凭各自喜好啦~\n\n**本教程以 windows 为例**\n\n\n# Github Pages\n\n**基于 [Github Pages 官方文档](https://docs.github.com/cn/github/working-with-github-pages/creating-a-github-pages-site)**\n\n前提是要有 github 账号，没有的同学们可以先去注册一个~\n\n1. 创建 Github Pages 站点：\n\n    - 新建一个仓库；\n    - 输入仓库的名称和说明（可选）。 如果您创建的是用户或组织站点，仓库名称**必须为** `<user>.github.io` 或 `<organization>.github.io` ；\n    - 设置仓库属性为 `public` ;\n    - 选择 `Initialize this repository with a README`（使用 readme 文件初始化此仓库；\n    - `create repository`\n\n2. 设置站点：\n\n    - 在站点仓库下，找到右上角 `Settings`\n\n    - 下滑，找到`Github Pages`选项卡，你能看到站点的 url 啦~\n        {% asset_img 1-1.jpg This is an github-pages image %}\n        > 这个 url 就是你网站的地址，通过它，大家就能访问到你的博客。\n\n    - Source 栏的 branch 选择 main（或者 master，总之就是除了 none 之外的那个），点击 save；\n\n    - Theme Chooser 那个不用管，因为后续要使用 hexo 的主题嘛；\n\n    - 将仓库克隆到本地，用 git 配置自己的身份信息：\n\n        ```\n        git config --global user.name \"github user name\"\n        git config --global user.email \"github user email\"\n        ```\n\n    - [添加 ssh-key](https://www.liaoxuefeng.com/wiki/896043488029600/896954117292416)（注意不要设置密码）主要是为了以后 hexo 发布比较方便。\n\n\n\n# hexo\n\n**基于 [hexo](https://hexo.io/zh-cn/docs/) 官方文档**\n\n## 安装\n\n安装 hexo 之前，先来把环境搞好：\n\n- 安装 [Node.js](http://nodejs.org/) (Node.js 版本需不低于 10.13，建议使用 Node.js 12.0 及以上版本)\n- 安装 Git （ 不过 github 那一步都搞好了，这里就不用了吧~）\n\n> 这里具体的安装细节提示可以直接参考各自官方文档，或者网上找找教程。\n\n现在可以安装 hexo。\n\n在合适的地方新建一个文件夹，用来存放自己的博客文件，比如我的博客文件都存放在`D:/blog`目录下。\n\n我使用 Windows 自带的控制台定位到`D:/blog`目录下执行以下操作：\n\n```\n$npm install -g hexo-cli\n```\n\n> 据说 linux 和 max 里是要在前面加个 sudo，不然会因为权限问题报错。\n\n装完输入 `hexo --version` 检查是否安装成功。\n\n## 建站\n\n`hexo init` 初始化文件夹；\n\n`npm install` 安装必须的插件；\n\n`hexo g` 生成静态文件；\n\n`hexo s` 将静态文件运行在本地服务器上，这个时候根据提示打开 `localhost:4000` 就能看到最基本的博客啦~\n\n`ctrl+c` 关闭本地服务器；\n\n## 与 github 连接\n\n打开 blog 根目录下的 `_config.yml` 文件，修改配置：\n\n```yml\n# URL\n## If your site is put in a subdirectory, set url as 'http://example.com/child' and root as '/child/'\nurl: https://github.com/xxx/xxx.github.io/\nroot: /xxx.github.io/\npermalink: :year/:month/:day/:title/\npermalink_defaults:\npretty_urls:\n  trailing_index: true # Set to false to remove trailing 'index.html' from permalinks\n  trailing_html: true # Set to false to remove trailing '.html' from\n\n...\n\n# Deployment\n## Docs: https://hexo.io/docs/one-command-deployment\ndeploy:\n  type: git\n  repo: git@github.com:xxx/xxx.github.io.git\n  branch: main\n```\n\n还是在 blog 根目录下：\n\n```c\nhexo g // 编译生成静态文件（每次修改完都必须重新编译）\nhexo d // 将博客发布到 github 上\n```\n\n现在可以通过 `https://xxx.github.io` 来访问你的博客啦~\n\n\n\n# 主题 - stun\n\n博主选的是 stun 这个主题，主要是看中了她 ~~可甜可盐~~  活泼大方、简洁美观 的风格，当然她不是最简洁的，最简洁的应该是 Next 这个主题了吧（\n\n这个官方文档超级全的！！！而且步骤都很详细！！！\n\n我在这里就不瞎写了，反正写的没有官网好，照着来没错的，嗯！\n\n官网指路：[stun](https://theme-stun.github.io/docs/zh-CN/guide/quick-start.html#%E5%AE%89%E8%A3%85)\n\n\nemmmm\n\n那我写点推荐和排雷：\n\n## 统计与分析推荐\n\n首推 [谷歌分析](https://theme-stun.github.io/docs/zh-CN/advanced/third-part.html#%E8%B0%B7%E6%AD%8C%E5%88%86%E6%9E%90) ！！！ 简单好用不要钱！！！\n\n来一篇好用的配置教程：[Google Analytics怎么用，谷歌分析工具使用教程](https://www.yundianseo.com/how-to-use-google-analytics/)\n\n\n\n## 评论系统推荐\n\n博主尝试了三种评论系统，各自利弊写在下面奥：\n\n1. Disqus\n\n    优点：\n\n    - 配置简单（所以博主最先选的就是这个）\n    - 完善的后台管理机制\n    - 丰富的表情可选\n    - 支持 markdown\n\n    缺点：\n\n    - 服务器在国外，不翻墙加载不出来\n\n    - 存在广告植入\n\n    - 要评论必须有 disqus  / google / Twitter/ facebook 账户\n\n\n\n2. Valine\n\n    优点：\n\n    - 配置简单（但没有 disqus 简单）\n    - 无后端，所以加载起来很快\n    - 页面设计简洁\n    - 评论不用登陆任何账户\n    - 支持 markdown\n\n    缺点：\n\n    - 页面设计过于简洁（我试验过之后才发现，是真的很简洁，白白的，也没有点赞的功能，想要拥有一个头像都要花挺多功夫\n    - 评论可以匿名（某同学说可能会有恶意评论，博主觉得他说的有道理…\n\n\n\n3. Utterances\n\n    优点：\n\n    - 配置看上去复杂但其实很简单：\n\n        将 [utterances app (opens new window)](https://github.com/apps/utterances)安装在你博客对应的 Github 仓库中。然后，按照 stun 官网的提示修改配置项即可；\n\n    - 是一个基于 github issues 的评论系统，管理方便；\n\n    - 支持 emoji 支持评论点赞\n\n    - 支持 markdown\n\n    缺点：\n\n    - 需要登录 github 账户才能评论（是缺点也是优点吧）\n\n## bug 分享\n\n### multiline key\n\n```\n message: 'can not read a block mapping entry; a multiline key may not be an implicit key at line 7, column 9:\\n'\n \"    subtitle: 'If you shed tears when you mi ... \\n\" +\n```\n\n网上大部分说是在对应位置缺了英文空格，但我不是这个错…\n\n这里要注意，错误可能不在这一行，可能出现在它前一行：\n\n```yml\ntitle: 'Karin Lv's Blog'\nsubtitle: 'If you shed tears...\n```\n\n我就是出现在它上一行的，这里单引号中间还有一个单引号，不符合语法规则了，所以报错…\n\n去掉外层的单引号就好了！\n\n### invalid characters\n\n出现非法字符，大概率是因为在配置文件里写了中文，但没有相应的设置。\n\n博主的做法比较简单粗暴：配置文件里尽量不写中文，都改成英文 QAQ\n\n> 大家可以在网上搜一下更专业的解决办法，不要学这个懒博主（\n\n","tags":["博客搭建"],"categories":["教程类"]},{"title":"计算机网络复习 - 网络层","url":"/2021/01/26/ComputerNetworkReview-Chap4/","content":"\n旨在理解网络层服务原理和熟悉因特网的网络层协议。\n<!--more-->\n\n# 综述\n\n通过学习这一章节，我们需要了解以下两方面内容：\n\n理解网络层服务原理：\n- 网络层服务模型\n- 网络层上的重要功能：转发和选路\n- 路由器工作原理\n- 选路算法\n\n因特网的网络层协议：\n- IP 协议\n- ICMP 协议\n- 选路协议：RIP,OSPF,BGP\n\n#  概念简介\n\n1. 网络层的**作用**：将报文段从**发送主机**传送到**接收主机**。\n\n> 每一台主机和路由器都运行网络层协议。\n>\n> 路由器：将分组从输入链路转发到输出链路。它运行的协议包括，网络层，链路层，物理层。\n>\n> 发送主机：将传输层报文段封装到网络层分组中，发送给边缘路由器。\n>\n> 接收主机：从边缘路由器接收分组，取出报文段交付给传输层。\n\n2. 网络层的主要功能：\n\n    选路：确定分组从源路由器到目的路由器的路径 – 利用各种**路由算法**来计算转发表。\n\n    转发：将分组从输入端口转移到合适的输出端口 – 根据转发表转运分组。\n\n    > 在传输分组之前，两个端系统需要建立连接。\n    >\n    > - 传输层连接：进程-进程，连接状态仅仅保存在端系统中，传输层服务在网络边缘实现\n    > - 网络层连接：主机-主机，连接状态保存在源主机，目的主机以及所有中间路由器上（路由器要保存转发表的嘛），网络层服务在网络核心实现。\n\n3. 网络服务模型\n\n    定义分组在发送主机与接收主机之间传输时的特性。\n\n    - 对单个分组提供的服务\n\n        >  保证交付；具有时延上界的保证交付；\n\n    - 对分组流提供的服务\n\n        > 有序交付；保证最小带宽；\n        > 保证最大时延抖动（分组端到端时延的最大差异）；\n\n        不同架构的网络提供的网络层服务可能不同，同一个网络也可以提供不同的网络层服务。\n\n# 虚电路和数据报网络\n\n两种基本的网络类型：\n\n**数据报网络**：提供网络层**无连接**服务\n\n**虚电路网络**：提供网络层**面向连接**服务\n\n## 虚电路 Virtual circuits\n\n网络层连接成为虚电路。\n\n虚电路是从源主机到目的主机的一条路径，类似电话电路，每条虚电路有唯一标识（虚电路号），每个分组应该携带虚电路号，表明所属的虚电路。\n\n> 传输**分组前建立虚电路**，传输结束后拆除虚电路;\n>\n> 每个路由器为经过它的虚电路维护状态（转发表项 - 进入端口，进入VC号，输出端口，输出VC号），分组携带 VC 号，每一次转发前用新的 VC 号替换分组中的 VC 号 ;\n>\n> 链路及路由器资源（带宽、缓存等）可以分配给虚电路，从而虚电路能提供**可预期的**网络服务。\n\n**信令协议：**用于 VC  的建立、维护与拆除。\n\n## 数据报网络\n\n- 分组携带目的主机地址，路由器按目的地址转发分组；\n\n- 路由器根据分组的目的地址转发分组，转发表记录目的地址到输出链路的映射；\n\n    > 实际上是存储**目的地址的范围**到**链路**的映射；\n    >\n    > 匹配规则：最长前缀匹配优先；\n\n- 转发表被选路模块修改，约1～5分钟更新一次；\n\n- 同一对主机之间传输的分组可能走不同的路径，从而可能重排序；\n\n## 数据报网络 VS 虚电路网络\n\n数据报网络：\n\n- 计算机之间交换数据：没有严格的时序要求；\n- 终端具有智能：将复杂的工作（如差错控制）推到网络边缘，以保持网络简单。\n\n虚电路网络：\n\n- 由电信网发展而来：严格时序和可靠性要求；\n- 终端无智能或很少智能：复杂工作由网络完成，以保持终端简单。\n\n\n\n# Internet 网络层协议\n\n## internet 网络层\n\n  {% asset_img 1.PNG This is an internet image %}\n\n## IPv4 数据报格式\n\n  {% asset_img 2.PNG This is an IP4 image %}\n\n## IP 数据报分片\n\n### 最大传输单元 MTU\n\nMTU：链路层数据帧可封装数据的上限；$MTU = MAX(data)$\n\n> 不同链路的 MTU 不同\n\n### 分组与重组\n\n大 IP 向比较小的 MTU 链路转发时，可以被分片 （fragmented）。\n\n1. 大 IP 分片成若干小 IP\n2. IP 分片在到达 **目的主机** 后重组（路由器只管分不管组装）\n\nIP 首部相关字段用于标识分片以及确定分片的顺序（便于在目的主机重组 IP 分组）\n\n> 涉及到的字段：总长度、标识、标志和片偏移字段。\n\n- 标识：每个分片必须携带与原始数据报相同的标识。\n- 标志位：\n    - MF（more fragments）：最后一个分片的MF=0，其余分片的 MF=1\n    - DF（don’t fragment）：DF=1 表示不允许对数据报分片\n- 片偏移量（13bits）：指示分片中的数据在原始数据报载荷中的位置（以 8 字节为单位）\n\n**分片的处理过程**\n\n根据报头长度 H 和输出线路的 MTU 为 M，原 IP 分组的总长度为 L 。\n\n一个最大分片可封装的数据为：$d = \\lfloor \\frac{M-20}{8} \\rfloor * 8 ~ Bytes$\n\n需要的总片数为： $n = \\lceil \\frac{L-20}{d} \\rceil$\n\n1. 将数据报的载荷划分成长度为 d 的若干片段（最后一个分片可能不足 d 字节）。\n\n2. 将原始报头加到每一个分片的前面，修改报头中的以下字段：\n\n    $\\text{总长度} = d + 20,  ~(1 <= i < n)$;   $ or = L-(n-1)d, ~(i = n)$\n\n    最后一个报头的 MF 位置 0，其余报头的 MF 位置 1\n\n    偏移量：$F_i = \\frac{d}{8} * (i-1), ~~~ 1\\le i \\le n$\n\n4. 计算头部检查和\n\n## IP 编址\n\n**接口：** 主机 or 路由器 与物理链路的边界。\n\n**IP 地址：**32bit 标识主机、路由器的接口。一般采用点分十进制标识（8位一组，转成十进制），例如：`127.0.0.1`\n\n**如何为接口分配 IP 地址？**\n\n-  IP 地址分为两部分，高位比特为网络号，低位比特为主机号。\n- **IP 子网：** IP 地址具有相同的网络号，**不跨越路由器**，可以彼此物理联通的接口。\n\n### 有类编址\n\n  {% asset_img 3.PNG This is an IP class image %}\n\n\n\nA,B,C 类 IP 地址可以用来给主机或路由器分配网络接口，但也有一些特殊情况。\n\n\n\n {% asset_img 4.PNG This is an IP special image %}\n\n### 子网划分\n\nIP 地址：\n\n- 网络号 NetID - 高位比特\n- 子网号 SubID - 原网络主机号部分比特\n- 主机号 HostID - 低位比特\n\n**如何确定是否划分了子网？利用多少位划分子网？**\n\n- **子网掩码**：形如 IP 地址。**取值：NetID、SubID 全部取 1， HostID 全部取 0**\n\n    > 例如：\n    >\n    > A 网默认子网掩码 ：`255.0.0.0`\n    >\n    > B 网默认子网掩码 ：`255.255.0.0`\n    >\n    > C 网默认子网掩码 ：`255.255.255.0`\n    >\n    > 借用 3 比特划分子网的 B 网的子网掩码：`255.255.224.0`\n\n- 对特定主机来说，前 32 位都看成网络号，即其子网掩码为 `255.255.255.255`\n\n- 子网地址 + 子网掩码 = 准确确定子网大小\n\n    > 例：将 子网 `201.2.3.0`, `255.255.255.0` 划分为等长的 4 个子网。\n    >\n    > 分析：\n    >\n    > C 网，主机号范围只有最后 8 位（一共可以有 256 个不同的主机号）\n    >\n    > $256 / 4 = 64$，所以将最后 8 位的前两位借为子网号，划分的四个子网如下：\n    >\n    > 1. `201.2.3.0    子网号(00)   255.255.255.192`\n    >\n    >     该子网的 IP 地址范围为 `201.2.3.0 ~ 201.2.3.63`\n    >\n    >     去掉子网 IP 地址(0)和该子网的广播地址(63)，**可分配的 IP 地址**为：`201.2.3.1 ~ 201.2.3.62`\n    >\n    > 2. `201.2.3.64   子网号(01)   255.255.255.192`\n    >\n    > 3. `201.2.3.128  子网号(10)   255.255.255.192`\n    >\n    > 4. `201.2.3.192  子网号(11)   255.255.255.192`\n\n#### 一个栗子\n\n{% asset_img 11.PNG This is an eg image %}\n\n> 其中 `0.0.0.0` 是一个特殊 IP ，表示所有待选 IP 都没有匹配到，这个特殊 IP 没有网络号。\n\n{% asset_img 12.PNG This is an eg-3 image %}\n\n\n\n### 无类域间路由 CIDR\n\nCIDR (Classless InterDomain Routing)：\n\n- 消除传统的 A、B、C 类地址界限：$NetID + SubID->Network~Prefix$\n\n- 融合子网地址与子网掩码，方便子网划分：`a.b.c.d/x` 其中 x 为前缀长度\n\n    > 例如：`200.23.16.0/23`\n    >\n    > C 类地址前 24 位都是网络号，但这里的 CIDR 地址的前缀只有 23 位。\n    >\n    > 实际上它是两个 C 类地址的组合：`200.23.16.0` 和 `200.23.17.0` (第24位分别为 0 和 1)\n\n- 子网 `201.2.2.3.64  255.255.255.192 -> 201.2.3.64/26`\n\n**优点**：\n\n- 提高 IPv4 地址空间分配效率\n\n- 提高路由效率\n    1. 将多个子网聚合为一个较大的子网\n    2. 构造超网（supernetting）\n    3. 路由聚合（route aggregation）\n\n#### 路由聚合\n\n路由表中符合聚合条件的若干条路由可以**合并成一条路由**：\n\n- 这些路由的前缀可以聚合成一个更短的前缀（称地址前缀）\n- 这些路由使用相同的下一跳\n- 路由聚合的过程可以递归进行\n\n  {% asset_img 5.PNG This is an CIDR image %}\n\n\n\n> 若个别路由不满足路由聚合的条件，可以给出一条聚合路由和若干条特定路由。\n>\n> **最长前缀匹配优先**：在所有匹配的路由表项中，选择前缀最长的路由表项。（避免路由黑洞现象【数据到达不了目标地址】）\n>\n>  {% asset_img 6.PNG This is an special CIDR image %}\n\n\n\n##  DHCP 协议\n\n**一个主机如何获得 IP 地址？**\n\n- 硬编码：静态配置（手动）\n\n    > 默认网关：数据报离开子网时，将要经过的路由器接口。数据报将通过这个路由器进一步转发到其他路径。\n\n- 动态主机配置协议 DHCP：自动获取（租赁）IP 地址、子网掩码、默认网关地址、缺省路由器、本地 DNS 服务器等配置信息。\n\n    > 即插即用；\n    >\n    > 允许地址重用（IP 地址采用租赁的形式，当一个主机不用时，归还 IP ，这个 IP 就可以分给其他主机使用）；\n    >\n    > 可以续租；\n\n**DHCP 工作过程：**\n\n- 主机**广播** “DHCP discover” 报文\n\n    > 寻找子网中的 DHCP 服务器\n\n- DHCP 服务器用 “DHCP offer” 报文进行**广播**响应\n\n    > 给出推荐的 IP 地址及租期、其它配置信息\n\n- 主机用 “DHCP request” 报文**广播**请求 IP 地址\n\n    > 主机选择一个 DHCP 服务器，向其请求 IP 地址\n\n- DHCP 服务器用“DHCP ack” 报文发送IP地址\n\n    > 响应客户的请求，确认所要求的参数\n\n- DHCP 服务器使用 UDP 端口 67，客户使用 UDP 端口 68\n\n**DHCP 的实现：**\n\n- 在应用层实现\n\n- 请求报文封装到 UDP 数据报中\n\n    > 封装 ：DHCP 应用层 -> UDP 传输层 -> IP 网络层-> Eth 链路层 -> Phy 物理层\n\n- IP 广播 -> 链路层广播（e.g. 以太网广播）\n\n## 网络地址转换 NAT\n\n**动机：**\n\n- IP地址支持许多用户同时上网\n- 仅为公共可访问的节点分配公用IP地址（减少需要的公用IP地址数）\n- 网络内部节点对外是不可见的（安全考虑）\n\n**NAT 实现：**\n\n- **外出的数据报**:  将数据报中的（源IP地址，源端口号）替换为（NAT IP地址，新端口号）\n- **NAT 转换表**：记录每个（源IP地址，源端口号）与（NAT IP地址，新端口号）的转换关系\n- **进入的数据报**: 取出数据报中的（目的IP地址，目的端口号）查找NAT转换表，然后用转换表中对应的（IP地址，端口号）进行替换\n\n**16比特端口号**:\n\n允许一个 NAT IP 地址同时支持65535个对外连接\n\n**NAT的使用有争议**:\n\n- 路由器应当只处理三层以下的报头（端口号在传输层）\n\n- 违反端到端原则（节点介入修改IP地址和端口号）\n\n    NAT 妨碍 P2P 应用程序：需要 NAT 穿越技术\n\n    > 方案一：静态配置 NAT ，将特定端口的连接请求转发给服务器。\n    >\n    > 方案二：利用 UPnP 互联网网关设备协议自动配置。可以学习 NAT 公共 IP 地址，并在 NAT 转换表中增删端口映射。\n    >\n    > 方案三：中继（如 Skype）NAT 内部客户-  中继 - NAT 外部客户；中继服务器桥接两个连接的分组。\n\n- 地址短缺问题应该由  IPv6 解决\n\n\n\n## 互联网控制报文协议 ICMP\n\nICMP 协议支持主机或路由器：差错或异常报告；网络探询；\n\n**两类 ICMP 报文**：\n\n- 差错报告报文\n\n    1. 目的不可达\n\n        > 路由器无法为一个数据报找到路由或主机无法交付一个数据报，然后丢弃数据报\n\n    2. 源抑制\n\n        >  拥塞控制的一种方法：警告源节点，在路径中的某处出现了拥塞，源节点必须放慢(抑制)发送过程\n\n    3. 超时/超期\n\n        > TTL = 0 或\n        >\n        > 目的结点在规定的时间内没有收到一个分组的所有分片\n\n    4. 参数问题\n\n        > 路由器或目的节点发现数据报首部中的字段值出错(二义性)，丢弃该数据报\n\n    5. 重定向\n\n        > 路由器发现这个 IP 数据报不应该由自己转发，则向源主机发送重定向报文，请求重定向\n\n- 网络探询报文\n    1. 回声（Echo）请求与应答报文（Reply）\n    2. 时间戳请求与应答报文\n\n> 由于 ICMP 报文可能需要经过几个网络才能到达源节点，ICMP 报文被封装在IP包中传输。\n>\n> ICMP 通常被认为是 IP 协议的一部分，因为 IP 协议使用 ICMP 向源节点发送错误报告。\n>\n> **Ping 利用 ICMP 报文测试目的主机是否活跃**，以及去往目的主机的路径是否正常：\n>\n> - 源主机发送 Type=8，Code=0 的 Echo Request 报文\n> - 若目的主机收到，发送 Type=0，Code=0 的 Echo Response 报文\n>     源主机计算并报告 RTT\n> - 若源主机连续几次超时（收不到Echo Response），向调用者报告目的不可达\n\n**不产生 ICMP 差错报文情形：**\n\n- 对于携带 ICMP 差错报文的数据报，不再产生 ICMP 差错报文\n- 对于分片的数据报，如果**不是第一个分片**，则不产生 ICMP 差错报文\n- 对于具有**组播(也称多播)地址**的数据报，不产生 ICMP 差错报文\n- 对于具有**特殊地址**(如 127.0.0.0 或 0.0.0.0 )，不产生 ICMP 差错报文\n\n**ICMP 报文格式：**\n\n  {% asset_img 7.PNG This is an ICMPimage %}\n\n  {% asset_img 8.PNG This is an ICMP-2 image %}\n\n\n\n## IPv6\n\n**动机：**\n\n- 32 位 IPv4 地址空间已经分配殆尽\n- 改进首部格式：快速处理、转发数据报；支持 QoS；\n\n**IPv6 与 IPv4 不兼容，但与其它所有因特网协议都兼容。**\n\n### IPv6 数据报格式\n\n- 固定长度的 40 字节基本首部\n\n- **不允许分片**\n\n  {% asset_img 9.PNG This is an ipv6 image %}\n\n### IPv6 地址\n\n128 位，使用冒号十六进制表示，每 16 位以十六进制的形式写成一组，组之间用冒号分隔，如 `8000:0:0:0:0123:4567:89AB:CDEF`\n\n地址表示的零压缩技术：可将连续的多组 0 压缩为一对冒号，如以上地址可表示为：``8000::0123:4567:89AB:CDEF`\n\nIPv6定义了三种地址类型：\n\n- 单播地址：一个特定的网络接口（一对一通信）\n- 多播地址：一组网络接口（一对多通信）\n- 任播地址（anycast）：一组网络接口中的任意一个（通常是最近的一个）\n\n### IPv6 vs. IPv4\n\n与IPv4固定头相比，IPv6的基本头中去掉了以下一些字段：\n\n- IHL：IPv6的基本头总是40字节长\n\n- 与分片相关的字段：**IPv6路由器不负责分片**\n\n- 头校验：计算校验和太花时间；现在的网络非常可靠，并且链路层和传输层上往往又都有校验和\n\n\n\nIPv6基本头中增加了：\n\n- 流标签：支持对数据包区分处理\n\n\n\n改变了以下字段的作用：\n\n- Type of Service：代之以 Traffic Class\n\n- 总长度：代之以载荷长度\n\n- Protocol：代之以Next header，允许任意扩展选项\n\n\n\n**IPv6 数据包如何穿越 IPv4 网络？**\n\n1. 报头转换\n\n    IPv4/IPv6节点（如路由器 B）在将数据报传递给 IPv4 路由器（如路由器 C）之前，将 IPv6 报头转换成 IPv4 报头\n    **缺点**：报头转换不完全，有信息丢失。\n\n2. 建立隧道\n    IPv6/IPv4 边界路由器将 IPv6 包封装到一个 IPv4 包中，送入 IPv4 网络，目的边界路由器取出IPv6包继续传输。\n    **优点**：保留原始数据报的全部信息。\n\n    {% asset_img 10.PNG This is an ipv6-ipv4 image %}\n\n\n\n# 路由算法\n\n路由算法也就是解决选路问题的算法。\n\n选路问题：给定一组路由器和连接路由器的链路，寻找一条从源路由器到目的路由器的最佳路径。\n\n学过数据结构之后我们知道，计算机网络中的路由器网络可以抽象成图模型。\n\n- 顶点：路由器\n- 边：链路\n- 边权：链路费用（可以是带宽的倒数、拥塞程度等）\n\n- **关键问题：源到目的的最小费用路径 == 最短路径问题**\n\n\n\n## 路由算法的分类\n\n1. 静态路由：\n\n    手工配置；路由更新慢；优先级高；\n\n2. 动态路由：\n\n    路由更新快，能定期更新且及时响应链路费用或网络拓扑变化。\n\n3. 全局算法：\n\n    所有路由器掌握完整的网络拓扑和链路费用信息。\n\n    e.g. 链路状态（LS ）路由算法。\n\n4. 分布式算法：\n\n    路由器只掌握物理相连的邻居及链路费用；\n\n    邻居间信息交换、运算的迭代过程；\n\n    e.g. 距离向量（DV）路由算法。\n\n## 链路状态（Link State）路由算法\n\n**基于图的最短路径算法 – Dijkstra 算法，得到源点到其他所有顶点的最短路径。**\n\n1. 所有节点（路由器）掌握网络拓扑和链路费用\n2. 计算一个结点（源）到所有顶点的最短路径\n3. 迭代：经过 k 次迭代后，得到到达 k 个目的结点的最短路径\n\n``` c\n// Dijkstra 算法\nInitialization:\n    N’ = {u}      // N’为已找到最短路径的节点集合，初始时只有u\n    for all nodes v    //标记源节点u到各个节点v的路径代价D(v)\n      if v adjacent to u\n          then D(v) = c(u,v)   //c(u,v)为链路(u,v)的代价\n      else D(v) = ∞\n\n   Loop\n     find w not in N’ such that D(w) is a minimum  //下一条最短路径\n    add w to N’     //将找到最短路径的节点加入N’\n    update D(v) for all v adjacent to w and not in N’ :\n       D(v) = min( D(v), D(w) + c(w,v) )    //更新到相关节点的路径代价\n    until all nodes in N'\n```\n\n{% asset_img 13.PNG This is an dijkstra image %}\n\n**震荡现象：** 若 A 是目的地，则下面这种情况下，最短路径循环变化，有可能一个数据报永远也到达不了 A。\n\n{% asset_img 14.PNG This is an dijkstra-problem image %}\n\n解决办法：引入路由延迟更新算法。\n\n\n\n## 距离向量（Distance Vector）路由算法\n\n**基于 bellman-ford 算法，得到源点到所有点的最短路径。**\n\n\n\n{% asset_img 15.PNG This is an bellman-ford image %}\n\n\n\n**特点：对每个路由器来说，只需要知道其邻居及其链路费用即可。**\n\n\n\n$D_x(y)$：从结点 x 到结点 y 的最小费用估计\n\n- x 维护距离向量 DV：$DV = \\{D_x(y):y\\in N\\}$\n\n结点 x：\n\n- 已知到达每个邻居的费用 c(x,v)\n- 维护其所有邻居 v 的距离向量：$DV = \\{D_v(y):y\\in N\\}$\n\n**核心思想**：\n\n- 每个节点不定时的将自身 DV 估计发送给邻居\n\n- 当 x 接收到最新的 DV 估计时，根据 B-F 方程更新自身的距离向量估计：\n\n    $D_x(y) <- ~~min_v \\{c(x,v) + D_v(y) ~for ~each~node~y\\in N\\}$\n\n- $D_x(y)$ 将**最终收敛于**实际最小费用 $d_x(y)$\n\n\n\n**无穷计数问题：好消息传播快，坏消息传播慢。**\n\n{% asset_img 16.PNG This is an bellman-ford-problem image %}\n\n\n\n**消除无穷计数问题：**\n\n1. 毒性逆转：\n\n    {% asset_img 17.PNG This is an bellman-ford-problem image %}\n\n2. 定义最大度量：\n\n    定义一个最大的有效费用值，如 15 跳，16 跳表示无穷大。\n\n    {% asset_img 18.PNG This is an mmd image %}\n\n    无穷计数不会真正的无穷下去，会在有限的步数内反应网络状态。比如上图中的 R1,R2 已经不可达了。\n\n## 层次路由\n\n将任意规模的网络抽象成一张图，这样计算路由过于理想化。\n\n在实际（大规模）网络中不可行：\n\n> 1. 路由表几乎无法存储；\n> 2. 路由计算过程的信息交换量巨大，会淹没电路。\n\n考虑网络管理自治性的问题：每个网络的管理可能都期望自主控制内部路由算法。\n\n### 自治系统 AS（autonomous systems）\n\n聚合路由器为一个区域：自治系统。\n\n同一个 AS 内的路由器运行相同的路由协议（算法）。\n\n- 自治系统内部路由协议\n- 不同的 AS 内的路由器可以运行不同的 AS 内部路由协议\n\n网关路由器：\n\n- 在 AS 边缘\n- 通过链路连接其他的 AS 网关路由器\n\n互连的 AS：\n\n- 转发表由 AS 内部路由算法与 AS 间路由算法共同设置。\n\n{% asset_img 19.PNG This is an AS image %}\n\n**举个栗子：**\n\n{% asset_img 20.PNG This is an eg-1 image %}\n\n{% asset_img 21.PNG This is an eg-2 image %}\n\n热土豆路由协议：选择最近的网关路由器。\n\n{% asset_img 22.PNG This is an eg-3 image %}\n\n\n\n# 路由协议\n\nInternet 采用层次路由。\n\nAS 内部路由协议也称内部网关协议IGP（Interior Gateway Protocols），\n\n最常见的有:\n\n- 路由信息协议 RIP（Routing Information Protocol）：较低层ISP和企业网中使用\n- 开放最短路径优先协议 OSPF（Open Shortest Path First）：较顶层 ISP 中使用\n\n外部网关协议 EGP（Exterior Gateway Protocols），目前只有： BGP（Border Gateway Protocol）\n\n## RIP\n\nRIP 采用**距离矢量选路算法**：\n\n- 距离度量：跳步数（MAX = 15 hops），每条链路一个跳步。\n- RIP 响应报文（RIP通告）\n    距离向量封装在RIP响应报文中传输；\n    每个报文携带一个目的子网列表（最多包含25个子网），以及到每个目的子网的最短距离；\n\n- RIP 响应报文的发送：\n    相邻路由器之间大约每 30 秒交换一次 RIP 响应报文\n    RIP 报文封装在 UDP 报文中发送，使用 UDP 端口 520\n\n    > RIP是一个应用层协议，其路由表示利用一个称作 route-d（daemon）的应用层进程进行管理。\n    >\n    > RIP：是应用层协议，但完成的是网络层功能。\n\n**RIP 链路的失效和恢复：**\n\n- 经过该邻居的路由不可用：需要重新计算路由\n\n- 向邻居发送新的通告\n\n- 若转发表改变，邻居再依次向外发送通告\n\n- 链路失效信息能否快速传播到全网？\n\n    可能发生无穷计数问题，但因为规定了最大跳数，故而可以在有限时间内收敛到正确状态。\n\n- **毒性逆转技术** 用于预防乒乓环路。\n\n> RIP 认为 15 跳步以内有效，16跳步及以上则认为网络不可达。所以 RIP 适用于小规模的自治系统，超过 15 跳的自治网络就不再适用了。\n\n## OSPF （Open Shortest Path First）\n\n**特点：**\n\n开放，公众可用；\n\n采用链路状态路由算法：\n\n1. LS 分组扩散（通告）\n2. 每个路由器构造完整的网络（AS）拓扑图\n3. 利用 Dijkstra 算法计算路由\n\nOSPF 通告中每个入口对应一个邻居；\n\nOSPF 通告在整个 AS 范围内泛洪，其报文直接封装到 IP 数据报中。\n\n与 OSPF 及其相似的路由协议： IS-IS 路由协议。\n\n## BGP – 自治系统间的路由选择\n\n用于确定跨越多个 AS 的源和目的对之间的路径。\n\n","tags":["计算机网络"],"categories":["复习笔记"]}]