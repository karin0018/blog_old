---
title: chap4 多层感知机(3)
toc: true
comments: true
math: true
date: 2021-12-04 17:44:01
tags: DeepLearning
categories: 跟李沐学 AI
---
# 数值稳定性和模型初始化

> 到目前为止，我们实现的每个模型都是根据**某个预先指定的分布**来初始化模型的参数。直到现在，我们认为初始化方案是理所当然的，忽略了如何做出这些选择的细节。你甚至可能会觉得，初始化方案的选择并不是特别重要。相反，**初始化方案的选择在神经网络学习中起着非常重要的作用**，**它对保持数值稳定性至关重要**。
>
> 此外，这些选择可以与非线性激活函数的选择以有趣的方式结合在一起。**我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快**。糟糕选择可能会导致我们在训练时遇到**梯度爆炸**或梯度消失。在本节中，我们将更详细地探讨这些主题，并讨论一些有用的启发式方法。你会发现这些启发式方法在你的整个深度学习生涯中都很有用。

<!--more-->

## 数值稳定性

神经网络的梯度：

- 考虑如下的 d 层神经网络
  $$
  \mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) \text{ 因此 } \mathbf{o} = f_L \circ \ldots \circ f_1(\mathbf{x})
  $$

- 如果所有隐藏变量和输入都是向量，我们可以将$\mathbf{o}$关于任何一组参数$\mathbf{W}^{(l)}$的梯度写为下式：

$$
\partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}_{ \mathbf{M}^{(L)} \stackrel{\mathrm{def}}{=}} \cdot \ldots \cdot \underbrace{\partial_{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}_{ \mathbf{M}^{(l+1)} \stackrel{\mathrm{def}}{=}} \underbrace{\partial_{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\mathrm{def}}{=}}.
$$


可以看见，我们做了很多的矩阵乘法，大量的矩阵乘法带来数值稳定性的常见两个问题：梯度爆炸和梯度消失。

### 梯度爆炸

参数更新过大，破坏了模型的稳定收敛。

e.g. 使用 ReLU 函数作为激活函数。

问题：

- 值超出值域，对于16 位浮点数尤为严重
- 对学习率敏感
  - 学习率过大->大参数值->更大的梯度
  - 学习率太小->训练没有进展
  - 可能要在训练过程中不断调整学习率

### 梯度消失

参数更新过小，每次更新时几乎不会移动，导致无法学习。

e.g. 使用 sigmoid 函数作为激活函数

问题：

- 梯度值变为 0，对于16 位浮点数尤为严重
- 不管如何选择学习率，训练都没有进展
- 对底部层尤为严重，仅仅顶部层训练的较好，无法让神经网络更深

### 总结

- 当数值过大或者过小时会导致数值问题
- 常发生在深度模型中，因为其会对 n 个数累乘



## 如何让训练更加稳定

目标：让梯度值在合理的范围内。

- 让乘法变加法
  - ResNet，LSTM
- 归一化
  - 梯度归一化（均值为 0，方差为 1），梯度裁剪
- 合理的权重初始和激活函数

### 权重初始化

- 在合理值区间里随机初始化参数
- 训练开始的时候更容易有数值不稳定
  - 远离最优解的地方损失函数表面可能很复杂
  - 最优解附近表面可能会比较平

- 使用 $N(0,0.01)$ 来初始化可能对小网络没问题，但不能保证深度神经网络

#### 默认初始化

在前面的部分中，我们使用正态分布来初始化权重值。如果我们不指定初始化方法，框架将使用默认的随机初始化方法，对于中等规模的问题，这种方法通常很有效。

#### Xavier初始化

**关键点：**

- 将每层的输出和梯度都看作随机变量
- 让他们的均值和方差都保持一致

让我们看看某些*没有非线性*的全连接层输出(例如，隐藏变量)$o_{i}$的尺度分布。
对于该层$n_\mathrm{in}$输入$x_j$及其相关权重$w_{ij}$，输出由下式给出

$$
o_{i} = \sum_{j=1}^{n_\mathrm{in}} w_{ij} x_j.
$$

权重$w_{ij}$都是从同一分布中独立抽取的。此外，让我们假设该分布具有零均值和方差$\sigma^2$。请注意，这并不意味着分布必须是高斯的，只是均值和方差需要存在。现在，让我们假设层$x_j$的输入也具有零均值和方差$\gamma^2$，并且它们独立于$w_{ij}$并且彼此独立。在这种情况下，我们可以按如下方式计算$o_i$的平均值和方差：

$$
\begin{aligned}
    E[o_i] & = \sum_{j=1}^{n_\mathrm{in}} E[w_{ij} x_j] \\&= \sum_{j=1}^{n_\mathrm{in}} E[w_{ij}] E[x_j] \\&= 0, \\
    \mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\
        & = n_\mathrm{in} \sigma^2 \gamma^2.
\end{aligned}
$$

**保持方差不变的一种方法是设置$n_\mathrm{in} \sigma^2 = 1$**。现在考虑反向传播过程，我们面临着类似的问题，尽管梯度是从更靠近输出的层传播的。使用与正向传播相同的推理，我们可以看到，除非$n_\mathrm{out} \sigma^2 = 1$，否则梯度的方差可能会增大，其中$n_\mathrm{out}$是该层的输出的数量。这使我们进退两难：我们不可能同时满足这两个条件。相反，我们只需满足：

$$
\begin{aligned}
\frac{1}{2} (n_\mathrm{in} + n_\mathrm{out}) \sigma^2 = 1 \text{ 或等价于 }
\sigma = \sqrt{\frac{2}{n_\mathrm{in} + n_\mathrm{out}}}.
\end{aligned}
$$


通常，Xavier初始化从均值为零，方差$\sigma^2 = \frac{2}{n_\mathrm{in} + n_\mathrm{out}}$的高斯分布中采样权重。我们也可以利用Xavier的直觉来选择从均匀分布中抽取权重时的方差。注意均匀分布$U(-a, a)$的方差为$\frac{a^2}{3}$。将$\frac{a^2}{3}$代入到$\sigma^2$的条件中，将得到初始化的建议：

$$
U\left(-\sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}, \sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}\right).
$$

尽管上述数学推理中，不存在非线性的假设在神经网络中很容易被违反，但Xavier初始化方法在实践中被证明是有效的。

#### 检查常用激活函数

线性的激活函数若想满足 Xavier初始化，则激活函数就必须为 $\sigma(x)=x$

可以对常用的激活函数做泰勒展开，看其是否与 $f(x)=x$ 低阶近似。

- $sigmoid(x) = 1/2 + x/4 - x^3/48+O(x^5)$
- $tanh(x)=0+x-x^3/3+O(x^5)$
- $relu(x)=0+x~ for ~ x \ge 0$

可以看出 tanh 和 relu 函数可以低阶近似于 $f(x)=x$ ，但是 sigmoid 函数需要调整：

$4 \times sigmoid(x)-2$



### 总结

- 合理的权重初始值和激活函数的选取可以提升数值的稳定性

