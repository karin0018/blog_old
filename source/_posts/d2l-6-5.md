---
title: chap6(5) 池化层
toc: true
comments: true
math: true
date: 2021-12-19 17:48:26
tags: DeepLearning
categories: 跟李沐学 AI
---

本节将介绍 *池化*（pooling）层，它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。
<!--more-->

通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率，聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。

而我们的机器学习任务通常会跟全局图像的问题有关（例如，“图像是否包含一只猫呢？”）， 所以我们最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。

此外，当检测较底层的特征时，我们通常希望这些特征保持某种程度上的平移不变性。例如，如果我们拍摄黑白之间轮廓清晰的图像 `X`，并将整个图像向右移动一个像素，即 `Z[i, j] = X[i, j + 1]`，则新图像 `Z` 的输出可能大不相同。而在现实中，随着拍摄角度的移动，任何物体几乎不可能发生在同一像素上。即使用三脚架拍摄一个静止的物体，由于快门的移动而引起的相机振动，可能会使所有物体左右移动一个像素（除了高端相机配备了特殊功能来解决这个问题）。

## 池化层

最大池化层：

- 返回滑动窗口中的最大值

平均池化层：

- 返回滑动窗口中的平均值



## 填充、步幅、多个通道

- 与卷积层类似
- 没有可学习的参数（没有 kernel）
- 在每个输入通道应用池化层获得相应的输出通道，不会做融合
- 输出通道数=输入通道数



## 总结

- 池化层返回窗口中最大或平均值
- 缓解卷积层对位置的敏感性
- 同样有窗口大小、填充、步幅作为超参数



## torch 知识点

### **1 torch.cat()**

> `torch.cat`(*tensors*,*dim=0*,*out=None*)→ Tensor

torch.cat()对tensors沿指定维度拼接，但返回的Tensor的维数不会变【理解为缝合拼接，拉长向量长度】

```python3
>>> import torch
>>> a = torch.rand((2, 3))
>>> b = torch.rand((2, 3))
>>> c = torch.cat((a, b))
>>> a.size(), b.size(), c.size()
(torch.Size([2, 3]), torch.Size([2, 3]), torch.Size([4, 3]))
```

可以看到 c 和 a、b 一样都是二维的。



### **2 torch.stack()**

> `torch.stack`(*tensors*,*dim=0*,*out=None*)→ Tensor

torch.stack()同样是对tensors沿指定维度拼接，但返回的Tensor会多一维【可以理解为叠加，一层层摞上去】

```python3
>>> import torch
>>> a = torch.rand((2, 3))
>>> b = torch.rand((2, 3))
>>> c = torch.stack((a, b))
>>> a.size(), b.size(), c.size()
(torch.Size([2, 3]), torch.Size([2, 3]), torch.Size([2, 2, 3]))
```

可以看到 c 是三维的，比 a、b 多了一维。

