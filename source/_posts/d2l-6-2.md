---
title: chap6 卷积神经网络(2) 图像卷积
toc: true
comments: true
math: true
date: 2021-12-13 19:49:47
tags: DeepLearning
categories: 跟李沐学 AI
---
上节我们解析了卷积层的原理，现在我们看看它的实际应用。由于卷积神经网络的设计是用于探索图像数据，本节我们将以图像为例。

[本章代码复现](https://github.com/karin0018/d2l_MuLi/tree/master/convolutional-neural-network)

<!-- more -->

## 互相关运算

严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是 *互相关运算* (cross-correlation)，而不是卷积运算。根据 :numref:`sec_why-conv` 中的描述，在卷积层中，输入张量和核张量通过(**互相关运算**)产生输出张量。

在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。

当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到一个单一的标量值，由此我们得出了这一位置的输出张量值。

{% asset_img correlation.svg 二维互相关运算。阴影部分是第一个输出元素，以及用于计算这个输出的输入和核张量元素：$0\times0+1\times1+3\times2+4\times3=19$. %}



## 卷积层

**功能**：卷积层对输入和卷积核权重进行**互相关运算**，并在**添加标量偏置**之后产生输出。

**被训练的参数**：**卷积核权重**和**标量偏置**。 

**初始化**：就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。



### 二维卷积层

- 输入： $X:n_h\times n_w$

- 核： $W: k_h \times k_w$

- 偏差： $b \in R$

- 输出：$Y : (n_h-k_h+1)\times(n_w-k_w+1)$
  $$
  \mathbf{Y=X*W}+b
  $$
  $\mathbf{W},b$ 都是可学习的参数，$*$ 是前面定义的互相关运算子

  > 所以卷积核是学习出来的 :)



## 总结

- 卷积层是将输入和核矩阵进行交叉相关计算，加上偏移之后得到输出
- 核矩阵和偏移是可以学习的参数
- 核矩阵的大小是超参数

