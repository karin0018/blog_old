---
title: chap10 注意力机制 (基础部分)
toc: true
comments: true
math: true
date: 2021-12-22 19:57:47
tags: DeepLearning
categories: 跟李沐学 AI
---



- 卷积、全连接、池化层都只考虑不随意线索

- 注意力机制则显示的考虑随意线索

  - 随意线索被称之为查询（query）

  - 每个输入是一个值（y: value）和不随意线索（x: key）对

    > value 是 key 的价值

  - 通过注意力池化层来有偏向性的选择某些输入

<!--more-->

## 非参注意力池化层

Nadaraya  和 Waston提出了一个更好的想法，根据输入的位置对输出 $y_i$ 进行加权：

$$
f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i,
$$


:label:`eq_nadaraya-waston`

其中 $K$ 是 *核*（kernel）。公式 :eqref:`eq_nadaraya-waston` 所描述的估计器被称为 *Nadaraya-Watson 核回归*（Nadaraya-Watson kernel regression）。在这里我们不会深入讨论核函数的细节。回想一下 `fig_qkv` 中的注意力机制框架，我们可以从注意力机制的角度重写 :eqref:`eq_nadaraya-waston` 成为一个更加通用的 *注意力汇聚*（attention pooling）公式：

$$
f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,
$$
:label:`eq_attn-pooling`

其中 $x$ 是查询，$(x_i, y_i)$ 是键值对。比较 `eq_attn-pooling` 和 `eq_avg-pooling`，注意力汇聚是 $y_i$ 的加权平均。将查询 $x$ 和键 $x_i$ 之间的关系建模为 *注意力权重*（attetnion weight） $\alpha(x, x_i)$，如 `eq_attn-pooling` 所示，这个权重将被分配给每一个对应值 $y_i$。对于任何查询，模型在所有键值对上的注意力权重都是一个有效的概率分布：它们是非负数的，并且总和为1。

考虑一个 *高斯核*（Gaussian kernel），其定义为：

$$
K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2}).
$$

将高斯核代入 `eq_attn-pooling` 和 `eq_nadaraya-waston` 可以得到：

$$
\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}
$$
:label:`eq_nadaraya-waston-gaussian`

在 `eq_nadaraya-waston-gaussian` 中，**如果一个键 $x_i$ 越是接近给定的查询 $x$, 那么分配给这个键对应值 $y_i$ 的注意力权重就会越大, 也就是“获得了更多的注意力”**。

值得注意的是，Nadaraya-Watson 核回归是一个非参数模型。因此， :eqref:`eq_nadaraya-waston-gaussian` 是 *非参数的注意力汇聚*（nonparametric attention pooling）的例子。



## 带参数注意力池化层

非参数的 Nadaraya-Watson 核回归具有 *一致性*（consistency） 的优点：如果有足够的数据，此模型会收敛到最优结果。尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中。

例如，与 :eqref:`eq_nadaraya-waston-gaussian` 略有不同，在下面的查询 $x$ 和键 $x_i$ 之间的距离乘以可学习参数 $w$：

$$
\begin{aligned}f(x) &= \sum_{i=1}^n \alpha(x, x_i) y_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_i)w)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i.\end{aligned}
$$
:eqlabel:`eq_nadaraya-waston-gaussian-para`



## 注意力分数

上文高斯核的指数部分视为 *注意力评分函数*（attention scoring function），简称 *评分函数*（scoring function）.

把这个函数的输出结果输入到 softmax 函数中进行运算，将得到与键对应的值的概率分布（即注意力权重）。最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。

从宏观来看，可以使用上述算法来实现 :numref:`fig_qkv` 中的注意力机制框架。

`fig_attention_output` 说明了如何将注意力汇聚的输出计算成为值的加权和，其中 *𝑎* 表示注意力评分函数。由于注意力权重是概率分布，因此加权和其本质上是加权平均值。

{% asset_img attention-output.svg attention-output %}

:label:`fig_attention_output` 

### [**加性注意力**]

:label:`subsec_additive-attention`

一般来说，**当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数**。给定查询 $\mathbf{q} \in \mathbb{R}^q$ 和键 $\mathbf{k} \in \mathbb{R}^k$，*加性注意力*（additive attention） 的评分函数为

$$
a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},
$$
:eqlabel:`eq_additive-attn`

其中可学习的参数是 $\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$ 和 $\mathbf w_v\in\mathbb R^{h}$。

将查询和键连接起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数 $h$。通过使用 $\tanh$ 作为激活函数，并且禁用偏置项。

### [**缩放点积注意力**]

使用点积可以得到计算效率更高的评分函数。但是点积操作要求查询和键具有相同的长度 $d$。

假设查询和键的所有元素都是独立的随机变量，并且都满足均值为 $0$ 和方差为 $1$。那么两个向量的点积的均值为 $0$，方差为 $d$。

为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是 $1$，则可以使用 *缩放点积注意力*（scaled dot-product attention） 评分函数：
$$
a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}
$$


将点积除以 $\sqrt{d}$。在实践中，我们通常从小批量的角度来考虑提高效率，

例如基于 $n$ 个查询和 $m$ 个键－值对计算注意力，其中查询和键的长度为 $d$，值的长度为 $v$。查询 $\mathbf Q\in\mathbb R^{n\times d}$、键 $\mathbf K\in\mathbb R^{m\times d}$ 和值 $\mathbf V\in\mathbb R^{m\times v}$ 的缩放点积注意力是
$$
 \mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.
$$
:eqlabel:`eq_softmax_QK_V`

### 总结

- 注意力分数是 query 和 key 的相似度，注意力权重是分数的 softmax 结果
- 两种常见的分数计算
  - 将 query 和 key 合并起来进入一个单输出的单隐藏层的 MLP
  - 直接将 query 和 key 做内积



## 使用注意力的 seq2seq

动机：希望在翻译句子的时候，每一步的翻译都关注到原句的对应词。



### 加入注意力

{% asset_img seq2seq-attention-details.svg seq2seq-attention-details %}

- 编码器对每次词的输出作为 key 和 value
- 解码器 RNN 对上一个词的输出是 query（找距离 query 最近的 key 的 value）
- 注意力的输出和下一个词的词嵌入（target embedding）合并进入解码器 RNN



### 总结

- Seq2seq 通过隐状态在编码器和解码器中传递信息
- 注意力机制可以根据解码器的 RNN 的输出匹配到合适的编码器的输出来传递更有效的信息

