---
title: chap9 现代循环神经网络(3) 深度循环神经网络
toc: true
comments: true
math: true
date: 2021-12-21 19:47:09
tags: DeepLearning
categories: 跟李沐学 AI
---

到目前为止，我们只讨论了具有一个单向隐藏层的循环神经网络。如何获得更多的非线性呢？

<!--more-->

- 浅 RNN：
  - 输入层
  - 隐藏层
  - 输出层

- 深 RNN：
  - 输入层
  - 隐藏层
  - 隐藏层
  - …
  - 输出层

{% asset_img deep_rnn.svg deep_rnn %}

假设我们在时间步 $t$ 有一个小批量的输入数据 $\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数：$n$，每个样本中的输入数：$d$）。

同时，将 $l^\mathrm{th}$ 隐藏层（$l=1,\ldots,L$）的隐藏状态设为 $\mathbf{H}_t^{(l)}  \in \mathbb{R}^{n \times h}$（隐藏单元数：$h$），

输出层变量设为 $\mathbf{O}_t \in \mathbb{R}^{n \times q}$（输出数：$q$）。

设置 $\mathbf{H}_t^{(0)} = \mathbf{X}_t$，第 $l$ 个隐藏层的隐藏状态使用激活函数 $\phi_l$ 的表示如下：
$$
\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)}  + \mathbf{b}_h^{(l)}),
$$

:eqlabel:`eq_deep_rnn_H`

其中，权重 $\mathbf{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h}$ 和 $\mathbf{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h}$ 和偏置 $\mathbf{b}_h^{(l)} \in \mathbb{R}^{1 \times h}$ **都是第 $l$ 个隐藏层的模型参数。**

最后，输出层的计算仅基于第 $l$ 个隐藏层最终的隐藏状态：

$$
\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q,
$$

其中，权重 $\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$ 和偏置 $\mathbf{b}_q \in \mathbb{R}^{1 \times q}$ 都是**输出层的模型参数**。



> 与多层感知机一样，**隐藏层的数目 $L$ 和隐藏单元的数目 $h$ 都是超参数。**也就是说，它们可以由我们来调整或指定。
>
> 另外，用门控循环单元或长短期记忆网络的隐藏状态来代替 :eqref:`eq_deep_rnn_H` 中的隐藏状态进行计算，可以很容易地得到**深度门控循环神经网络。**



## 总结

- 深度循环神经网络使用多个隐藏层来获得更多的非线性。

