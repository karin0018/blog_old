---
title: chap6 卷积神经网络(3) 填充和步幅
toc: true
comments: true
math: true
date: 2021-12-13 21:26:31
tags: DeepLearning
categories: 跟李沐学 AI
---

假设输入形状为 $n_h\times n_w$，卷积核形状为 $k_h\times k_w$，那么输出形状将是 $(n_h-k_h+1) \times (n_w-k_w+1)$。
因此，卷积的输出形状取决于输入形状和卷积核的形状。

还有什么因素会影响输出的大小呢？本节我们将介绍 *填充*（padding）和 *步幅* (stride)。

假设以下情景：
有时，在应用了连续的卷积之后，我们最终**得到的输出远小于输入大小**。这是由于卷积核的宽度和高度通常大于 $1$ 所导致的。比如，一个 $240 \times 240$ 像素的图像，经过 $10$ 层 $5 \times 5$ 的卷积后，将减少到 $200 \times 200$ 像素。如此一来，原始图像的边界丢失了许多有用信息。 而***填充*** 是解决此问题最有效的方法。
有时，我们可能希望**大幅降低图像的宽度和高度**。例如，如果我们发现**原始的输入分辨率十分冗余**。 ***步幅 ***则可以在这类情况下提供帮助。

[本章代码复现](https://github.com/karin0018/d2l_MuLi/tree/master/convolutional-neural-network)

<!--more-->

## 填充

通常，如果我们添加 $p_h$ 行填充（大约一半在顶部，一半在底部）和 $p_w$ 列填充（左侧大约一半，右侧一半），则输出形状将为

$$
(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1)。
$$


这意味着输出的高度和宽度将分别增加 $p_h$ 和 $p_w$。

在许多情况下，我们需要设置 $p_h=k_h-1$ 和 $p_w=k_w-1$，使输入和输出具有相同的高度和宽度。

这样可以在构建网络时更容易地预测每个图层的输出形状。

假设 $k_h$ 是奇数，我们将在高度的两侧填充 $p_h/2$ 行。

如果 $k_h$ 是偶数，则一种可能性是在输入顶部填充 $\lceil p_h/2\rceil$ 行，在底部填充 $\lfloor p_h/2\rfloor$ 行。同理，我们填充宽度的两侧。

> 卷积神经网络中卷积核的高度和宽度通常为奇数，例如 1、3、5 或 7。
> 选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。

此外，使用奇数核和填充也提供了书写上的便利。对于任何二维张量 `X`，当满足：

1. 内核的大小是奇数；

2. 所有边的填充行数和列数相同；

3. 输出与输入具有相同高度和宽度

  则可以得出：输出 `Y[i, j]` 是通过以输入 `X[i, j]` 为中心，与卷积核进行互相关计算得到的。



## 步幅

- 填充减小的输出大小与层数线性相关
  - 需要大量计算才能得到较小的输出

- 步幅是指行/列的滑动步长

通常，当垂直步幅为 $s_h$ 、水平步幅为 $s_w$ 时，输出形状为

$$
\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.
$$


如果我们设置了 $p_h=k_h-1$ 和 $p_w=k_w-1$，则输出形状将简化为 
$$
\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor
$$

更进一步，如果输入的高度和宽度可以被垂直和水平步幅整除，则输出形状将为 $(n_h/s_h) \times (n_w/s_w)$。



## 小结

- 填充和步幅是卷积神经网络的超参数。

* 填充可以增加输出的高度和宽度。这常用来使输出与输入具有相同的高和宽。
* 步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的 $1/n$（ $n$ 是一个大于 $1$ 的整数）。
* 填充和步幅可用于有效地调整数据的维度。

